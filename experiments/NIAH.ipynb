{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55f41874-e95c-4420-983c-17b4fe343fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b418c5c-6a0e-4494-9f8c-38233cf207e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from model.long import LongConfig, LongForCausalLM\n",
    "# from model.long_new import LongConfig, LongHFModel\n",
    "from transformers import MambaConfig, MambaForCausalLM\n",
    "from transformers import GPT2Config, GPT2LMHeadModel\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm \n",
    "import numpy as np\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90228299-4680-48ed-95bc-75465a3477a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeedleHaystackDataset(Dataset):\n",
    "    def __init__(self, size=2000, min_len=32, max_len=64, vocab_size=128, depth=None):\n",
    "        \"\"\"\n",
    "        depth: Float between 0.0 and 1.0. \n",
    "               If None, depth is randomized for every sample (0% to 100%).\n",
    "               If set (e.g., 0.5), the needle is always placed at 50% context.\n",
    "        \"\"\"\n",
    "        self.size = size\n",
    "        self.min_len = min_len\n",
    "        self.max_len = max_len\n",
    "        self.vocab_size = vocab_size\n",
    "        self.depth = depth\n",
    "        \n",
    "        # We reserve the last token as the specific \"Prompt\" trigger\n",
    "        self.prompt_token = torch.tensor([vocab_size - 1]) \n",
    "        \n",
    "    def __len__(self): \n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 1. Determine total length of the sequence\n",
    "        curr_len = np.random.randint(self.min_len, self.max_len + 1)\n",
    "        \n",
    "        # 2. Generate the Key (Needle)\n",
    "        # Range: [1, vocab-2] to avoid padding (0) and prompt token (vocab-1)\n",
    "        key = torch.randint(1, self.vocab_size - 1, (1,))\n",
    "        \n",
    "        # 3. Generate Noise (Haystack)\n",
    "        # We need space for 2 prompt tokens and 2 key tokens (4 tokens total overhead)\n",
    "        noise_len = max(0, curr_len - 4)\n",
    "        noise = torch.randint(1, self.vocab_size - 1, (noise_len,))\n",
    "        \n",
    "        # 4. Determine Insertion Point (Depth)\n",
    "        if self.depth is not None:\n",
    "            # Fixed depth (e.g., 0.9 for 90% deep)\n",
    "            insert_idx = int(noise_len * self.depth)\n",
    "        else:\n",
    "            # Fully Random depth (0% to 100%)\n",
    "            # FIX: Previously this was noise_len // 2 (biased to start)\n",
    "            insert_idx = torch.randint(0, noise_len + 1, (1,)).item()\n",
    "        \n",
    "        # 5. Construct Sequence\n",
    "        # [Noise Part A] -> [Prompt] -> [Key] -> [Noise Part B] -> [Prompt] -> [Key (Target)]\n",
    "        input_ids = torch.cat([\n",
    "            noise[:insert_idx], \n",
    "            self.prompt_token, key,      \n",
    "            noise[insert_idx:], \n",
    "            self.prompt_token, key       \n",
    "        ])\n",
    "        \n",
    "        # 6. Create Labels (Mask everything except the final Key)\n",
    "        labels = input_ids.clone()\n",
    "        # Mask everything up to the final token\n",
    "        labels[:-1] = -100 \n",
    "        \n",
    "        return {\"input_ids\": input_ids, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc22afc3-4ae6-4cfe-b006-d6be89f88621",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_collate_fn(batch):\n",
    "    inputs = rnn_utils.pad_sequence([x['input_ids'] for x in batch], batch_first=True, padding_value=0)\n",
    "    labels = rnn_utils.pad_sequence([x['labels'] for x in batch], batch_first=True, padding_value=-100)\n",
    "    return {\"input_ids\": inputs, \"labels\": labels}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6aa6b2b-a8a4-483f-8d09-9cec8a31c906",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 128\n",
    "DIM = 64\n",
    "LAYERS = 2\n",
    "HEADS = 4\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "config_kwargs = {\n",
    "     \"vocab_size\": VOCAB_SIZE, \n",
    "     \"ssm_cfg\": {\"dropout\": 0.0 }\n",
    "}\n",
    "mamba_config = MambaConfig(\n",
    "    hidden_size = DIM,\n",
    "    num_hidden_layers = LAYERS, \n",
    "    **config_kwargs\n",
    ")\n",
    "mamba_model = MambaForCausalLM(mamba_config).to(DEVICE)\n",
    "\n",
    "gpt2_config = GPT2Config(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    # 1. Context Window\n",
    "    # GPT-2 has a HARD limit. You must set this >= your max haystack length.\n",
    "    n_positions=8192, \n",
    "    \n",
    "    # 2. Dimensions (Matching your request)\n",
    "    n_embd=128,       # This is \"hidden_size\"\n",
    "    n_layer=2,        # This is \"num_hidden_layers\"\n",
    "    \n",
    "    # 3. Heads\n",
    "    # n_embd (128) must be divisible by n_head. \n",
    "    # 4 heads gives 32 dimension per head (standard).\n",
    "    n_head=4, \n",
    "    \n",
    "    # 4. Cleanup\n",
    "    bos_token_id=0,\n",
    "    eos_token_id=0,\n",
    "    \n",
    "    # Optional: Disable dropout for pure algorithmic testing (like your Mamba config)\n",
    "    resid_pdrop=0.0,\n",
    "    embd_pdrop=0.0,\n",
    "    attn_pdrop=0.0,\n",
    "    use_cache=False # False for training with Gradient Checkpointing, True for generation\n",
    ")\n",
    "\n",
    "model_gpt2 = GPT2LMHeadModel(gpt2_config).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d455a8c-dd6b-4b87-b982-7b778b2f1dff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LongForCausalLM(\n",
      "  (long_model): LongModel(\n",
      "    (wte): Embedding(1000, 64)\n",
      "    (layers): ModuleList(\n",
      "      (0): LongBlock(\n",
      "        (attn): LongAttention(\n",
      "          (q_proj): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (k_proj): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (v_proj): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (conv): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)\n",
      "          (input_gate_proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (output_gate_proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (gamma_proj): Linear(in_features=64, out_features=4, bias=True)\n",
      "          (o_proj): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (v_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "          (grp_norm): GroupNorm(4, 64, eps=1e-05, affine=True)\n",
      "          (mem_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (mlp): LongMLP(\n",
      "          (w_gate): Linear(in_features=64, out_features=256, bias=False)\n",
      "          (w_val): Linear(in_features=64, out_features=256, bias=False)\n",
      "          (w_out): Linear(in_features=256, out_features=64, bias=False)\n",
      "        )\n",
      "        (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): LongBlock(\n",
      "        (attn): RoPESelfAttention(\n",
      "          (q_proj): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (k_proj): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (v_proj): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (o_proj): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (rotary_emb): RotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LongMLP(\n",
      "          (w_gate): Linear(in_features=64, out_features=256, bias=False)\n",
      "          (w_val): Linear(in_features=64, out_features=256, bias=False)\n",
      "          (w_out): Linear(in_features=256, out_features=64, bias=False)\n",
      "        )\n",
      "        (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=64, out_features=1000, bias=False)\n",
      ")\n",
      "\n",
      "ðŸš€ Training Long-LLM (HF)...\n",
      "   Ep 5: Loss 6.9099\n",
      "   Ep 10: Loss 6.4961\n",
      "   Ep 15: Loss 3.8728\n",
      "   Ep 20: Loss 2.4074\n",
      "   Ep 25: Loss 0.7026\n",
      "   Ep 30: Loss 0.5587\n",
      "   ðŸ§ª Benchmarking Long-LLM (HF)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7430c03624e140ab876ef66505a48c42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Long-LLM (HF):   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1a9c92d251246dd984b09c8b5fd56af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "L=64:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length 64: Acc 87.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a35037aaa34146fab4be200bee0b66f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "L=128:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length 128: Acc 84.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da2f3990e78546c4b68e0888f1ab9829",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "L=256:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length 256: Acc 89.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9831c85f1f3412784ee1a76a9bf540c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "L=512:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length 512: Acc 87.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ea99c07824c41f1b9bb96cb8ce39a8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "L=1024:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length 1024: Acc 87.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edb32b03ae354535986b7e3296541b6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "L=2048:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length 2048: Acc 92.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "409c586bdbb14aed9aa281ec3764e6fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "L=4096:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length 4096: Acc 87.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a6223f5b65e4783bebd0f9d6a5d6bce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "L=8192:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length 8192: Acc 77.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba7c6b95bb0e4e738acf4b2f613c26be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "L=16384:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length 16384: Acc 36.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b08d3894169645bd8071e5c0aff38de2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "L=32768:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length 32768: Acc 5.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37aef47b1b7a4e11aa729e2809fabe5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "L=65536:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length 65536: Acc 0.00%\n",
      "\n",
      "ðŸš€ Training Mamba (HF)...\n",
      "   Ep 5: Loss 5.7632\n",
      "   Ep 10: Loss 0.0284\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def run_benchmark():\n",
    "    VOCAB_SIZE = 1000\n",
    "    DIM = 64\n",
    "    LAYERS = 2\n",
    "    HEADS = 4\n",
    "    EPOCHS = 30\n",
    "    BATCH_SIZE = 64\n",
    "    \n",
    "    # --- 1. Model Setup (Hugging Face Style) ---\n",
    "    \n",
    "    # Setup Long-LLM Config\n",
    "    # Assuming LongConfig is your custom config class\n",
    "    long_config = LongConfig(\n",
    "        vocab_size = VOCAB_SIZE, \n",
    "        hidden_size = DIM, \n",
    "        num_hidden_layers = LAYERS, \n",
    "        num_heads = HEADS,\n",
    "        expansion_ratio = 4, \n",
    "        hybrid_ratio = 2,\n",
    "        gate_init_bias = -3,\n",
    "        # This dim to support long context distances\n",
    "        rope_base_dim = 1000000,\n",
    "    )\n",
    "    long_llm = LongForCausalLM(long_config).to(DEVICE)\n",
    "    print(long_llm)\n",
    "    # long_llm = LongNet(VOCAB_SIZE, DIM, LAYERS, HEADS).to(DEVICE)\n",
    "\n",
    "    # Setup Mamba HF\n",
    "    mamba_config = MambaConfig(\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        hidden_size=DIM,\n",
    "        num_hidden_layers=LAYERS,\n",
    "        ssm_cfg={\"dropout\": 0.0}\n",
    "    )\n",
    "    mamba_hf = MambaForCausalLM(mamba_config).to(DEVICE)\n",
    "\n",
    "\n",
    "    models = {\n",
    "        \"Long-LLM (HF)\": long_llm,\n",
    "        \"Mamba (HF)\": mamba_hf,\n",
    "    }\n",
    "    \n",
    "    # test_lengths = [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536, 131072, 262144]\n",
    "    test_lengths = [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536] \n",
    "\n",
    "    results = {name: [] for name in models}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nðŸš€ Training {name}...\")\n",
    "\n",
    "        lr = 0.001 if name == \"Mamba (HF)\" else 0.005\n",
    "        # lr = 0.001\n",
    "        \n",
    "        opt = optim.AdamW(model.parameters(), lr=lr)\n",
    "        train_ds = NeedleHaystackDataset(size=10000, min_len=32, max_len=128, vocab_size=VOCAB_SIZE)\n",
    "        train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, collate_fn=pad_collate_fn, shuffle=True)\n",
    "        \n",
    "        model.train()\n",
    "        for ep in range(EPOCHS):\n",
    "            loss_list = []\n",
    "            for batch in train_loader:\n",
    "                x, y = batch['input_ids'].to(DEVICE), batch['labels'].to(DEVICE)\n",
    "                \n",
    "                # --- HF-STYLE FORWARD ---\n",
    "                # HF models can calculate loss internally if labels are passed\n",
    "                outputs = model(input_ids=x, labels=y)\n",
    "                \n",
    "                if hasattr(outputs, 'loss') and outputs.loss is not None:\n",
    "                    loss = outputs.loss\n",
    "                else:\n",
    "                    # Fallback for models that don't compute loss internally\n",
    "                    logits = outputs.logits if hasattr(outputs, 'logits') else outputs\n",
    "                    shift_logits = logits[:, :-1, :].contiguous()\n",
    "                    shift_labels = y[:, 1:].contiguous()\n",
    "                    loss = F.cross_entropy(\n",
    "                        shift_logits.view(-1, VOCAB_SIZE),\n",
    "                        shift_labels.view(-1)\n",
    "                    )\n",
    "                \n",
    "                opt.zero_grad()\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                loss_list.append(loss.item())\n",
    "                \n",
    "            if (ep+1) % 5 == 0:\n",
    "                print(f\"   Ep {ep+1}: Loss {np.mean(loss_list):.4f}\")\n",
    "\n",
    "        # --- 2. Benchmark Evaluation ---\n",
    "        print(f\"   ðŸ§ª Benchmarking {name}...\")\n",
    "        model.eval()\n",
    "        \n",
    "        # Outer progress bar for context lengths\n",
    "        length_pbar = tqdm(test_lengths, desc=f\"Evaluating {name}\")\n",
    "        for L in length_pbar:\n",
    "            length_pbar.set_description(f\"Eval {name} [Len: {L}]\")\n",
    "            gc.collect(); torch.cuda.empty_cache()\n",
    "            \n",
    "            eval_batch = 64 if L <= 1024 else (8 if L <= 8192 else 2)\n",
    "            test_ds = NeedleHaystackDataset(size=100, min_len=L, max_len=L, vocab_size=VOCAB_SIZE, depth=0.5)\n",
    "            test_loader = DataLoader(test_ds, batch_size=eval_batch, collate_fn=pad_collate_fn)\n",
    "            \n",
    "            hits, total = 0, 0\n",
    "            try:\n",
    "                with torch.no_grad():\n",
    "                    # Inner progress bar for evaluation batches\n",
    "                    for batch in tqdm(test_loader, desc=f\"L={L}\", leave=False):\n",
    "                        x, y = batch['input_ids'].to(DEVICE), batch['labels'].to(DEVICE)\n",
    "                        outputs = model(x)\n",
    "                        logits = outputs.logits if hasattr(outputs, 'logits') else outputs\n",
    "                        \n",
    "                        for i in range(x.size(0)):\n",
    "                            valid = (y[i] != -100).nonzero(as_tuple=True)[0]\n",
    "                            if len(valid) == 0: continue\n",
    "                            target_pos = valid[-1].item()\n",
    "                            pred = logits[i, target_pos-1].argmax().item()\n",
    "                            if pred == y[i, target_pos].item(): hits += 1\n",
    "                            total += 1\n",
    "                            \n",
    "                acc = hits / total if total > 0 else 0\n",
    "                results[name].append(acc)\n",
    "                print(f\"Length {L}: Acc {acc * 100:.2f}%\")\n",
    "                # Update the outer bar with the current accuracy\n",
    "                length_pbar.set_postfix(acc=f\"{acc:.1%}\")\n",
    "                \n",
    "            except RuntimeError as e:\n",
    "                if \"out of memory\" in str(e).lower():\n",
    "                    results[name].append(0.0)\n",
    "                    length_pbar.set_postfix(status=\"OOM\")\n",
    "                else:\n",
    "                    raise e\n",
    "                    \n",
    "    # --- 3. Visualization ---\n",
    "    # (Same as your provided code, just use results dictionary)\n",
    "    # ...\n",
    "    print(\"\\nðŸ“Š Generating Figure...\")\n",
    "    plt.figure(figsize=(12, 7), dpi=150)\n",
    "    \n",
    "    # Custom styling for professional look\n",
    "    markers = {\n",
    "        \"Long-LLM (HF)\": \"o\",  # Circle\n",
    "        \"Mamba (HF)\": \"s\",     # Square\n",
    "    }\n",
    "    colors = {\n",
    "        \"Long-LLM (HF)\": \"#1f77b4\", # Deep Blue\n",
    "        \"Mamba (HF)\": \"#ff7f0e\",    # Bright Orange\n",
    "    }\n",
    "\n",
    "    for name, accs in results.items():\n",
    "        # Plotting results\n",
    "        plt.plot(\n",
    "            test_lengths, \n",
    "            accs, \n",
    "            marker=markers.get(name, \"d\"), \n",
    "            color=colors.get(name, \"#7f7f7f\"), \n",
    "            linewidth=2.5, \n",
    "            markersize=8,\n",
    "            label=name,\n",
    "            alpha=0.8\n",
    "        )\n",
    "        \n",
    "    # Formatting the X-axis for context lengths\n",
    "    plt.xscale('log', base=2)\n",
    "    x_labels = [str(l) if l < 1024 else f\"{l//1024}k\" for l in test_lengths]\n",
    "    plt.xticks(test_lengths, x_labels)\n",
    "    \n",
    "    # Labels and Legend\n",
    "    plt.xlabel(\"Context Length (Number of Tokens)\", fontsize=12, fontweight='bold')\n",
    "    plt.ylabel(\"Needle Retrieval Accuracy\", fontsize=12, fontweight='bold')\n",
    "    plt.title(\"NIAH Benchmark: Long-LLM vs. Mamba vs. GPT-2\\n(Fixed 32k Window Scaling)\", fontsize=14, pad=20)\n",
    "    \n",
    "    plt.ylim(-0.05, 1.05)\n",
    "    plt.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n",
    "    plt.legend(frameon=True, loc='lower left', fontsize=10)\n",
    "    \n",
    "    # Adding a target line for 100% accuracy\n",
    "    plt.axhline(y=1.0, color='black', linestyle='--', alpha=0.1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"niah_results.png\")\n",
    "    plt.show()\n",
    "    print(\"âœ… Visualization saved as 'benchmark_results.png'.\")\n",
    "\n",
    "run_benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8785b7d4-078b-4a96-8a6b-227bd5206b4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (MyProject)",
   "language": "python",
   "name": "myproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
