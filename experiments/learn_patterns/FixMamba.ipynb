{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41179ff7-c06a-4af3-95ce-c8565b042fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mamba_ssm import MambaLMHeadModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bb6d5e-b7d8-49af-a3cb-632d022b470a",
   "metadata": {},
   "source": [
    "### Declaring the Config Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbaa9e56-2153-46e6-a56d-6ca4feb89c04",
   "metadata": {},
   "source": [
    "I choose to keep the config class from the mamba_ssm default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4eb5d9d2-469f-4efc-b28d-b4ef8c1cc13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "\n",
    "@dataclass \n",
    "class MambaConfig:\n",
    "    d_model: int = 2560\n",
    "    d_intermediate: int = 0\n",
    "    n_layer: int = 64\n",
    "    vocab_size: int = 50277\n",
    "    ssm_cfg: dict = field(default_factory=dict)\n",
    "    attn_layer_idx: list = field(default_factory=list)\n",
    "    attn_cfg: dict = field(default_factory=dict)\n",
    "    rms_norm: bool = True\n",
    "    residual_in_fp32: bool = True\n",
    "    fused_add_norm: bool = True\n",
    "    pad_vocab_size_multiple: int = 8\n",
    "    tie_embeddings: bool = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dcb166a-e837-40c5-bb64-7b2f302fb357",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 128\n",
    "DIM = 64\n",
    "LAYERS = 2\n",
    "HEADS = 4\n",
    "\n",
    "mamba_config = MambaConfig(\n",
    "    d_model = DIM,\n",
    "    n_layer = LAYERS, \n",
    "    vocab_size = VOCAB_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "876d84c7-49bf-4f08-a572-8194e3348eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MambaConfig(d_model=64, d_intermediate=0, n_layer=2, vocab_size=128, ssm_cfg={}, attn_layer_idx=[], attn_cfg={}, rms_norm=True, residual_in_fp32=True, fused_add_norm=True, pad_vocab_size_multiple=8, tie_embeddings=True)\n"
     ]
    }
   ],
   "source": [
    "print(mamba_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328be078-2d8c-4edf-b5df-31cfe1ece1f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Long Attention",
   "language": "python",
   "name": "longattention"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
