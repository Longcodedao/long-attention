{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf71aaa9-2e3f-45f0-a7a7-0fc9482f0c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8ece4b6-ac2f-4602-abf0-ad537ddcdcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim \n",
    "import math\n",
    "from model.long import chunked_parallel_scan, recurrent_scan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71cdcc09-1051-48c3-8dc7-73b9490f1fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LongAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        self.d_model = config.hidden_size\n",
    "        self.num_heads = config.num_heads\n",
    "        self.head_dim = self.d_mode // self.num_heads \n",
    "\n",
    "        # Projections \n",
    "        self.q_proj = nn.Linear(self.d_model, self.d_model, bias = False)\n",
    "        self.k_proj = nn.Linear(self.d_model, self.d_model, bias = False)\n",
    "        self.v_proj = nn.Linear(self.d_model, self.d_model, bias = False)\n",
    "\n",
    "        # Local Convolution\n",
    "        self.conv = nn.Conv1d(self.d_model, \n",
    "                              self.d_model,\n",
    "                              kernel_size = config.conv_kernel,\n",
    "                              groups = self.d_model,\n",
    "                              padding = config.conv_kernel - 1)\n",
    "\n",
    "        # Gates \n",
    "        self.input_gate_proj = nn.Linear(self.d_model, self.d_model, bias = True)\n",
    "        self.output_gate_proj = nn.Linear(self.d_model, self.d_model, bias = True)\n",
    "        self.gamma_proj = nn.Linear(self.d_model, self.num_heads, bias = True)\n",
    "\n",
    "        self.o_proj = nn.Linear(self.d_model, self.d_model)\n",
    "\n",
    "        self.v_norm = nn.LayerNorm(self.head_dim)\n",
    "        self.grp_norm = nn.GroupNorm(self.num_heads, self.d_model)\n",
    "        self.mem_norm = nn.LayerNorm(self.head_dim)\n",
    "\n",
    "        \n",
    "        self.reset_parameters()\n",
    "\n",
    "    def init_decays(self):\n",
    "        \"\"\"\n",
    "        Initialize Gamma decays for training\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            min_decay = 0.5 \n",
    "            max_decay = 0.9999\n",
    "\n",
    "            target_decays = 1 - torch.exp(\n",
    "                torch.linspace(\n",
    "                    math.log(1 - min_decay),\n",
    "                    math.log(1 - max_decay),\n",
    "                    self.num_heads\n",
    "                )\n",
    "            )\n",
    "\n",
    "            gamma_bias_init = torch.log(target_decays / (1 - target_decays))\n",
    "            self.gamma_proj.bias.copy_(gamma_bias_init)\n",
    "            nn.init.zeros_(self.gamma_proj.weight)\n",
    "            \n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        \"\"\"\n",
    "        Custom initialization logic.\n",
    "        \"\"\"\n",
    "        # 1. Projections\n",
    "        nn.init.normal_(self.q_proj.weight, std = 0.02)\n",
    "        nn.init.normal_(self.k_proj.weight, std = 0.02)\n",
    "        nn.init.normal_(self.v_proj.weight, std = 0.02)\n",
    "        nn.init.normal_(self.o_proj.weight, std = 0.02)\n",
    "\n",
    "        # Takes the initial gate bias to the initialize value\n",
    "        # Prevents initial instability\n",
    "        nn.init.zeros_(self.input_gate_proj.weight)\n",
    "        nn.init.constant_(self.input_gate_proj.bias, self.config.gate_init_bias)\n",
    "\n",
    "        # Gamma Initialization\n",
    "        self.init_decays()\n",
    "\n",
    "        # Output Gate: Start NEUTRAL (Bias 0.0)\n",
    "        nn.init.constant_(self.output_gate_proj.bias, 0.0)\n",
    "\n",
    "        # Scales the contribution of each layer relative to the depth of \n",
    "        # the model\n",
    "        nn.init.normal_(self.o_proj.weight, \n",
    "                        std = 0.02 / math.sqrt(self.config.num_hidden_layers))\n",
    "\n",
    "    def forward(self, x, state = None):\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        # --- 1. Local Convolution (Merged Logic) ---\n",
    "        if state is not None:\n",
    "            rnn_state, conv_cache = state \n",
    "            x_t = x.transpose(1, 2).contiguous()\n",
    "\n",
    "            # Concatenate history [old_cache, new_input]\n",
    "            conv_window = torch.cat([conv_cache, x_t], dim = 2)\n",
    "            conv_window = conv_window.contiguous()\n",
    "\n",
    "            # If (Cache + Input) < Kernel Size, we must pad left to run the convolution\n",
    "            # This happens when processing the very first few tokens of a generation\n",
    "            window_len = conv_window.shape[-1]\n",
    "\n",
    "            if window_len < self.config.conv_kernel:\n",
    "                pad_amt = self.config.conv_kernel - window_len\n",
    "\n",
    "                # Pad let (zeros) to reach the kernel size\n",
    "                conv_window = F.pad(conv_window, (pad_amt, 0)).contiguous()\n",
    "\n",
    "            x_conv = F.conv1d(\n",
    "                conv_window, \n",
    "                self.conv.weight,\n",
    "                bias = self.conv.bias, \n",
    "                padding = 0, \n",
    "                groups = self.d_model\n",
    "            )\n",
    "\n",
    "            x_conv = x_conv[:, :, :T].tranpose(1, 2).contiguous()\n",
    "\n",
    "            # Update cache: Keep the last (kernel_size - 1) elements\n",
    "            if self.config.conv_kernel > 1:\n",
    "                new_conv_cache = conv_window[:, :, 1:].contiguous()\n",
    "            else:\n",
    "                # Should ideally empty if kernel = 1\n",
    "                new_conv_cache = conv_cache\n",
    "\n",
    "        else:\n",
    "            x_input = x.transpose(1, 2).contiguous()\n",
    "\n",
    "            pad_amt = self.config.conv_kernel -1\n",
    "            x_padded = F.pad(x_input, (pad_amt, 0))\n",
    "\n",
    "            x_conv = F.conv1d(\n",
    "                x_padded,\n",
    "                self.conv.weight,\n",
    "                bias = self.conv.bias,\n",
    "                padding = 0,\n",
    "                grouops = self.d_model\n",
    "            )\n",
    "            \n",
    "            x_conv = x_conv[:, :, :T].transpose(1, 2).contiguous()\n",
    "\n",
    "            rnn_state = None\n",
    "            if self.config.conv_kernel > 1:\n",
    "                new_conv_cache = x.transpose(1, 2)[:, :, -(self.config.conv_kernel-1):].contiguous()\n",
    "            else:\n",
    "                new_conv_cache = None\n",
    "\n",
    "\n",
    "        x_conv = F.silu(x_conv)\n",
    "\n",
    "        # --- 2. Projections & Stability ---\n",
    "        q = self.q_proj(x).view(B, T, self.num_heads, self.head_dim)\n",
    "        k = self.k_proj(x).view(B, T, self.num_heads, self.head_dim)\n",
    "        v = self.v_proj(x).view(B, T, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Normalize Q, K for stable matching (Cosine Attention style)\n",
    "        # Normalize V for stable accumulation\n",
    "        q = F.normalize(q, p = 2, dim = -1)\n",
    "        k = F.normalize(k, p = 2, dim = -1)\n",
    "        v = self.v_norm(v)\n",
    "\n",
    "        # --- 3. Gating (Depends on the Conv1d Input ???) ---\n",
    "        # Gating mechanism all depends on the Convolution\n",
    "        i_project = self.input_gate_proj(x_conv)\n",
    "        i_gate = torch.sigmoid(i_project).view(B, T, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Gamma decay\n",
    "        gamma = torch.sigmoid(self.gamma_proj(x_conv)).view(B, T, self.num_heads, self.head_dim)\n",
    "\n",
    "        # --- 4. Scan Logic ---\n",
    "        if rnn_state is None:\n",
    "            mem = chunked_parallel_scan(k, v, i_gate, gamma)\n",
    "\n",
    "            # Save the final state for potential continuation\n",
    "            next_rnn_state = mem[:, -1].detach().clone()\n",
    "        else:\n",
    "            mem, next_rnn_state = recurrent_scan(k, v, i_gate,\n",
    "                                                 gamma, rnn_state)\n",
    "\n",
    "        # --- 5. Output Projection & Gating ---\n",
    "        # Normalize memory state before combining with Query\n",
    "        mem_out = self.mem_norm(mem)\n",
    "\n",
    "        # Attention: Q * Memory\n",
    "        scale = 1.0 / math.sqrt(self.head_dim)\n",
    "        out = (mem_out * q * scale).reshape(B, T, C)\n",
    "\n",
    "        # Group Normalization on the combined output\n",
    "        out = self.grp_norm(out.reshape(B*T, C)).view(B, T, C)\n",
    "\n",
    "        # Outut gating\n",
    "        out = out * torch.sigmoid(self.output_gate_proj(x_conv))\n",
    "\n",
    "        return self.o_proj(out), (next_rnn_state, new_conv_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8676e9-f287-4f94-936e-46a7e16961cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Long Attention",
   "language": "python",
   "name": "longattention"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
