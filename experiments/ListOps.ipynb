{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcd2b333-ed94-4680-ba63-9609aa1bf731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Ensure the parent directory is in the path to find 'model/' folder\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# --- Imports from your structure ---\n",
    "try:\n",
    "    from model.long import LongConfig, LongForCausalLM\n",
    "    from transformers import MambaConfig, MambaForCausalLM\n",
    "    from transformers import GPT2Config, GPT2LMHeadModel\n",
    "except ImportError:\n",
    "    print(\"Warning: Custom model modules (Holo/Long) not found. Ensure 'model' folder is in path.\")\n",
    "\n",
    "# Check for GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Running on device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94e76c0e-ae2d-46b1-80a5-b2742a507e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed = 42):\n",
    "    \"\"\"\n",
    "    Sets the seed for reproducibility across all libraries.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # Ensure deterministic behavior for CUDA (slightly slower but \n",
    "    # reproducible)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    print(f\"ðŸŒ± Seed set to {seed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a644bf5-b4f9-4832-a8e0-71dbf6358470",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class ListOpsGenerator:\n",
    "    def __init__(self, max_depth=5):\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "        # <PAD> is at index 0\n",
    "        # MIN, MAX, MED (median), SM (sum modulo 10)\n",
    "        self.tokens = [\"<PAD>\", \"[\", \"]\", \"MIN\", \"MAX\", \"MED\", \"SM\"] + \\\n",
    "                         [str(i) for i in range(10)]\n",
    "        self.vocab = {t: i for i, t in enumerate(self.tokens)}\n",
    "        self.rev_vocab = {i: t for t, i in self.vocab.items()}\n",
    "        \n",
    "        self.PAD_TOKEN_ID = 0      # Use this for INPUTS (Embedding layer)\n",
    "        self.IGNORE_INDEX = -100   # Use this for TARGETS (Loss function) - Not used for single answer, but good practice\n",
    "\n",
    "    \n",
    "    def generate_tree(self, current_depth):\n",
    "        # Base case: depth 0 or small random chance to stop early\n",
    "        if current_depth == 0 or random.random() < 0.1:\n",
    "            return str(random.randint(0, 9))\n",
    "        \n",
    "        op = random.choice([\"MIN\", \"MAX\", \"MED\", \"SM\"])\n",
    "        # Reduce max children slightly to keep length manageable\n",
    "        num_children = random.randint(2, 4) \n",
    "        children = [self.generate_tree(current_depth - 1) for _ in range(num_children)]\n",
    "        return f\"[{op} \" + \" \".join(children) + \"]\"\n",
    "\n",
    "    \n",
    "    def solve(self, sequence):\n",
    "        # FIX: Add spaces around brackets so they split into separate tokens!\n",
    "        # [MIN 8 4] -> ( MIN 8 4 ) -> ['(', 'MIN', '8', '4', ')']\n",
    "        tokens = sequence.replace(\"[\", \" ( \").replace(\"]\", \" ) \").split()\n",
    "        \n",
    "        def parse(toks):\n",
    "            token = toks.pop(0)\n",
    "            if token == \"(\":\n",
    "                op = toks.pop(0)\n",
    "                vals = []\n",
    "                while toks[0] != \")\":\n",
    "                    vals.append(parse(toks))\n",
    "                toks.pop(0) # Remove )\n",
    "                \n",
    "                if op == \"MIN\": return min(vals)\n",
    "                if op == \"MAX\": return max(vals)\n",
    "                if op == \"MED\": return int(np.median(vals))\n",
    "                if op == \"SM\": return sum(vals) % 10\n",
    "            else:\n",
    "                return int(token)\n",
    "        try:\n",
    "            return parse(tokens.copy())\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Print the actual error message for debugging next time\n",
    "            print(f\"Solver failed on: {sequence} | Error: {e}\")\n",
    "            return None\n",
    "\n",
    "    \n",
    "    def generate_sample(self, target_length):\n",
    "        while True:\n",
    "            depth = random.randint(2, self.max_depth)\n",
    "            seq_str = self.generate_tree(depth)\n",
    "\n",
    "            # Tokenize\n",
    "            token_strs = seq_str.replace(\"[\", \" [ \").replace(\"]\", \" ] \").split()\n",
    "            tokens = [self.vocab[t] for t in token_strs]\n",
    "            \n",
    "            if len(tokens) <= target_length:\n",
    "                break\n",
    "        \n",
    "        answer = self.solve(seq_str)\n",
    "        if answer is None:\n",
    "            return self.generate_sample(target_length)\n",
    "\n",
    "        # 1. Input: The Sequence + 1 Padding token (placeholder for answer generation)\n",
    "        #    We assume the model sees \"]\" and then predicts the answer.\n",
    "        #    So we need an input position *after* \"]\" to hold the gradient for the answer.\n",
    "\n",
    "        # Current tokens: [ ... ]\n",
    "        # We append PAD to inputs so there is a position corresponding to the answer label.\n",
    "        input_ids = tokens + [self.PAD_TOKEN_ID]\n",
    "\n",
    "        # 2. Targets: Ignore everything until the very last token\n",
    "        target_ids = [self.IGNORE_INDEX] * len(tokens)\n",
    "        target_ids.append(self.vocab[str(answer)])\n",
    "\n",
    "        # 3. Padding to fixed length\n",
    "        padding_needed = target_length - len(input_ids)\n",
    "        if padding_needed > 0:\n",
    "            input_ids += [self.PAD_TOKEN_ID] * padding_needed\n",
    "            target_ids += [self.IGNORE_INDEX] * padding_needed\n",
    "        \n",
    "        return torch.tensor(input_ids, dtype=torch.long), torch.tensor(target_ids, dtype=torch.long)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07e4100e-01fa-4538-90ef-ba91a3718419",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListOpsStaticDataset(Dataset):\n",
    "    def __init__(self, generator, num_samples, length):\n",
    "        self.generator = generator\n",
    "        self.length = length\n",
    "        self.samples = []\n",
    "        \n",
    "        # PRE-GENERATE data so it stays fixed\n",
    "        print(f\"    ...Pre-generating {num_samples} samples of length {length}...\")\n",
    "        for _ in range(num_samples):\n",
    "            self.samples.append(self.generator.generate_sample(self.length))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)        \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return the saved sample (Input, Target)\n",
    "        return self.samples[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ab20dd3c-1c9d-4267-b374-33ddf64a4e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_and_display(generator, input_ids, target_ids):\n",
    "    # 1. Convert tensors to standard python lists\n",
    "    input_list = input_ids.tolist()\n",
    "    target_list = target_ids.tolist()\n",
    "\n",
    "    decoded_tokens = []\n",
    "    for tid in input_list:\n",
    "        if tid == generator.PAD_TOKEN_ID:\n",
    "            break\n",
    "        decoded_tokens.append(generator.rev_vocab.get(tid, \"<UNK>\"))\n",
    "\n",
    "    expression = \" \".join(decoded_tokens)\n",
    "\n",
    "    # The target tensor is mostly -100 (IGNORE_INDEX). We find the one valid value.\n",
    "    answer_token = None\n",
    "    for tid in target_list:\n",
    "        if tid != generator.IGNORE_INDEX:\n",
    "            answer_token = generator.rev_vocab.get(tid, \"<UNK>\")\n",
    "            break\n",
    "\n",
    "    # 4. Print\n",
    "    print(f\"ðŸ”¹ Expression: {expression}\")\n",
    "    if answer_token:\n",
    "        print(f\"ðŸ”¸ Target:     {answer_token}\")\n",
    "    else:\n",
    "        print(f\"ðŸ”¸ Target:     (None found - check data generation!)\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dcb73691-4e3c-4fb4-97e6-81ca37df9d03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<PAD>': 0,\n",
       " '[': 1,\n",
       " ']': 2,\n",
       " 'MIN': 3,\n",
       " 'MAX': 4,\n",
       " 'MED': 5,\n",
       " 'SM': 6,\n",
       " '0': 7,\n",
       " '1': 8,\n",
       " '2': 9,\n",
       " '3': 10,\n",
       " '4': 11,\n",
       " '5': 12,\n",
       " '6': 13,\n",
       " '7': 14,\n",
       " '8': 15,\n",
       " '9': 16}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "30c75864-2c05-4f3b-be5e-47f8ef0efc8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ...Pre-generating 100 samples of length 20...\n"
     ]
    }
   ],
   "source": [
    "generator = ListOpsGenerator(max_depth=5) # Adjust depth based on length if needed\n",
    "\n",
    "lops = ListOpsStaticDataset(generator, num_samples = 100, length = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3366ccc7-cdff-46b1-8b3b-64a04b71712f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1,  5, 12,  1,  6,  1,  6,  8, 14, 11,  2,  1,  4, 10, 15, 16,  7,  2,\n",
      "         2,  2,  0])\n",
      "tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100,   10])\n",
      "ðŸ”¹ Expression: [ MED 5 [ SM [ SM 1 7 4 ] [ MAX 3 8 9 0 ] ] ]\n",
      "ðŸ”¸ Target:     3\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "inp, out = lops[1]\n",
    "print(inp)\n",
    "print(out)\n",
    "\n",
    "decode_and_display(generator, inp, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c9a6a88f-47ea-4626-91d7-c2070bc07419",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "def get_model(model_name, vocab_size, max_seq_len, \n",
    "              hidden_dim=128, num_layers=6, num_heads=4, device='cuda'):\n",
    "    \"\"\"\n",
    "    Initializes models with flexible configuration.\n",
    "    Default (ListOps Baseline): DIM=128, LAYERS=6, HEADS=4\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure GPT-2/Transformer position embeddings fit the data\n",
    "    safe_max_pos = max(max_seq_len, 4096) \n",
    "\n",
    "    if model_name == \"Long-LLM\":\n",
    "        config = LongConfig(\n",
    "            vocab_size = vocab_size, \n",
    "            hidden_size = hidden_dim, \n",
    "            num_hidden_layers = num_layers, \n",
    "            num_heads = num_heads,\n",
    "            max_position_embeddings = safe_max_pos,\n",
    "            expansion_ratio = 8/3, \n",
    "            hybrid_ratio = 0,\n",
    "            gate_init_bias = 0.0,\n",
    "            # vital for long context \n",
    "            rope_base_dim = 1000000,\n",
    "        )\n",
    "        model = LongForCausalLM(config)\n",
    "\n",
    "    elif model_name == \"Mamba\":\n",
    "        config = MambaConfig(\n",
    "            vocab_size = vocab_size,\n",
    "            hidden_size = hidden_dim,\n",
    "            num_hidden_layers = num_layers,\n",
    "            ssm_cfg = {\"dropout\": 0.0},\n",
    "            \n",
    "            # Default state_size is 16 so we keep it here\n",
    "        )\n",
    "        model = MambaForCausalLM(config)\n",
    "\n",
    "    elif model_name == \"GPT-2\":\n",
    "        config = GPT2Config(\n",
    "            vocab_size = vocab_size, \n",
    "            n_positions = safe_max_pos, \n",
    "            n_embd = hidden_dim, \n",
    "            n_layer = num_layers, \n",
    "            n_head = num_heads,\n",
    "            resid_pdrop = 0.1, # Dropout when the model is large to prevent overfitting\n",
    "            embd_pdrop = 0.1, \n",
    "            attn_pdrop = 0.1, \n",
    "            use_cache = False\n",
    "        )\n",
    "        model = GPT2LMHeadModel(config)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {model_name}\")\n",
    "        \n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d4479de1-806e-4dba-8154-3a174f5bfcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_name, model, train_loader, \n",
    "                epochs=20, lr=5e-4, max_grad_norm=1.0,\n",
    "                pad_token = 0): \n",
    "    # Lowered LR\n",
    "    model.train()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        for batch_idx, (inputs, targets) in enumerate(loop):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "                        \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if model_name == \"Mamba\":\n",
    "                outputs = model(input_ids=inputs, labels=targets)\n",
    "            else:\n",
    "                # 0 is the Padding token\n",
    "                attention_mask = (inputs != pad_token).long()\n",
    "                outputs = model(input_ids=inputs, \n",
    "                                attention_mask=attention_mask, \n",
    "                                labels=targets)\n",
    "            \n",
    "            loss = outputs.loss\n",
    "\n",
    "            # Backprop\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            loop.set_postfix(loss=f\"{total_loss/(batch_idx+1):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f64645d6-5a8f-4f8a-a28e-a4445708fb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model_name, model, test_loader, generator,\n",
    "                   pad_token = 0,\n",
    "                   ignore_index = -100):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            if model_name == \"Mamba\":\n",
    "                outputs = model(input_ids=inputs, labels=targets)\n",
    "            else:\n",
    "                # 0 is the Padding token\n",
    "                attention_mask = (inputs != pad_token).long()\n",
    "                outputs = model(input_ids=inputs, \n",
    "                                attention_mask=attention_mask, \n",
    "                                labels=targets)\n",
    "\n",
    "            logits = outputs.logits            \n",
    "            # We constructed Targets specifically to align with Inputs.\n",
    "            # Input:  [A, B, C, PAD]\n",
    "            # Target: [-100, -100, -100, ANS]\n",
    "            # If your dataset ALREADY aligned them, we compare directly.\n",
    "\n",
    "            # Shift the HF way (Shift-by-one)\n",
    "            shift_logits = logits[:, :-1, :].contiguous()\n",
    "            shift_labels = targets[:, 1:].contiguous()\n",
    "            \n",
    "            # 3. Get predictions (B, Seq_Len)\n",
    "            preds = torch.argmax(shift_logits, dim=-1)\n",
    "            \n",
    "            # 4. Mask out the -100 positions in targets to calculate accuracy\n",
    "            # This ensures we only compare valid tokens\n",
    "            valid_mask = (shift_labels != ignore_index)\n",
    "            \n",
    "            # Compare only valid positions\n",
    "            correct_preds = (preds == shift_labels) & valid_mask\n",
    "            \n",
    "            correct += correct_preds.sum().item()\n",
    "            total += valid_mask.sum().item()\n",
    "    \n",
    "    return correct / total if total > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c19ae2d7-d453-4054-bc68-6ab77d3e4297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: Mamba\n",
      "------------------------------\n",
      "Total Parameters:     1,527,424\n",
      "Trainable Parameters: 1,527,424\n",
      "------------------------------\n",
      "Model name: Long-LLM\n",
      "------------------------------\n",
      "Total Parameters:     1,911,576\n",
      "Trainable Parameters: 1,911,576\n",
      "------------------------------\n",
      "Model name: GPT-2\n",
      "------------------------------\n",
      "Total Parameters:     1,842,176\n",
      "Trainable Parameters: 1,842,176\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "def print_model_stats(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    # print(f\"Model Structure:\\n{model}\\n\") # Optional: prints layers\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Total Parameters:     {total_params:,}\")\n",
    "    print(f\"Trainable Parameters: {trainable_params:,}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "vocab_size = 1000\n",
    "MODELS_TO_TEST = [\"Mamba\", \"Long-LLM\", \"GPT-2\"] \n",
    "for model_name in MODELS_TO_TEST:\n",
    "        # Clean Memory\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        # 2. Initialize Model\n",
    "        if (model_name == \"Long-LLM\"):\n",
    "            model = get_model(model_name, vocab_size, num_layers=6, max_seq_len = 512)\n",
    "        elif (model_name == \"GPT-2\"):\n",
    "            model = get_model(model_name, vocab_size, num_layers=6, max_seq_len = 512)\n",
    "        else:\n",
    "            model = get_model(model_name, vocab_size, num_layers=12, max_seq_len = 512)\n",
    "        print(f\"Model name: {model_name}\")\n",
    "        print_model_stats(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1d3e17-dba7-429c-a77e-4792c279ea7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Benchmarking Sequence Length: 512 ===\n",
      "    ...Pre-generating 10000 samples of length 512...\n",
      "    ...Pre-generating 1000 samples of length 512...\n",
      "  > Training Long-LLM...\n",
      "LongForCausalLM(\n",
      "  (long_model): LongModel(\n",
      "    (wte): Embedding(1000, 128)\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x LongBlock(\n",
      "        (attn): LongAttention(\n",
      "          (q_proj): Linear(in_features=128, out_features=128, bias=False)\n",
      "          (k_proj): Linear(in_features=128, out_features=128, bias=False)\n",
      "          (v_proj): Linear(in_features=128, out_features=128, bias=False)\n",
      "          (conv): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)\n",
      "          (input_gate_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (output_gate_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (gamma_proj): Linear(in_features=128, out_features=4, bias=True)\n",
      "          (o_proj): Linear(in_features=128, out_features=128, bias=False)\n",
      "          (v_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "          (grp_norm): GroupNorm(4, 128, eps=1e-05, affine=True)\n",
      "          (mem_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (mlp): LongMLP(\n",
      "          (w_gate): Linear(in_features=128, out_features=512, bias=False)\n",
      "          (w_val): Linear(in_features=128, out_features=512, bias=False)\n",
      "          (w_out): Linear(in_features=512, out_features=128, bias=False)\n",
      "        )\n",
      "        (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=128, out_features=1000, bias=False)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cabffabf5774a8ebf30ffd4a987a210",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3faa5de2c1f44638cfca8ec7658ab05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5abd383c787e4e848b941b65a9627076",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0defad10d6cf4772b8a66d7ca35135f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e9fb1df8fce4d48b130612318509b62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3249fbcf3e0741d1bb7524d05e31bbdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f3f4804a2554ff1a2d744a66f252afe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57a4a84b246b4815bdbf529047ae3379",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c444a55d31ac4b4ab0f1242b06b56964",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15d9d00072cd402b82f0feb429c593f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def run_benchmarks():\n",
    "    # SETTINGS\n",
    "    SEEDS = [42, 1337, 2024]\n",
    "    CONTEXT_LENGTHS = [512, 1024] # Example lengths\n",
    "    TRAIN_SAMPLES = 5000 \n",
    "    TEST_SAMPLES = 5000\n",
    "    EPOCHS = 8\n",
    "    \n",
    "    # Small of testing, increase for real results\n",
    "    # EPOCHS = 5\n",
    "    EPOCHS = 10\n",
    "    results = {}\n",
    "    \n",
    "    for seq_len in CONTEXT_LENGTHS:\n",
    "        print(f\"\\n=== Benchmarking Sequence Length: {seq_len} ===\")\n",
    "        \n",
    "        # 2. GENERATE DATA ONCE PER LENGTH (Fairness)\n",
    "        generator = ListOpsGenerator(max_depth=5) # Adjust depth based on length if needed\n",
    "        \n",
    "        train_dataset = ListOpsStaticDataset(generator, TRAIN_SAMPLES, seq_len)\n",
    "        test_dataset = ListOpsStaticDataset(generator, TEST_SAMPLES, seq_len)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "    \n",
    "        current_results = {}\n",
    "        \n",
    "        # 3. TEST MODELS ON SAME DATA\n",
    "        for model_name in [\"Long-LLM\", \"Mamba\", \"GPT-2\"]:\n",
    "            print(f\"  > Training {model_name}...\")\n",
    "            \n",
    "            # 2. Initialize Model\n",
    "            model = get_model(model_name, \n",
    "                              vocab_size, \n",
    "                              max_seq_len=seq_len, \n",
    "                              hidden_dim=128).to(device)\n",
    "            \n",
    "            if seq_len == CONTEXT_LENGTHS[0]: # Print stats only once\n",
    "                print_model_stats(model, model_name)     \n",
    "                \n",
    "            # Train\n",
    "            train_model(model_name, model, train_loader, epochs=10, lr=5e-4) # 10 epochs is enough if data is 10k\n",
    "            \n",
    "            # Evaluate\n",
    "            acc = evaluate_model(model_name, model, test_loader, generator)\n",
    "            print(f\"  >> {model_name} Accuracy: {acc:.2%}\")\n",
    "            \n",
    "            # Save result\n",
    "            if model_name not in results: results[model_name] = []\n",
    "            results[model_name].append(acc)\n",
    "            \n",
    "            # Cleanup\n",
    "            del model\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ca37ef-44a8-4557-8f72-7ac441aa2ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plotting Results ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['b', 'g', 'r', 'c', 'm']\n",
    "markers = ['o', 's', '^', 'D', 'v']\n",
    "\n",
    "for idx, model_name in enumerate(results):\n",
    "    # Ensure we only plot if we have results (in case of OOM stops)\n",
    "    if len(results[model_name]) == len(CONTEXT_LENGTHS):\n",
    "        plt.plot(CONTEXT_LENGTHS, results[model_name], \n",
    "                 label=model_name,\n",
    "                 color=colors[idx % len(colors)],\n",
    "                 marker=markers[idx % len(markers)],\n",
    "                 linewidth=2,\n",
    "                 markersize=8)\n",
    "\n",
    "plt.xlabel(\"Sequence Length\", fontsize=12, fontweight='bold')\n",
    "plt.ylabel(\"Accuracy\", fontsize=12, fontweight='bold')\n",
    "plt.title(f\"ListOps Benchmark (Fixed Data)\", fontsize=14, pad=20)\n",
    "plt.ylim(-0.05, 1.05)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"listops.jpg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcc2f5c-e099-42c7-93c0-da2f7d709e3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Long Attention",
   "language": "python",
   "name": "longattention"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
