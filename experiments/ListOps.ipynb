{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcd2b333-ed94-4680-ba63-9609aa1bf731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Ensure the parent directory is in the path to find 'model/' folder\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# --- Imports from your structure ---\n",
    "try:\n",
    "    from model.long import LongConfig, LongForCausalLM\n",
    "    from transformers import MambaConfig, MambaForCausalLM\n",
    "    from transformers import GPT2Config, GPT2LMHeadModel\n",
    "except ImportError:\n",
    "    print(\"Warning: Custom model modules (Holo/Long) not found. Ensure 'model' folder is in path.\")\n",
    "\n",
    "# Check for GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Running on device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a644bf5-b4f9-4832-a8e0-71dbf6358470",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class ListOpsGenerator:\n",
    "    def __init__(self, max_depth=5):\n",
    "        self.max_depth = max_depth\n",
    "        # <PAD> is at index 0\n",
    "        self.tokens = [\"<PAD>\", \"[\", \"]\", \"MIN\", \"MAX\", \"MED\", \"SM\"] + [str(i) for i in range(10)]\n",
    "        self.vocab = {t: i for i, t in enumerate(self.tokens)}\n",
    "        self.rev_vocab = {i: t for t, i in self.vocab.items()}\n",
    "        \n",
    "        # --- CRITICAL FIXES ---\n",
    "        self.PAD_TOKEN_ID = 0      # Use this for INPUTS (Embedding layer)\n",
    "        self.IGNORE_INDEX = -100   # Use this for TARGETS (Loss function) - Not used for single answer, but good practice\n",
    "\n",
    "    def generate_tree(self, current_depth):\n",
    "        # Base case: depth 0 or small random chance to stop early\n",
    "        if current_depth == 0 or random.random() < 0.1:\n",
    "            return str(random.randint(0, 9))\n",
    "        \n",
    "        op = random.choice([\"MIN\", \"MAX\", \"MED\", \"SM\"])\n",
    "        # Reduce max children slightly to keep length manageable\n",
    "        num_children = random.randint(2, 4) \n",
    "        children = [self.generate_tree(current_depth - 1) for _ in range(num_children)]\n",
    "        return f\"[{op} \" + \" \".join(children) + \"]\"\n",
    "\n",
    "    def solve(self, sequence):\n",
    "        # FIX: Add spaces around brackets so they split into separate tokens!\n",
    "        # [MIN 8 4] -> ( MIN 8 4 ) -> ['(', 'MIN', '8', '4', ')']\n",
    "        tokens = sequence.replace(\"[\", \" ( \").replace(\"]\", \" ) \").split()\n",
    "        \n",
    "        def parse(toks):\n",
    "            token = toks.pop(0)\n",
    "            if token == \"(\":\n",
    "                op = toks.pop(0)\n",
    "                vals = []\n",
    "                while toks[0] != \")\":\n",
    "                    vals.append(parse(toks))\n",
    "                toks.pop(0) # Remove )\n",
    "                \n",
    "                if op == \"MIN\": return min(vals)\n",
    "                if op == \"MAX\": return max(vals)\n",
    "                if op == \"MED\": return int(np.median(vals))\n",
    "                if op == \"SM\": return sum(vals) % 10\n",
    "            else:\n",
    "                return int(token)\n",
    "        try:\n",
    "            return parse(tokens.copy())\n",
    "        except Exception as e:\n",
    "            # Print the actual error message for debugging next time\n",
    "            print(f\"Solver failed on: {sequence} | Error: {e}\")\n",
    "            return None\n",
    "\n",
    "    def generate_sample(self, target_length):\n",
    "        while True:\n",
    "            depth = random.randint(2, self.max_depth)\n",
    "            seq_str = self.generate_tree(depth)\n",
    "            token_strs = seq_str.replace(\"[\", \" [ \").replace(\"]\", \" ] \").split()\n",
    "            tokens = [self.vocab[t] for t in token_strs]\n",
    "            \n",
    "            if len(tokens) <= target_length:\n",
    "                break\n",
    "        \n",
    "        answer = self.solve(seq_str)\n",
    "        if answer is None:\n",
    "            return self.generate_sample(target_length)\n",
    "\n",
    "        # Padding for Input\n",
    "        padding_needed = target_length - len(tokens)\n",
    "        input_ids = tokens + [self.PAD_TOKEN_ID] * padding_needed\n",
    "        \n",
    "        target_ids = [self.IGNORE_INDEX] * (len(tokens) - 1)  # Ignore intermediate tokens\n",
    "        target_ids.append(self.vocab[str(answer)])           # Target is the answer at the end of the sequence\n",
    "        target_ids += [self.IGNORE_INDEX] * padding_needed   # Ignore padding\n",
    "        \n",
    "        return torch.tensor(input_ids, dtype=torch.long), torch.tensor(target_ids, dtype=torch.long)\n",
    "        \n",
    "class ListOpsStaticDataset(Dataset):\n",
    "    def __init__(self, generator, num_samples, length):\n",
    "        self.generator = generator\n",
    "        self.length = length\n",
    "        self.samples = []\n",
    "        \n",
    "        # PRE-GENERATE data so it stays fixed\n",
    "        print(f\"    ...Pre-generating {num_samples} samples of length {length}...\")\n",
    "        for _ in range(num_samples):\n",
    "            self.samples.append(self.generator.generate_sample(self.length))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return the saved sample (Input, Target)\n",
    "        return self.samples[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9a6a88f-47ea-4626-91d7-c2070bc07419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_name, vocab_size, max_seq_len, \n",
    "              hidden_dim=128, num_layers=6, num_heads=4, device='cuda'):\n",
    "    \"\"\"\n",
    "    Initializes models with flexible configuration.\n",
    "    Default (ListOps Baseline): DIM=128, LAYERS=6, HEADS=4\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure GPT-2/Transformer position embeddings fit the data\n",
    "    safe_max_pos = max(max_seq_len, 4096) \n",
    "\n",
    "    if model_name == \"Long-LLM\":\n",
    "        config = LongConfig(\n",
    "            vocab_size = vocab_size, \n",
    "            hidden_size = hidden_dim, \n",
    "            num_hidden_layers = num_layers, \n",
    "            num_heads = num_heads,\n",
    "            max_position_embeddings = safe_max_pos,\n",
    "            expansion_ratio = 8/3, \n",
    "            hybrid_ratio = 0,\n",
    "            gate_init_bias = 0.0,\n",
    "        )\n",
    "        model = LongForCausalLM(config)\n",
    "\n",
    "    elif model_name == \"Mamba\":\n",
    "        config = MambaConfig(\n",
    "            vocab_size = vocab_size,\n",
    "            hidden_size = hidden_dim,\n",
    "            num_hidden_layers = num_layers,\n",
    "            ssm_cfg = {\"dropout\": 0.0},\n",
    "            \n",
    "            # Mamba state_size mặc định là 16, với ListOps có thể giữ nguyên\n",
    "        )\n",
    "        model = MambaForCausalLM(config)\n",
    "\n",
    "    elif model_name == \"GPT-2\":\n",
    "        config = GPT2Config(\n",
    "            vocab_size = vocab_size, \n",
    "            n_positions = safe_max_pos, \n",
    "            n_embd = hidden_dim, \n",
    "            n_layer = num_layers, \n",
    "            n_head = num_heads,\n",
    "            resid_pdrop = 0.1, # Nên để dropout nhẹ (0.1) khi train model lớn hơn\n",
    "            embd_pdrop = 0.1, \n",
    "            attn_pdrop = 0.1, \n",
    "            use_cache = False\n",
    "        )\n",
    "        model = GPT2LMHeadModel(config)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {model_name}\")\n",
    "        \n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4479de1-806e-4dba-8154-3a174f5bfcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_name, model, train_loader, epochs=20, lr=5e-4, max_grad_norm=1.0): # Lowered LR\n",
    "    model.train()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "    \n",
    "    vocab = train_loader.dataset.generator.vocab \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        for batch_idx, (inputs, targets) in enumerate(loop):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            # print(targets.shape)\n",
    "            # print(targets[0, -10:])\n",
    "            \n",
    "            batch_size, seq_len = inputs.shape\n",
    "            # --- FIX: Create Attention Mask ---\n",
    "            # 1 for valid tokens, 0 for PAD (index 0)\n",
    "            attention_mask = (inputs != 0).to(device)\n",
    "            # 2. Reshape to [batch_size, 1, 1, seq_len] \n",
    "            # This allows it to be broadcasted across heads and the 'query' dimension\n",
    "\n",
    "            if model_name != 'Mamba':\n",
    "                attention_mask = attention_mask.view(batch_size, 1, 1, seq_len)\n",
    "        \n",
    "                # 3. (Optional) Expand if your model specifically requires this exact shape\n",
    "                # Otherwise, PyTorch broadcasting handles this automatically in the model\n",
    "                attention_mask = attention_mask.expand(batch_size, 4, seq_len, seq_len)\n",
    "                # print(attention_mask)\n",
    "                \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Pass the mask to the model\n",
    "            outputs = model(inputs, attention_mask=attention_mask)\n",
    "            logits = outputs.logits # [B, Seq, Vocab]\n",
    "            \n",
    "            # (Rest of your loss calculation code remains the same...)\n",
    "            # target_token_ids = []\n",
    "            # for t in targets:\n",
    "            #     tid = vocab[str(t.item())]\n",
    "            #     target_token_ids.append(tid)\n",
    "            # target_token_ids = torch.tensor(target_token_ids).to(device)\n",
    "            \n",
    "            # last_logits = logits[:, -1, :] \n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "            # loss = criterion(last_logits, target_token_ids)\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            loop.set_postfix(loss=f\"{total_loss/(batch_idx+1):.4f}\")\n",
    "\n",
    "            \n",
    "def evaluate_model(model_name, model, test_loader, generator):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # 1. Re-create the same mask used in training\n",
    "            batch_size, seq_len = inputs.shape\n",
    "            attention_mask = (inputs != 0).to(device)\n",
    "\n",
    "            if model_name != \"Mamba\":\n",
    "                attention_mask = attention_mask.view(batch_size, 1, 1, seq_len)\n",
    "                attention_mask = attention_mask.expand(batch_size, 4, seq_len, seq_len)\n",
    "            \n",
    "            # 2. Forward pass\n",
    "            outputs = model(inputs, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            # 3. Get predictions (B, Seq_Len)\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            # 4. Mask out the -100 positions in targets to calculate accuracy\n",
    "            # This ensures we only compare valid tokens\n",
    "            valid_mask = (targets != -100)\n",
    "            \n",
    "            # Compare only valid positions\n",
    "            correct_preds = (preds == targets) & valid_mask\n",
    "            \n",
    "            correct += correct_preds.sum().item()\n",
    "            total += valid_mask.sum().item()\n",
    "    \n",
    "    return correct / total if total > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c19ae2d7-d453-4054-bc68-6ab77d3e4297",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The fast path is not available because one of `(selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)` is None. Falling back to the sequential implementation of Mamba, as use_mambapy is set to False. To install follow https://github.com/state-spaces/mamba/#installation for mamba-ssm and install the kernels library using `pip install kernels` or https://github.com/Dao-AILab/causal-conv1d for causal-conv1d. For the mamba.py backend, follow https://github.com/alxndrTL/mamba.py.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: Mamba\n",
      "------------------------------\n",
      "Total Parameters:     1,527,424\n",
      "Trainable Parameters: 1,527,424\n",
      "------------------------------\n",
      "Model name: Long-LLM\n",
      "------------------------------\n",
      "Total Parameters:     2,039,576\n",
      "Trainable Parameters: 2,039,576\n",
      "------------------------------\n",
      "Model name: GPT-2\n",
      "------------------------------\n",
      "Total Parameters:     1,842,176\n",
      "Trainable Parameters: 1,842,176\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "def print_model_stats(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    # print(f\"Model Structure:\\n{model}\\n\") # Optional: prints layers\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Total Parameters:     {total_params:,}\")\n",
    "    print(f\"Trainable Parameters: {trainable_params:,}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "vocab_size = 1000\n",
    "MODELS_TO_TEST = [\"Mamba\", \"Long-LLM\", \"GPT-2\"] \n",
    "for model_name in MODELS_TO_TEST:\n",
    "        # Clean Memory\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        # 2. Initialize Model\n",
    "        if (model_name == \"Long-LLM\"):\n",
    "            model = get_model(model_name, vocab_size, num_layers=6, max_seq_len = 512)\n",
    "        elif (model_name == \"GPT-2\"):\n",
    "            model = get_model(model_name, vocab_size, num_layers=6, max_seq_len = 512)\n",
    "        else:\n",
    "            model = get_model(model_name, vocab_size, num_layers=12, max_seq_len = 512)\n",
    "        print(f\"Model name: {model_name}\")\n",
    "        print_model_stats(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1d3e17-dba7-429c-a77e-4792c279ea7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Benchmarking Sequence Length: 512 ===\n",
      "    ...Pre-generating 10000 samples of length 512...\n",
      "    ...Pre-generating 1000 samples of length 512...\n",
      "  > Training Long-LLM...\n",
      "LongForCausalLM(\n",
      "  (long_model): LongModel(\n",
      "    (wte): Embedding(1000, 128)\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x LongBlock(\n",
      "        (attn): LongAttention(\n",
      "          (q_proj): Linear(in_features=128, out_features=128, bias=False)\n",
      "          (k_proj): Linear(in_features=128, out_features=128, bias=False)\n",
      "          (v_proj): Linear(in_features=128, out_features=128, bias=False)\n",
      "          (conv): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)\n",
      "          (input_gate_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (output_gate_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (gamma_proj): Linear(in_features=128, out_features=4, bias=True)\n",
      "          (o_proj): Linear(in_features=128, out_features=128, bias=False)\n",
      "          (v_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "          (grp_norm): GroupNorm(4, 128, eps=1e-05, affine=True)\n",
      "          (mem_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (mlp): LongMLP(\n",
      "          (w_gate): Linear(in_features=128, out_features=512, bias=False)\n",
      "          (w_val): Linear(in_features=128, out_features=512, bias=False)\n",
      "          (w_out): Linear(in_features=512, out_features=128, bias=False)\n",
      "        )\n",
      "        (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=128, out_features=1000, bias=False)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cabffabf5774a8ebf30ffd4a987a210",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3faa5de2c1f44638cfca8ec7658ab05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5abd383c787e4e848b941b65a9627076",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0defad10d6cf4772b8a66d7ca35135f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e9fb1df8fce4d48b130612318509b62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3249fbcf3e0741d1bb7524d05e31bbdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f3f4804a2554ff1a2d744a66f252afe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57a4a84b246b4815bdbf529047ae3379",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c444a55d31ac4b4ab0f1242b06b56964",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15d9d00072cd402b82f0feb429c593f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. PARAMETERS\n",
    "vocab_size = 1000\n",
    "# INCREASE THIS! 200 is too small.\n",
    "TRAIN_SAMPLES = 10000 \n",
    "TEST_SAMPLES = 1000\n",
    "CONTEXT_LENGTHS = [512, 1024, 2048] # Example lengths\n",
    "\n",
    "results = {}\n",
    "\n",
    "for seq_len in CONTEXT_LENGTHS:\n",
    "    print(f\"\\n=== Benchmarking Sequence Length: {seq_len} ===\")\n",
    "    \n",
    "    # 2. GENERATE DATA ONCE PER LENGTH (Fairness)\n",
    "    generator = ListOpsGenerator(max_depth=5) # Adjust depth based on length if needed\n",
    "    \n",
    "    train_dataset = ListOpsStaticDataset(generator, TRAIN_SAMPLES, seq_len)\n",
    "    test_dataset = ListOpsStaticDataset(generator, TEST_SAMPLES, seq_len)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "    \n",
    "    # 3. TEST MODELS ON SAME DATA\n",
    "    for model_name in [\"Long-LLM\", \"Mamba\", \"GPT-2\"]:\n",
    "        print(f\"  > Training {model_name}...\")\n",
    "        \n",
    "        # 2. Initialize Model\n",
    "        if (model_name == \"Long-LLM\"):\n",
    "            model = get_model(model_name, vocab_size, num_layers=6, max_seq_len = 512).to(device)\n",
    "            print(model)\n",
    "        elif (model_name == \"GPT-2\"):\n",
    "            model = get_model(model_name, vocab_size, num_layers=6, max_seq_len = 512).to(device)\n",
    "        else:\n",
    "            model = get_model(model_name, vocab_size, num_layers=12, max_seq_len = 512).to(device)\n",
    "        \n",
    "        # Train\n",
    "        train_model(model_name, model, train_loader, epochs=10, lr=5e-4) # 10 epochs is enough if data is 10k\n",
    "        \n",
    "        # Evaluate\n",
    "        acc = evaluate_model(model_name, model, test_loader, generator)\n",
    "        print(f\"  >> {model_name} Accuracy: {acc:.2%}\")\n",
    "        \n",
    "        # Save result\n",
    "        if model_name not in results: results[model_name] = []\n",
    "        results[model_name].append(acc)\n",
    "        \n",
    "        # Cleanup\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ca37ef-44a8-4557-8f72-7ac441aa2ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plotting Results ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['b', 'g', 'r', 'c', 'm']\n",
    "markers = ['o', 's', '^', 'D', 'v']\n",
    "\n",
    "for idx, model_name in enumerate(results):\n",
    "    # Ensure we only plot if we have results (in case of OOM stops)\n",
    "    if len(results[model_name]) == len(CONTEXT_LENGTHS):\n",
    "        plt.plot(CONTEXT_LENGTHS, results[model_name], \n",
    "                 label=model_name,\n",
    "                 color=colors[idx % len(colors)],\n",
    "                 marker=markers[idx % len(markers)],\n",
    "                 linewidth=2,\n",
    "                 markersize=8)\n",
    "\n",
    "plt.xlabel(\"Sequence Length\", fontsize=12, fontweight='bold')\n",
    "plt.ylabel(\"Accuracy\", fontsize=12, fontweight='bold')\n",
    "plt.title(f\"ListOps Benchmark (Fixed Data)\", fontsize=14, pad=20)\n",
    "plt.ylim(-0.05, 1.05)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"listops.jpg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcc2f5c-e099-42c7-93c0-da2f7d709e3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Long Attention",
   "language": "python",
   "name": "longattention"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
