{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcd2b333-ed94-4680-ba63-9609aa1bf731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Ensure the parent directory is in the path to find 'model/' folder\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# --- Imports from your structure ---\n",
    "try:\n",
    "    from model.holo import HoloConfig, HoloForCausalLM\n",
    "    from model.long import LongConfig, LongForCausalLM\n",
    "    from transformers import MambaConfig, MambaForCausalLM\n",
    "    from transformers import GPT2Config, GPT2LMHeadModel\n",
    "except ImportError:\n",
    "    print(\"Warning: Custom model modules (Holo/Long) not found. Ensure 'model' folder is in path.\")\n",
    "\n",
    "# Check for GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Running on device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a644bf5-b4f9-4832-a8e0-71dbf6358470",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class ListOpsGenerator:\n",
    "    def __init__(self, max_depth=5):\n",
    "        self.max_depth = max_depth\n",
    "        # <PAD> is at index 0\n",
    "        self.tokens = [\"<PAD>\", \"[\", \"]\", \"MIN\", \"MAX\", \"MED\", \"SM\"] + [str(i) for i in range(10)]\n",
    "        self.vocab = {t: i for i, t in enumerate(self.tokens)}\n",
    "        self.rev_vocab = {i: t for t, i in self.vocab.items()}\n",
    "        \n",
    "        # --- CRITICAL FIXES ---\n",
    "        self.PAD_TOKEN_ID = 0      # Use this for INPUTS (Embedding layer)\n",
    "        self.IGNORE_INDEX = -100   # Use this for TARGETS (Loss function) - Not used for single answer, but good practice\n",
    "\n",
    "    def generate_tree(self, current_depth):\n",
    "        # Base case: depth 0 or small random chance to stop early\n",
    "        if current_depth == 0 or random.random() < 0.1:\n",
    "            return str(random.randint(0, 9))\n",
    "        \n",
    "        op = random.choice([\"MIN\", \"MAX\", \"MED\", \"SM\"])\n",
    "        # Reduce max children slightly to keep length manageable\n",
    "        num_children = random.randint(2, 4) \n",
    "        children = [self.generate_tree(current_depth - 1) for _ in range(num_children)]\n",
    "        return f\"[{op} \" + \" \".join(children) + \"]\"\n",
    "\n",
    "    def solve(self, sequence):\n",
    "        # FIX: Add spaces around brackets so they split into separate tokens!\n",
    "        # [MIN 8 4] -> ( MIN 8 4 ) -> ['(', 'MIN', '8', '4', ')']\n",
    "        tokens = sequence.replace(\"[\", \" ( \").replace(\"]\", \" ) \").split()\n",
    "        \n",
    "        def parse(toks):\n",
    "            token = toks.pop(0)\n",
    "            if token == \"(\":\n",
    "                op = toks.pop(0)\n",
    "                vals = []\n",
    "                while toks[0] != \")\":\n",
    "                    vals.append(parse(toks))\n",
    "                toks.pop(0) # Remove )\n",
    "                \n",
    "                if op == \"MIN\": return min(vals)\n",
    "                if op == \"MAX\": return max(vals)\n",
    "                if op == \"MED\": return int(np.median(vals))\n",
    "                if op == \"SM\": return sum(vals) % 10\n",
    "            else:\n",
    "                return int(token)\n",
    "        try:\n",
    "            return parse(tokens.copy())\n",
    "        except Exception as e:\n",
    "            # Print the actual error message for debugging next time\n",
    "            print(f\"Solver failed on: {sequence} | Error: {e}\")\n",
    "            return None\n",
    "\n",
    "    def generate_sample(self, target_length):\n",
    "        while True:\n",
    "            depth = random.randint(2, self.max_depth)\n",
    "            seq_str = self.generate_tree(depth)\n",
    "            token_strs = seq_str.replace(\"[\", \" [ \").replace(\"]\", \" ] \").split()\n",
    "            tokens = [self.vocab[t] for t in token_strs]\n",
    "            \n",
    "            if len(tokens) <= target_length:\n",
    "                break\n",
    "        \n",
    "        answer = self.solve(seq_str)\n",
    "        if answer is None:\n",
    "            return self.generate_sample(target_length)\n",
    "\n",
    "        # Padding for Input\n",
    "        padding_needed = target_length - len(tokens)\n",
    "        input_ids = tokens + [self.PAD_TOKEN_ID] * padding_needed\n",
    "        \n",
    "        target_ids = [self.IGNORE_INDEX] * (len(tokens) - 1)  # Ignore intermediate tokens\n",
    "        target_ids.append(self.vocab[str(answer)])           # Target is the answer at the end of the sequence\n",
    "        target_ids += [self.IGNORE_INDEX] * padding_needed   # Ignore padding\n",
    "        \n",
    "        return torch.tensor(input_ids, dtype=torch.long), torch.tensor(target_ids, dtype=torch.long)\n",
    "        \n",
    "class ListOpsStaticDataset(Dataset):\n",
    "    def __init__(self, generator, num_samples, length):\n",
    "        self.generator = generator\n",
    "        self.length = length\n",
    "        self.samples = []\n",
    "        \n",
    "        # PRE-GENERATE data so it stays fixed\n",
    "        print(f\"    ...Pre-generating {num_samples} samples of length {length}...\")\n",
    "        for _ in range(num_samples):\n",
    "            self.samples.append(self.generator.generate_sample(self.length))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return the saved sample (Input, Target)\n",
    "        return self.samples[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9a6a88f-47ea-4626-91d7-c2070bc07419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_name, vocab_size, max_seq_len, \n",
    "              hidden_dim=128, num_layers=6, num_heads=4, device='cuda'):\n",
    "    \"\"\"\n",
    "    Initializes models with flexible configuration.\n",
    "    Default (ListOps Baseline): DIM=128, LAYERS=6, HEADS=4\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure GPT-2/Transformer position embeddings fit the data\n",
    "    safe_max_pos = max(max_seq_len, 4096) \n",
    "\n",
    "    if model_name == \"Long-LLM\":\n",
    "        config = LongConfig(\n",
    "            vocab_size = vocab_size, \n",
    "            hidden_size = hidden_dim, \n",
    "            num_hidden_layers = num_layers, \n",
    "            num_heads = num_heads,\n",
    "            max_position_embeddings = safe_max_pos,\n",
    "            expansion_ratio = 8/3, \n",
    "            hybrid_ratio = 4,\n",
    "            gate_init_bias = 0.0,\n",
    "        )\n",
    "        model = LongForCausalLM(config)\n",
    "\n",
    "    elif model_name == \"Mamba\":\n",
    "        config = MambaConfig(\n",
    "            vocab_size = vocab_size,\n",
    "            hidden_size = hidden_dim,\n",
    "            num_hidden_layers = num_layers,\n",
    "            ssm_cfg = {\"dropout\": 0.0},\n",
    "            \n",
    "            # Mamba state_size mặc định là 16, với ListOps có thể giữ nguyên\n",
    "        )\n",
    "        model = MambaForCausalLM(config)\n",
    "\n",
    "    elif model_name == \"GPT-2\":\n",
    "        config = GPT2Config(\n",
    "            vocab_size = vocab_size, \n",
    "            n_positions = safe_max_pos, \n",
    "            n_embd = hidden_dim, \n",
    "            n_layer = num_layers, \n",
    "            n_head = num_heads,\n",
    "            resid_pdrop = 0.1, # Nên để dropout nhẹ (0.1) khi train model lớn hơn\n",
    "            embd_pdrop = 0.1, \n",
    "            attn_pdrop = 0.1, \n",
    "            use_cache = False\n",
    "        )\n",
    "        model = GPT2LMHeadModel(config)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {model_name}\")\n",
    "        \n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4479de1-806e-4dba-8154-3a174f5bfcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_name, model, train_loader, epochs=20, lr=5e-4, max_grad_norm=1.0): # Lowered LR\n",
    "    model.train()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "    \n",
    "    vocab = train_loader.dataset.generator.vocab \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        for batch_idx, (inputs, targets) in enumerate(loop):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            # print(targets.shape)\n",
    "            # print(targets[0, -10:])\n",
    "            \n",
    "            batch_size, seq_len = inputs.shape\n",
    "            # --- FIX: Create Attention Mask ---\n",
    "            # 1 for valid tokens, 0 for PAD (index 0)\n",
    "            attention_mask = (inputs != 0).to(device)\n",
    "            # 2. Reshape to [batch_size, 1, 1, seq_len] \n",
    "            # This allows it to be broadcasted across heads and the 'query' dimension\n",
    "\n",
    "            if model_name != 'Mamba':\n",
    "                attention_mask = attention_mask.view(batch_size, 1, 1, seq_len)\n",
    "        \n",
    "                # 3. (Optional) Expand if your model specifically requires this exact shape\n",
    "                # Otherwise, PyTorch broadcasting handles this automatically in the model\n",
    "                attention_mask = attention_mask.expand(batch_size, 4, seq_len, seq_len)\n",
    "                # print(attention_mask)\n",
    "                \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Pass the mask to the model\n",
    "            outputs = model(inputs, attention_mask=attention_mask)\n",
    "            logits = outputs.logits # [B, Seq, Vocab]\n",
    "            \n",
    "            # (Rest of your loss calculation code remains the same...)\n",
    "            # target_token_ids = []\n",
    "            # for t in targets:\n",
    "            #     tid = vocab[str(t.item())]\n",
    "            #     target_token_ids.append(tid)\n",
    "            # target_token_ids = torch.tensor(target_token_ids).to(device)\n",
    "            \n",
    "            # last_logits = logits[:, -1, :] \n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "            # loss = criterion(last_logits, target_token_ids)\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            loop.set_postfix(loss=f\"{total_loss/(batch_idx+1):.4f}\")\n",
    "\n",
    "            \n",
    "def evaluate_model(model_name, model, test_loader, generator):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # 1. Re-create the same mask used in training\n",
    "            batch_size, seq_len = inputs.shape\n",
    "            attention_mask = (inputs != 0).to(device)\n",
    "\n",
    "            if model_name != \"Mamba\":\n",
    "                attention_mask = attention_mask.view(batch_size, 1, 1, seq_len)\n",
    "                attention_mask = attention_mask.expand(batch_size, 4, seq_len, seq_len)\n",
    "            \n",
    "            # 2. Forward pass\n",
    "            outputs = model(inputs, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            # 3. Get predictions (B, Seq_Len)\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            # 4. Mask out the -100 positions in targets to calculate accuracy\n",
    "            # This ensures we only compare valid tokens\n",
    "            valid_mask = (targets != -100)\n",
    "            \n",
    "            # Compare only valid positions\n",
    "            correct_preds = (preds == targets) & valid_mask\n",
    "            \n",
    "            correct += correct_preds.sum().item()\n",
    "            total += valid_mask.sum().item()\n",
    "    \n",
    "    return correct / total if total > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c19ae2d7-d453-4054-bc68-6ab77d3e4297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: Mamba\n",
      "------------------------------\n",
      "Total Parameters:     1,527,424\n",
      "Trainable Parameters: 1,527,424\n",
      "------------------------------\n",
      "Model name: Long-LLM\n",
      "------------------------------\n",
      "Total Parameters:     1,877,012\n",
      "Trainable Parameters: 1,877,012\n",
      "------------------------------\n",
      "Model name: GPT-2\n",
      "------------------------------\n",
      "Total Parameters:     1,842,176\n",
      "Trainable Parameters: 1,842,176\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "def print_model_stats(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    # print(f\"Model Structure:\\n{model}\\n\") # Optional: prints layers\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Total Parameters:     {total_params:,}\")\n",
    "    print(f\"Trainable Parameters: {trainable_params:,}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "vocab_size = 1000\n",
    "MODELS_TO_TEST = [\"Mamba\", \"Long-LLM\", \"GPT-2\"] \n",
    "for model_name in MODELS_TO_TEST:\n",
    "        # Clean Memory\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        # 2. Initialize Model\n",
    "        if (model_name == \"Long-LLM\"):\n",
    "            model = get_model(model_name, vocab_size, num_layers=6, max_seq_len = 512)\n",
    "        elif (model_name == \"GPT-2\"):\n",
    "            model = get_model(model_name, vocab_size, num_layers=6, max_seq_len = 512)\n",
    "        else:\n",
    "            model = get_model(model_name, vocab_size, num_layers=12, max_seq_len = 512)\n",
    "        print(f\"Model name: {model_name}\")\n",
    "        print_model_stats(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d1d3e17-dba7-429c-a77e-4792c279ea7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Benchmarking Sequence Length: 512 ===\n",
      "    ...Pre-generating 10000 samples of length 512...\n",
      "    ...Pre-generating 1000 samples of length 512...\n",
      "  > Training Long-LLM...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "240e6697a047463f86d948760e719607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c7bc6b90e914f83a4e383a274869e3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7099079b1b4d46dc83f6a81af2e484a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86623e490131491fa561ee3e674688a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdedbdfe28a34177ac6db73b603595ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08aa08a6d5fd44429f8d6bccaeab9f89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7beb990a5b0c49fea0110daad6fa86c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f77778c087be45cfa78238b75f1ea80f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f24c46d6d0614e9eac307aaef28432ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6307ac6abc384858b820f0c7d3c7c008",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  >> Long-LLM Accuracy: 36.30%\n",
      "  > Training Mamba...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ee9802770ce41b2a902ce5a63a70845",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed349da4650a4079ac8314472d7935e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c6f9f3d67a1442a8d0f66562d37089e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d954cc1dadeb4339b539fac615d07add",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12d013c98cdc448da2d7f4d27de3ba8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "555428d2626f4836960886e009eb5108",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6800bb421a574173aecb89dd0d5b7a41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc1d821d6ab2426b8fcc82d5e7d93f8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0d186cd249b48f5b2129a389c770112",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f582f07679e4f5cb65e4a7b482d2751",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  >> Mamba Accuracy: 31.50%\n",
      "  > Training GPT-2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf7cbf41b9784dcc926470d5e8b9fc6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d96a39caae645e9b5210abc6f3e03b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5d03f694883449cbcca40707c84aeac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9080b811b9643df8cb8e4d9a7233774",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89adfdfac95244d8929f5967adf9d3e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "541f4707df804cbda5dab9eb7207ab46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9015b89caba64b9085fa3fbf78ae1f51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "548395ed2b3f45b8a41ec4d5c4c2b8dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3078f2cc8bab486d8759f614718372a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9962f7b2735448ab612427aa1043a59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  >> GPT-2 Accuracy: 30.50%\n",
      "\n",
      "=== Benchmarking Sequence Length: 1024 ===\n",
      "    ...Pre-generating 10000 samples of length 1024...\n",
      "    ...Pre-generating 1000 samples of length 1024...\n",
      "  > Training Long-LLM...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "761963e35c124c2d8548cf7940eb8839",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63fd67e5489f478d92506f74b20c6ed9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6a8598e567841ddaf8576b814c847a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b86213dc0b7b4a0698ac3369cdeda346",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1797b92fb8c4e4283b48e1f7072df4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d364b94bd8e84ffe832052a8bc9151f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92c87c8f9374483c93537918c00583e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ca3785d200144538d8d9fb0ada0951c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af39a937be1c4495b43c4e74bb4bee3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7f46e97e41b4dd282a5d7fc810efa80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  >> Long-LLM Accuracy: 31.40%\n",
      "  > Training Mamba...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de494ff2473541fdae6c2b87deb7d700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f63c01cfd004c70b05981ec403dec2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d40cd8e6a4be4b8d9eb1cf4483428952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "573e2f5de302492c9b655e235a843a3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "202abfdb41cc4b26b25ae05085039e09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f284881f14574c099cd74544f40e8aaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2e9c31da84c48feb2b112adf63ef2cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3877307da9d84acb839f44fc22dda71b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39fd4e7d75484bd288579abd3534d8e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bb5ce265fe24daf8ce02344323a3ec0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  >> Mamba Accuracy: 27.60%\n",
      "  > Training GPT-2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dae09663d86459bb0e33b6fa61d033e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a024eba270e34e24845a29a5dc26c3df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b854527ec2e74b37a4913db6c91c980d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f304db0aa684702a3fccc44eaee0e54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2ac27493afe4735a4fb8f750ea036cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12a197d4790f437d8b28e20afd225797",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a8ee6eddc984681b8dda8cddbf13366",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "617e3ad663ae4185b2b7355b23dd21b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1021b9eddc254c6e9ac3b35201bf8f17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d897e758a69434499a578e5d230f57b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  >> GPT-2 Accuracy: 27.40%\n",
      "\n",
      "=== Benchmarking Sequence Length: 2048 ===\n",
      "    ...Pre-generating 10000 samples of length 2048...\n",
      "    ...Pre-generating 1000 samples of length 2048...\n",
      "  > Training Long-LLM...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6047a6ec5b24c5281c18cfc8cbff22e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cd67d8cb4e04cc3921f3186144533d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/10:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     32\u001b[39m     model = get_model(model_name, vocab_size, num_layers=\u001b[32m12\u001b[39m, max_seq_len = \u001b[32m512\u001b[39m).to(device)\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5e-4\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# 10 epochs is enough if data is 10k\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[32m     38\u001b[39m acc = evaluate_model(model_name, model, test_loader, generator)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 49\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model_name, model, train_loader, epochs, lr, max_grad_norm)\u001b[39m\n\u001b[32m     46\u001b[39m loss = criterion(logits.view(-\u001b[32m1\u001b[39m, logits.size(-\u001b[32m1\u001b[39m)), targets.view(-\u001b[32m1\u001b[39m))\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# loss = criterion(last_logits, target_token_ids)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n\u001b[32m     51\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/long-attention/.venv/lib/python3.12/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/long-attention/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/long-attention/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 1. PARAMETERS\n",
    "vocab_size = 1000\n",
    "# INCREASE THIS! 200 is too small.\n",
    "TRAIN_SAMPLES = 10000 \n",
    "TEST_SAMPLES = 1000\n",
    "CONTEXT_LENGTHS = [512, 1024, 2048] # Example lengths\n",
    "\n",
    "results = {}\n",
    "\n",
    "for seq_len in CONTEXT_LENGTHS:\n",
    "    print(f\"\\n=== Benchmarking Sequence Length: {seq_len} ===\")\n",
    "    \n",
    "    # 2. GENERATE DATA ONCE PER LENGTH (Fairness)\n",
    "    generator = ListOpsGenerator(max_depth=5) # Adjust depth based on length if needed\n",
    "    \n",
    "    train_dataset = ListOpsStaticDataset(generator, TRAIN_SAMPLES, seq_len)\n",
    "    test_dataset = ListOpsStaticDataset(generator, TEST_SAMPLES, seq_len)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "    \n",
    "    # 3. TEST MODELS ON SAME DATA\n",
    "    for model_name in [\"Long-LLM\", \"Mamba\", \"GPT-2\"]:\n",
    "        print(f\"  > Training {model_name}...\")\n",
    "        \n",
    "        # 2. Initialize Model\n",
    "        if (model_name == \"Long-LLM\"):\n",
    "            model = get_model(model_name, vocab_size, num_layers=6, max_seq_len = 512).to(device)\n",
    "        elif (model_name == \"GPT-2\"):\n",
    "            model = get_model(model_name, vocab_size, num_layers=6, max_seq_len = 512).to(device)\n",
    "        else:\n",
    "            model = get_model(model_name, vocab_size, num_layers=12, max_seq_len = 512).to(device)\n",
    "        \n",
    "        # Train\n",
    "        train_model(model_name, model, train_loader, epochs=10, lr=5e-4) # 10 epochs is enough if data is 10k\n",
    "        \n",
    "        # Evaluate\n",
    "        acc = evaluate_model(model_name, model, test_loader, generator)\n",
    "        print(f\"  >> {model_name} Accuracy: {acc:.2%}\")\n",
    "        \n",
    "        # Save result\n",
    "        if model_name not in results: results[model_name] = []\n",
    "        results[model_name].append(acc)\n",
    "        \n",
    "        # Cleanup\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ca37ef-44a8-4557-8f72-7ac441aa2ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plotting Results ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['b', 'g', 'r', 'c', 'm']\n",
    "markers = ['o', 's', '^', 'D', 'v']\n",
    "\n",
    "for idx, model_name in enumerate(results):\n",
    "    # Ensure we only plot if we have results (in case of OOM stops)\n",
    "    if len(results[model_name]) == len(CONTEXT_LENGTHS):\n",
    "        plt.plot(CONTEXT_LENGTHS, results[model_name], \n",
    "                 label=model_name,\n",
    "                 color=colors[idx % len(colors)],\n",
    "                 marker=markers[idx % len(markers)],\n",
    "                 linewidth=2,\n",
    "                 markersize=8)\n",
    "\n",
    "plt.xlabel(\"Sequence Length\", fontsize=12, fontweight='bold')\n",
    "plt.ylabel(\"Accuracy\", fontsize=12, fontweight='bold')\n",
    "plt.title(f\"ListOps Benchmark (Fixed Data)\", fontsize=14, pad=20)\n",
    "plt.ylim(-0.05, 1.05)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"listops.jpg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcc2f5c-e099-42c7-93c0-da2f7d709e3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (MyProject)",
   "language": "python",
   "name": "myproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
