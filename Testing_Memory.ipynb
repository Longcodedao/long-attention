{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "327e1755-2688-4e47-bd54-b96a2f6b9a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from model import HoloConfig, HoloModel, HoloForCausalLM, \\\n",
    "                  HoloBlock, HoloAttentionV1, HoloAttentionV2\n",
    "from transformers import PretrainedConfig\n",
    "import gc\n",
    "import contextlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45899143-5da2-4d4d-abd9-bce4d835503b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_params(num):\n",
    "    if num >= 1_000_000_000:\n",
    "        return f\"{num / 1_000_000_000:.2f} B\"\n",
    "    elif num >= 1_000_000:\n",
    "        return f\"{num / 1_000_000:.2f} M\"\n",
    "    else:\n",
    "        return f\"{num / 1_000:.2f} K\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32b71858-df5e-480f-b87f-08b46890369f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_peak_memory_mb():\n",
    "    \"\"\"Returns peak CUDA memory used in MB since last reset.\"\"\"\n",
    "    return torch.cuda.max_memory_allocated() / 1024 / 1024\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"Returns formatted string of parameter count.\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da087884-b761-4a49-a7a7-ca408518e058",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_detailed_stats(model, config, name):\n",
    "    total_params = count_parameters(model)\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    # Calculate Embedding Params\n",
    "    embed_params = sum(p.numel() for p in model.get_input_embeddings().parameters())\n",
    "    \n",
    "    # Calculate Non-Embedding Params\n",
    "    non_embed_params = total_params - embed_params\n",
    "    \n",
    "    return {\n",
    "        \"Name\": f\"Holo-{name.upper()}\",\n",
    "        \"Layers\": config.num_hidden_layers,\n",
    "        \"Model Dim\": config.d_model,\n",
    "        \"Holo Dim\": config.hd_dim,\n",
    "        \"Total Params\": format_params(total_params),\n",
    "        \"Non-Embed Params\": format_params(non_embed_params),\n",
    "        \"Exact Count\": total_params\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55b3ddd0-01ac-4341-a14f-eb276c92fa47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================================================\n",
      "Model           | Layers | d_model  | hd_dim   | Total Params | Active Params\n",
      "-------------------------------------------------------------------------------------\n",
      "Holo-SMALL      | 12     | 768      | 3072     | 180.26 M     | 141.66 M    \n",
      "=====================================================================================\n",
      "Holo-MEDIUM     | 24     | 1024     | 8192     | 857.04 M     | 805.58 M    \n",
      "=====================================================================================\n",
      "Holo-LARGE      | 36     | 1280     | 10240    | 1.95 B       | 1.89 B      \n",
      "=====================================================================================\n",
      "\n",
      "* 'Active Params' excludes the vocabulary embedding matrix (which is static lookup).\n"
     ]
    }
   ],
   "source": [
    "print(f\"{'='*85}\")\n",
    "print(f\"{'Model':<15} | {'Layers':<6} | {'d_model':<8} | {'hd_dim':<8} | {'Total Params':<12} | {'Active Params':<12}\")\n",
    "print(f\"{'-'*85}\")\n",
    "\n",
    "sizes = [\"small\", \"medium\", \"large\"]\n",
    "\n",
    "for size in sizes:\n",
    "    config = HoloConfig.from_preset(size, use_version = 2)\n",
    "\n",
    "    try: \n",
    "        with torch.device(\"meta\"):\n",
    "            model = HoloForCausalLM(config)\n",
    "    except:\n",
    "        # Fallback to CPU if meta device fails (older pytorch)\n",
    "        model = HoloForCausalLM(config)\n",
    "\n",
    "    # 3. Get Stats\n",
    "    stats = get_detailed_stats(model, config, size)\n",
    "    print(f\"{stats['Name']:<15} | {stats['Layers']:<6} | {stats['Model Dim']:<8} | {stats['Holo Dim']:<8} | {stats['Total Params']:<12} | {stats['Non-Embed Params']:<12}\")\n",
    "\n",
    "    print(f\"{'='*85}\")\n",
    "    \n",
    "print(\"\\n* 'Active Params' excludes the vocabulary embedding matrix (which is static lookup).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2521a413-51b2-4a5c-b400-656eca83bf8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================================================\n",
      "Model           | Layers | d_model  | hd_dim   | Total Params | Active Params\n",
      "-------------------------------------------------------------------------------------\n",
      "Holo-SMALL      | 12     | 768      | 3072     | 180.26 M     | 141.66 M    \n",
      "=====================================================================================\n",
      "Holo-MEDIUM     | 24     | 1024     | 8192     | 857.04 M     | 805.58 M    \n",
      "=====================================================================================\n",
      "Holo-LARGE      | 36     | 1280     | 10240    | 1.95 B       | 1.89 B      \n",
      "=====================================================================================\n",
      "\n",
      "* 'Active Params' excludes the vocabulary embedding matrix (which is static lookup).\n"
     ]
    }
   ],
   "source": [
    "print(f\"{'='*85}\")\n",
    "print(f\"{'Model':<15} | {'Layers':<6} | {'d_model':<8} | {'hd_dim':<8} | {'Total Params':<12} | {'Active Params':<12}\")\n",
    "print(f\"{'-'*85}\")\n",
    "\n",
    "sizes = [\"small\", \"medium\", \"large\"]\n",
    "\n",
    "for size in sizes:\n",
    "    config = HoloConfig.from_preset(size, use_version = 1)\n",
    "\n",
    "    try: \n",
    "        with torch.device(\"meta\"):\n",
    "            model = HoloForCausalLM(config)\n",
    "    except:\n",
    "        # Fallback to CPU if meta device fails (older pytorch)\n",
    "        model = HoloForCausalLM(config)\n",
    "\n",
    "    # 3. Get Stats\n",
    "    stats = get_detailed_stats(model, config, size)\n",
    "    print(f\"{stats['Name']:<15} | {stats['Layers']:<6} | {stats['Model Dim']:<8} | {stats['Holo Dim']:<8} | {stats['Total Params']:<12} | {stats['Non-Embed Params']:<12}\")\n",
    "\n",
    "    print(f\"{'='*85}\")\n",
    "    \n",
    "print(\"\\n* 'Active Params' excludes the vocabulary embedding matrix (which is static lookup).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9d42bf-e138-4d22-b123-c19fdae9a58c",
   "metadata": {},
   "source": [
    "### Profiling the Memory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "337da758-90aa-4b7f-8a20-ccc0777dd98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Settings: Batch Size=2, Seq Len=512\n",
      "\n",
      "--- Profiling Holo-SMALL (Ckpt=True) ---\n",
      "   » Precision: Mixed (torch.bfloat16)\n",
      "   Initializing model...\n",
      "   » Gradient Checkpointing: ENABLED\n",
      "   Model Parameters: 180.26 M\n",
      "   Static Model VRAM: 688.54 MB\n",
      "  Running Inference (B=2, L=512)...\n",
      "  Peak Inference VRAM: 1173.75 MB\n",
      "   Running Training Step (Forward + Backward)...\n",
      "  Peak Training VRAM: 1710.31 MB\n",
      "\n",
      "--- Profiling Holo-MEDIUM (Ckpt=True) ---\n",
      "   » Precision: Mixed (torch.bfloat16)\n",
      "   Initializing model...\n",
      "   » Gradient Checkpointing: ENABLED\n",
      "   Model Parameters: 857.04 M\n",
      "   Static Model VRAM: 3289.24 MB\n",
      "  Running Inference (B=2, L=512)...\n",
      "  Peak Inference VRAM: 5365.47 MB\n",
      "   Running Training Step (Forward + Backward)...\n",
      "  Peak Training VRAM: 7212.78 MB\n",
      "\n",
      "--- Profiling Holo-LARGE (Ckpt=True) ---\n",
      "   » Precision: Mixed (torch.bfloat16)\n",
      "   Initializing model...\n",
      "   » Gradient Checkpointing: ENABLED\n",
      "   Model Parameters: 1.95 B\n",
      "   Static Model VRAM: 7540.49 MB\n",
      "  Running Inference (B=2, L=512)...\n",
      "  Peak Inference VRAM: 11807.93 MB\n",
      "   Running Training Step (Forward + Backward)...\n",
      "  Peak Training VRAM: 15774.12 MB\n",
      "\n",
      "============================================================\n",
      "Model Size | Params          | Inference (MB)  | Training (MB)  \n",
      "------------------------------------------------------------\n",
      "SMALL      | 180.26 M        | 1173.75         | 1710.31        \n",
      "MEDIUM     | 857.04 M        | 5365.47         | 7212.78        \n",
      "LARGE      | 1.95 B          | 11807.93        | 15774.12       \n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "def profile_config(size_name, config, batch_size=4, seq_len=1024,\n",
    "                   use_autocast = True, use_checkpointing = False):\n",
    "    print(f\"\\n--- Profiling Holo-{size_name.upper()} (Ckpt={use_checkpointing}) ---\")\n",
    "    \n",
    "    # 1. Setup Environment\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    if device.type == 'cpu':\n",
    "        print(\"❌ GPU not detected. Cannot profile CUDA memory.\")\n",
    "        return\n",
    "\n",
    "    # Define the Context Manager based on the flag\n",
    "    # Use bfloat16 for modern GPUs (Ampere+), otherwise fallback to float16 usually\n",
    "    dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "\n",
    "    if use_autocast:\n",
    "        amp_ctx = torch.amp.autocast(device_type = \"cuda\", dtype = dtype)\n",
    "        print(f\"   » Precision: Mixed ({dtype})\")\n",
    "    else:\n",
    "        amp_ctx = contextlib.nullcontext()\n",
    "        print(f\"   » Precision: FP32 (Full)\")     \n",
    "        \n",
    "    try:\n",
    "        # 2. Initialize Model\n",
    "        print(\"   Initializing model...\")\n",
    "        model = HoloForCausalLM(config).to(device)\n",
    "\n",
    "        if use_checkpointing:\n",
    "            model.gradient_checkpointing_enable()\n",
    "            print(\"   » Gradient Checkpointing: ENABLED\")\n",
    "        \n",
    "        param_count = count_parameters(model)\n",
    "        model_mem = get_peak_memory_mb()\n",
    "        print(f\"   Model Parameters: {format_params(param_count)}\")\n",
    "        print(f\"   Static Model VRAM: {model_mem:.2f} MB\")\n",
    "\n",
    "        # 3. Prepare Dummy Data\n",
    "        input_ids = torch.randint(0, config.vocab_size, (batch_size, seq_len)).to(device)\n",
    "        \n",
    "        # --- 4. Profile Inference (Forward Pass) ---\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        print(f\"  Running Inference (B={batch_size}, L={seq_len})...\")  \n",
    "        model.eval() # Ensure eval mode\n",
    "        with torch.no_grad():\n",
    "            with amp_ctx: # <--- Applies autocast if enabled\n",
    "                outputs = model(input_ids)        \n",
    "                \n",
    "        inference_peak = get_peak_memory_mb()\n",
    "        print(f\"  Peak Inference VRAM: {inference_peak:.2f} MB\")\n",
    "        # Cleanup inference tensors to get a clean slate for training\n",
    "        del outputs\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "        \n",
    "        # --- 5. Profile Training (Forward + Backward) ---\n",
    "        print(f\"   Running Training Step (Forward + Backward)...\")\n",
    "        \n",
    "        model.train() # Switch to train mode\n",
    "        \n",
    "        # Forward Pass (Inside Autocast)\n",
    "        with amp_ctx: \n",
    "            outputs = model(input_ids, labels=input_ids)\n",
    "            loss = outputs.loss        \n",
    "            \n",
    "        # Backward Pass (Outside Autocast)\n",
    "        # Note: For bfloat16, GradScaler is usually not needed. \n",
    "        # If using float16, you would need scaler.scale(loss).backward() here.\n",
    "        loss.backward()\n",
    "        \n",
    "        training_peak = get_peak_memory_mb()\n",
    "        print(f\"  Peak Training VRAM: {training_peak:.2f} MB\")\n",
    "        \n",
    "        # Cleanup\n",
    "        del model, outputs, loss, input_ids\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        return {\n",
    "            \"size\": size_name,\n",
    "            \"params\": param_count,\n",
    "            \"inference_mb\": inference_peak,\n",
    "            \"training_mb\": training_peak\n",
    "        }\n",
    "\n",
    "    except torch.cuda.OutOfMemoryError:\n",
    "        print(f\"❌ OOM: Holo-{size_name.upper()} is too large for this GPU.\")\n",
    "        torch.cuda.empty_cache()\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {e}\")\n",
    "        return None\n",
    "\n",
    "sizes = [\"small\", \"medium\", \"large\"]\n",
    "results = []\n",
    "\n",
    "# Test Settings\n",
    "BATCH_SIZE = 2\n",
    "SEQ_LEN = 512 # Keep modest for testing\n",
    "\n",
    "print(f\"Global Settings: Batch Size={BATCH_SIZE}, Seq Len={SEQ_LEN}\")\n",
    "\n",
    "for size in sizes:\n",
    "    # Load the preset config\n",
    "    cfg = HoloConfig.from_preset(size, use_version = 2)\n",
    "    \n",
    "    # Run profiler\n",
    "    res = profile_config(size, cfg, batch_size=BATCH_SIZE, seq_len=SEQ_LEN, use_checkpointing = True)\n",
    "    if res:\n",
    "        results.append(res)\n",
    "\n",
    "# Print Summary Table\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"{'Model Size':<10} | {'Params':<15} | {'Inference (MB)':<15} | {'Training (MB)':<15}\")\n",
    "print(\"-\" * 60)\n",
    "for r in results:\n",
    "    print(f\"{r['size'].upper():<10} | {format_params(r['params']):<15} | {r['inference_mb']:<15.2f} | {r['training_mb']:<15.2f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a448cf35-4ede-4679-b772-2512beadc6ae",
   "metadata": {},
   "source": [
    "### Version 2 for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bc96fc7-f6db-49c8-b9a6-fd7d3e4f4fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Settings: Batch Size=2, Seq Len=512\n",
      "\n",
      "--- Profiling Holo-SMALL (Ckpt=True) ---\n",
      "   » Precision: Mixed (torch.bfloat16)\n",
      "   Initializing model...\n",
      "   » Gradient Checkpointing: ENABLED\n",
      "   Model Parameters: 180.26 M\n",
      "   Static Model VRAM: 707.66 MB\n",
      "  Running Inference (B=2, L=512)...\n",
      "  Peak Inference VRAM: 1183.75 MB\n",
      "   Running Training Step (Forward + Backward)...\n",
      "  Peak Training VRAM: 1709.47 MB\n",
      "\n",
      "--- Profiling Holo-MEDIUM (Ckpt=True) ---\n",
      "   » Precision: Mixed (torch.bfloat16)\n",
      "   Initializing model...\n",
      "   » Gradient Checkpointing: ENABLED\n",
      "   Model Parameters: 857.04 M\n",
      "   Static Model VRAM: 3289.24 MB\n",
      "  Running Inference (B=2, L=512)...\n",
      "  Peak Inference VRAM: 5365.47 MB\n",
      "   Running Training Step (Forward + Backward)...\n",
      "  Peak Training VRAM: 7212.78 MB\n",
      "\n",
      "--- Profiling Holo-LARGE (Ckpt=True) ---\n",
      "   » Precision: Mixed (torch.bfloat16)\n",
      "   Initializing model...\n",
      "   » Gradient Checkpointing: ENABLED\n",
      "   Model Parameters: 1.95 B\n",
      "   Static Model VRAM: 7540.49 MB\n",
      "  Running Inference (B=2, L=512)...\n",
      "  Peak Inference VRAM: 11807.93 MB\n",
      "   Running Training Step (Forward + Backward)...\n",
      "  Peak Training VRAM: 15774.12 MB\n",
      "\n",
      "============================================================\n",
      "Model Size | Params          | Inference (MB)  | Training (MB)  \n",
      "------------------------------------------------------------\n",
      "SMALL      | 180.26 M        | 1173.75         | 1710.31        \n",
      "MEDIUM     | 857.04 M        | 5365.47         | 7212.78        \n",
      "LARGE      | 1.95 B          | 11807.93        | 15774.12       \n",
      "SMALL      | 180.26 M        | 1183.75         | 1709.47        \n",
      "MEDIUM     | 857.04 M        | 5365.47         | 7212.78        \n",
      "LARGE      | 1.95 B          | 11807.93        | 15774.12       \n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Test Settings\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 2\n",
    "SEQ_LEN = 512 # Keep modest for testing\n",
    "\n",
    "print(f\"Global Settings: Batch Size={BATCH_SIZE}, Seq Len={SEQ_LEN}\")\n",
    "\n",
    "for size in sizes:\n",
    "    # Load the preset config\n",
    "    cfg = HoloConfig.from_preset(size, use_version = 2)\n",
    "    \n",
    "    # Run profiler\n",
    "    res = profile_config(size, cfg, batch_size=BATCH_SIZE, seq_len=SEQ_LEN, use_checkpointing = True)\n",
    "    if res:\n",
    "        results.append(res)\n",
    "\n",
    "# Print Summary Table\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"{'Model Size':<10} | {'Params':<15} | {'Inference (MB)':<15} | {'Training (MB)':<15}\")\n",
    "print(\"-\" * 60)\n",
    "for r in results:\n",
    "    print(f\"{r['size'].upper():<10} | {format_params(r['params']):<15} | {r['inference_mb']:<15.2f} | {r['training_mb']:<15.2f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc58472b-9488-4fe8-8d0c-f9888cb006fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test Settings\n",
    "# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# BATCH_SIZE = 2\n",
    "# SEQ_LEN = 512 # Keep modest for testing\n",
    "\n",
    "# cfg = HoloConfig.from_preset(size = \"small\", use_version = 2)\n",
    "# model = HoloForCausalLM(cfg).to(DEVICE)\n",
    "\n",
    "# input_ids = torch.randint(0, cfg.vocab_size, (BATCH_SIZE, SEQ_LEN)).to(DEVICE)\n",
    "\n",
    "# out = model(input_ids)\n",
    "# print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e51c63-5d61-43e1-b038-210abf34b1c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76b8092-5632-496f-adb6-0efc7c576a6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (main venv)",
   "language": "python",
   "name": "main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
