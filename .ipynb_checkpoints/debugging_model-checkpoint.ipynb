{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "759e9471-5e3c-4ae4-883d-1aa8cea6d61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import PretrainedConfig\n",
    "import math\n",
    "from typing import Tuple, Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b15bb7-6cf0-44cd-9191-9d634ef2f496",
   "metadata": {},
   "source": [
    "## Configuration of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0017b374-3c15-499c-abcd-0c9d60bfa7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HoloConfig(PretrainedConfig):\n",
    "    model_type = \"holo\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size=50257,        # Default to GPT-2 tokenizer size\n",
    "        hidden_size=768,         # This is 'd_model'\n",
    "        hd_dim=2048,             # The Holographic Bus (Key/Value expansion)\n",
    "        num_hidden_layers=12,    # Depth\n",
    "        expansion_factor=4,      # MLP expansion (usually 4x hidden_size)\n",
    "        max_position_embeddings=8192,\n",
    "        layer_norm_eps=1e-5,\n",
    "        initializer_range=0.02,\n",
    "        dropout = 0.0,\n",
    "        pad_token_id=0,\n",
    "        bos_token_id=1,\n",
    "        eos_token_id=2,\n",
    "        tie_word_embeddings=False, # Whether to tie input/output embeddings\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Configuration class for HoloGPT.\n",
    "        \n",
    "        Args:\n",
    "            hd_dim (int): The dimension of the holographic binding space. \n",
    "                          Ideally 2x-4x larger than hidden_size to reduce \n",
    "                          superposition noise (crosstalk).\n",
    "        \"\"\"\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = hidden_size\n",
    "        self.hd_dim = hd_dim\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.expansion_factor = expansion_factor\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.layer_norm_eps = layer_norm_eps\n",
    "        self.initializer_range = initializer_range\n",
    "        self.tie_word_embeddings = tie_word_embeddings\n",
    "        self.dropout = dropout\n",
    "\n",
    "        super().__init__(\n",
    "            pad_token_id=pad_token_id,\n",
    "            bos_token_id=bos_token_id,\n",
    "            eos_token_id=eos_token_id,\n",
    "            **kwargs,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662fa425-a9b9-480d-8663-2ab92f6e6d82",
   "metadata": {},
   "source": [
    "### Displaying the configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dbaa24b-6907-4bbc-93ef-05fd19eb3cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HoloConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"d_model\": 768,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"expansion_factor\": 4,\n",
      "  \"hd_dim\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"holo\",\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config_small = HoloConfig(\n",
    "    vocab_size = 50257,\n",
    "    hidden_size = 768,       # Standard small width\n",
    "    hd_dim = 1536,           # 2x expansion for memory clarity\n",
    "    num_hidden_layers = 12,  # Standard depth\n",
    "    expansion_factor = 4     # Standard MLP width\n",
    ")\n",
    "print(config_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbbc3a14-0b4d-48a0-9480-c6dda515324b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HoloConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"d_model\": 1024,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"expansion_factor\": 4,\n",
      "  \"hd_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"holo\",\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config_medium = HoloConfig(\n",
    "    vocab_size=50257,\n",
    "    hidden_size=1024,       # Increased width\n",
    "    hd_dim=3072,            # 3x expansion (High fidelity memory)\n",
    "    num_hidden_layers=24,   # Deeper network for complex reasoning\n",
    "    expansion_factor=4\n",
    ")\n",
    "print(config_medium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2f88f80-8101-4dc1-a94a-682e39c73246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HoloConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"d_model\": 1600,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"expansion_factor\": 4,\n",
      "  \"hd_dim\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"holo\",\n",
      "  \"num_hidden_layers\": 48,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config_large = HoloConfig(\n",
    "    vocab_size=50257,\n",
    "    hidden_size=1600,       # GPT-2 XL width\n",
    "    hd_dim=4096,            # Massive holographic bus\n",
    "    num_hidden_layers=48,   # Very deep\n",
    "    expansion_factor=4\n",
    ")\n",
    "print(config_large)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff0e050-8676-41d2-81b6-73776ec99659",
   "metadata": {},
   "source": [
    "### Debugging the Model Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f3be5d-1a7c-4013-8c86-b3915a45089e",
   "metadata": {},
   "source": [
    "file functional.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ade20fe-d64e-4fe6-be18-3b8a81f66b7e",
   "metadata": {},
   "source": [
    "1. The Problem: \"Local Blur\" in Standard RoPE:\n",
    "   - In standard Rotary Positional Embeddings (RoPE), the frequencies are generated using a geometric progression (decreasing frequencies).\n",
    "   - Low Frequencies: Many dimensions in standard RoPE have very small frequencies (rotations).\n",
    "   - The Consequence: If the rotation speed is slow, the rotor for position $t$ is almost identical to the rotor for position $t+1$.\n",
    "   - Local Blur: When the model calculates attention (similarity) between tokens, nearby tokens (e.g., pos 50 and pos 51) have very high \"positional overlap.\" The model struggles to distinguish precise ordering among neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8516d401-5190-4454-8baa-7465101372f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Optional, Union\n",
    "\n",
    "def generate_random_phasors(hd_dim: int, scaling_factor: float = 10.0, \n",
    "                            device: torch.device = None) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Generates fixed orthogonal high-frequency phases for the Holographic Key.\n",
    "    Replaces Standard RoPE to fix the 'Local Blur' issue.\n",
    "    \n",
    "    Args:\n",
    "        hd_dim: The holographic dimension (must be even if using view_as_complex logic later, \n",
    "                but here we assume complex64 tensors).\n",
    "        scaling_factor: The scaling factor to force the frequencies to spin faster\n",
    "                        (Preventing the Local Blur problem)\n",
    "    \n",
    "    Returns:\n",
    "        freqs: A tensor of shape (hd_dim,) containing random frequencies.\n",
    "    \"\"\"\n",
    "    # Drawn from a wide uniform distribution to ensure orthogonality across the spectrum\n",
    "    # High frequencies (> 1.0) are critical for \"Needle in a Haystack\" precision.\n",
    "    return torch.randn(hd_dim, device=device) * scaling_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3576c15d-bf3e-4fba-b656-f8bc7527ca67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rotors(\n",
    "    seq_len: int, \n",
    "    freqs: torch.Tensor, \n",
    "    offset: int = 0\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Creates the Holographic Rotors (Positional Encodings) in the Complex Plane.\n",
    "    Formula: Rotor_t = exp(i * t * theta)\n",
    "    \n",
    "    Args:\n",
    "        seq_len: Length of the sequence.\n",
    "        freqs: The fixed random frequencies (hd_dim,).\n",
    "        offset: Starting position index (for cache/inference steps).\n",
    "        \n",
    "    Returns:\n",
    "        rotors: Complex tensor (1, seq_len, hd_dim)\n",
    "    \"\"\"\n",
    "    # Create position indices [0, 1, 2, ...]\n",
    "    t = torch.arange(seq_len, device=freqs.device, dtype=torch.float32) + offset\n",
    "    \n",
    "    # Outer product: positions * frequencies\n",
    "    # Shape: (seq_len, hd_dim)\n",
    "    angles = torch.outer(t, freqs)\n",
    "    \n",
    "    # Polar to Rectangular: exp(i * theta) = cos(theta) + i*sin(theta)\n",
    "    rotors = torch.polar(torch.ones_like(angles), angles)\n",
    "    \n",
    "    return rotors.unsqueeze(0) # Add batch dim for broadcasting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95f2a8b1-e30d-40c7-89ea-55fd56a7d265",
   "metadata": {},
   "outputs": [],
   "source": [
    "def holo_bind_and_accumulate(\n",
    "    v: torch.Tensor, \n",
    "    rotors: torch.Tensor\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    The Core Holographic Memory Operation (The 'Write' Head).\n",
    "    Performs Binding -> Accumulation with Mixed Precision stability.\n",
    "    \n",
    "    Args:\n",
    "        v: Value tensor (Batch, Seq, HD_Dim) [Complex64]\n",
    "        rotors: Positional Rotors (1, Seq, HD_Dim) [Complex64]\n",
    "        \n",
    "    Returns:\n",
    "        memory_trace: The cumulative sum of bound states.\n",
    "    \"\"\"\n",
    "    # 1. Holographic Binding (Element-wise Rotation)\n",
    "    # This encodes the \"Position\" into the \"Value\"\n",
    "    bound_state = v * rotors\n",
    "    \n",
    "    # 2. Linear Memory Accumulation (The Scan)\n",
    "    # CRITICAL: Force FP32 for the cumulative sum to prevent \"Swamping\" \n",
    "    # (where small new tokens vanish in long contexts).\n",
    "    # We cast to complex128 (Float64 real/imag) or complex64 (Float32 real/imag).\n",
    "    # If input is BF16/FP16, this step must upgrade precision.\n",
    "    bound_state_fp32 = bound_state.to(torch.complex64) \n",
    "    \n",
    "    memory_trace = torch.cumsum(bound_state_fp32, dim=1)\n",
    "    \n",
    "    return memory_trace\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0eabeb53-b287-46ce-b876-09b0cbfb727d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def holo_retrieve(\n",
    "    memory_trace: torch.Tensor, \n",
    "    rotors: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    The Holographic Retrieval Operation (The 'Read' Head).\n",
    "    Performs Unbinding -> Normalization.\n",
    "    \n",
    "    Args:\n",
    "        memory_trace: Accumulated memory (Batch, Seq, HD_Dim)\n",
    "        rotors: Positional Rotors (1, Seq, HD_Dim)\n",
    "        \n",
    "    Returns:\n",
    "        retrieved: The decoded signal (Real-valued projection ready for output).\n",
    "    \"\"\"\n",
    "    B, T, D = memory_trace.shape\n",
    "    \n",
    "    # 1. Unbinding (Derotation)\n",
    "    # Multiply by the CONJUGATE of the rotor. \n",
    "    # If we bound with exp(i*theta), we unbind with exp(-i*theta).\n",
    "    # This cancels the phase for the target position, leaving the signal at DC (0 freq).\n",
    "    raw_retrieval = memory_trace * torch.conj(rotors)\n",
    "    \n",
    "    # 2. Normalization (The Scaling Law Fix)\n",
    "    # The magnitude of a random walk grows by sqrt(T). \n",
    "    # We divide by sqrt(T) to keep the signal variance roughly 1.0 for the MLP.\n",
    "    scale = torch.sqrt(\n",
    "        torch.arange(1, T + 1, device=memory_trace.device, dtype=torch.float32)\n",
    "    ).view(1, T, 1)\n",
    "    \n",
    "    # Avoid div by zero (sanity check)\n",
    "    scale = torch.clamp(scale, min=1.0)\n",
    "    \n",
    "    normalized_retrieval = raw_retrieval / scale\n",
    "    \n",
    "    # 3. Project to Real\n",
    "    # The information is stored in the Magnitude/Real alignment.\n",
    "    # We return the real part. The Imaginary part contains the \"crosstalk noise\" \n",
    "    # from other positions, which the MLP will filter out.\n",
    "    return normalized_retrieval.real\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1062b109-0c56-4c11-bc0e-6c8caa18fe48",
   "metadata": {},
   "source": [
    "file \"layers.py** file\n",
    "\n",
    "This file defines the mechansim of the Attention (HoloAttention) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "95d89d25-fce5-4dbc-8d33-b34cf918c35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HoloAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    The Holographic 'Attention' Mechanism.\n",
    "    Replaces N^2 Softmax Attention with O(N) Complex-Valued Recurrence.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.d_model = config.d_model\n",
    "        self.hd_dim = config.hd_dim \n",
    "\n",
    "        # 1. Projections (Real -> Complex)\n",
    "        # We project inputs into the Holographic \"Hyper-Dimension\"\n",
    "        self.k_proj = nn.Linear(config.d_model, config.hd_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(config.d_model, config.hd_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(config.d_model, config.d_model, bias=False) # Output is Real\n",
    "\n",
    "        # 2. Fixed Random Phasors (The \"Keys\")\n",
    "        # Registered as buffer so they save with the model don't update via GD\n",
    "        self.register_buffer(\"freqs\", generate_random_phasors(config.hd_dim))\n",
    "\n",
    "        # 3. Residual Dropout\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "\n",
    "    def forward(self, x): \n",
    "        B, T, C = x.shape\n",
    "\n",
    "        # --- Step 1: Project to Holographic Space --- \n",
    "        # k, v shape: (B, T, hd_dim)\n",
    "        # We cast to complex64 immediately to enable phase operations\n",
    "        k_real = self.k_proj(x)\n",
    "        v_real = self.v_proj(x)\n",
    "\n",
    "        # In a full implementation, K determines *which* frequency to write to.\n",
    "        # For this version (Linear Associative Memory), we use V as the content\n",
    "        # and implicit position as the key.\n",
    "        # Future improvement: Use K to modulate the frequencies (Data-Dependent).\n",
    "        v = v_real.to(torch.complex64)\n",
    "\n",
    "        # --- Step 2: Generate Positional Rotors ---\n",
    "        # Rotors shape: (1, T, hd_dim)\n",
    "        rotors = compute_rotors(T, self.freqs)\n",
    "\n",
    "        # --- Step 3: Bind & Accumulate (The O(N) Magic) ---\n",
    "        # This replaces the Attention Matrix calculation\n",
    "        memory_trace = holo_bind_and_accumulate(v, rotors)\n",
    "\n",
    "        # --- Step 4: Retrieve (Derotate) --- \n",
    "        # This replaces the Attention * Value calculation \n",
    "        output_complex = holo_retrieve(memory_trace, rotors)\n",
    "        output_real = output_complex.real\n",
    "\n",
    "        projected = self.o_proj(output_real)\n",
    "        \n",
    "        # --- Step 5: Project Output ---\n",
    "        # We take the Real part (Magnitude/Phase alignment)\n",
    "        return self.resid_dropout(projected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d077ba7f-0014-42a6-a614-fe0a9ce823e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (139791767.py, line 17)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mnn.Linear(config.d_model * config.expansion_factor, config.d_model)\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "class HoloBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard Transformer Block structure, but swapping Self-Attention \n",
    "    for HoloAttention.\n",
    "    Structure: Input -> LN -> Holo -> Add -> LN -> MLP -> Add\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(config.d_model)\n",
    "        self.attn = HoloAttention(config)\n",
    "\n",
    "        \n",
    "        self.ln2 = nn.LayerNorm(config.d_model)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(config.d_model, config.d_model * config.expansion_factor),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(config.d_model * config.expansion_factor, config.d_model),\n",
    "            nn.Dropout(config.dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1. Holographic Mixer Path\n",
    "        residual = x\n",
    "        x = self.ln1(x)\n",
    "        x = residual + self.attn(x)\n",
    "        \n",
    "        # 2. MLP Path (The \"Denoising\" Step)\n",
    "        residual = x\n",
    "        x = self.ln2(x)\n",
    "        x = residual + self.mlp(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1d1207-fc49-4633-8fd2-7a46ede84dca",
   "metadata": {},
   "source": [
    "file **modeling_holo.py** file\n",
    "\n",
    "Creating a Model with all of the contents take the inspiration from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55a4b592-f853-472c-a06d-e7991a1b4a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedModel, AutoModelForCausalLM\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPast, \\\n",
    "                                            CausalLMOutputWithPast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eed6798d-1c06-421b-affe-0fac75029aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HoloPreTrainedModel(PreTrainedModel):\n",
    "    \"\"\"\n",
    "    Base class for Holo-Transformer weights initialization and utilities\n",
    "    \"\"\"\n",
    "    config_class = HoloConfig\n",
    "    base_model_prefix = \"holo\"\n",
    "    supports_gradient_checkpointing = True\n",
    "    _no_split_modules = [\"HoloBlock\"]\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"\n",
    "        Standard GPT-style initialization.\n",
    "        \"\"\"\n",
    "        std = self.config.initializer_range\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=std)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c20a5ebd-5fab-44fd-b597-1f66b43be554",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HoloModel(HoloPreTrainedModel):\n",
    "    \"\"\"\n",
    "    The bare Holo-Transformer backbone (Embeddings + Layers).\n",
    "    \"\"\"\n",
    "    def __init__(self, config: HoloConfig):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "\n",
    "        # 1. Embeddings\n",
    "        # Note: We do NOT use Positional Embeddings here. \n",
    "        # The Holographic Layer handles position via complex rotation internally.\n",
    "        self.wte = nn.Embedding(config.vocab_size, config.d_model)\n",
    "        self.drop = nn.Dropout(config.dropout) # Usually 0 for LLMs, but kept for interface\n",
    "\n",
    "        # 2. The Stack\n",
    "        self.h = nn.ModuleList([HoloBlock(config) for _ in range(config.num_hidden_layers)])\n",
    "        \n",
    "        # 3. Final Norm\n",
    "        self.ln_f = nn.LayerNorm(config.d_model, eps=config.layer_norm_eps)\n",
    "\n",
    "        self.post_init()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.wte\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.wte = value\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        **kwargs # Catch-all for past_key_values if added later\n",
    "    ) -> Union[Tuple, BaseModelOutputWithPast]:\n",
    "        \n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # 1. Prepare Input\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.wte(input_ids)\n",
    "        \n",
    "        hidden_states = inputs_embeds\n",
    "        hidden_states = self.drop(hidden_states)\n",
    "\n",
    "        # 2. Run Layers\n",
    "        all_hidden_states = () if return_dict else None\n",
    "        \n",
    "        for block in self.h:\n",
    "            if return_dict:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "            \n",
    "            # The Magic happens here\n",
    "            hidden_states = block(hidden_states)\n",
    "\n",
    "        # 3. Finalize\n",
    "        hidden_states = self.ln_f(hidden_states)\n",
    "\n",
    "        if not return_dict:\n",
    "            return (hidden_states,)\n",
    "\n",
    "        return BaseModelOutputWithPast(\n",
    "            last_hidden_state=hidden_states,\n",
    "            hidden_states=all_hidden_states,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1f9b15-d26a-4133-b344-5a71a701ced2",
   "metadata": {},
   "source": [
    "Defining the HoloBlock :))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7803cd0-61e3-4479-b70d-1368d68d1688",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HoloForCausalLM(HoloPreTrainedModel):\n",
    "    \"\"\"\n",
    "    The End-to-End Language Model (Backbone + LM Head).\n",
    "    Use this for training.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: HoloConfig):\n",
    "        super().__init__(config)\n",
    "        self.holo = HoloModel(config)\n",
    "        \n",
    "        # LM Head (Projects back to Vocab)\n",
    "        self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
    "\n",
    "        # Weight Tying (Optional but standard)\n",
    "        if config.tie_word_embeddings:\n",
    "            self.lm_head.weight = self.holo.wte.weight\n",
    "\n",
    "        self.post_init()\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.lm_head\n",
    "\n",
    "    def set_output_embeddings(self, new_embeddings):\n",
    "        self.lm_head = new_embeddings\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        **kwargs\n",
    "    ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
    "        \n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # 1. Run Backbone\n",
    "        outputs = self.holo(\n",
    "            input_ids=input_ids,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            return_dict=return_dict,\n",
    "            **kwargs\n",
    "        )\n",
    "        hidden_states = outputs[0]\n",
    "\n",
    "        # 2. Compute Logits\n",
    "        logits = self.lm_head(hidden_states)\n",
    "\n",
    "        # 3. Compute Loss (if labels provided)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            \n",
    "            # Flatten tokens\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(shift_logits.view(-1, self.config.vocab_size), shift_labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[1:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return CausalLMOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "        )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (main venv)",
   "language": "python",
   "name": "main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
