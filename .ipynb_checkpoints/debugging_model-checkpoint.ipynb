{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "759e9471-5e3c-4ae4-883d-1aa8cea6d61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import PretrainedConfig\n",
    "import math\n",
    "from typing import Tuple, Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b15bb7-6cf0-44cd-9191-9d634ef2f496",
   "metadata": {},
   "source": [
    "## Configuration of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0017b374-3c15-499c-abcd-0c9d60bfa7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HoloConfig(PretrainedConfig):\n",
    "    model_type = \"holo\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size=50257,        # Default to GPT-2 tokenizer size\n",
    "        hidden_size=768,         # This is 'd_model'\n",
    "        hd_dim=2048,             # The Holographic Bus (Key/Value expansion)\n",
    "        num_hidden_layers=12,    # Depth\n",
    "        expansion_factor=4,      # MLP expansion (usually 4x hidden_size)\n",
    "        max_position_embeddings=8192,\n",
    "        layer_norm_eps=1e-5,\n",
    "        initializer_range=0.02,\n",
    "        pad_token_id=0,\n",
    "        bos_token_id=1,\n",
    "        eos_token_id=2,\n",
    "        tie_word_embeddings=False, # Whether to tie input/output embeddings\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Configuration class for HoloGPT.\n",
    "        \n",
    "        Args:\n",
    "            hd_dim (int): The dimension of the holographic binding space. \n",
    "                          Ideally 2x-4x larger than hidden_size to reduce \n",
    "                          superposition noise (crosstalk).\n",
    "        \"\"\"\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = hidden_size\n",
    "        self.hd_dim = hd_dim\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.expansion_factor = expansion_factor\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.layer_norm_eps = layer_norm_eps\n",
    "        self.initializer_range = initializer_range\n",
    "        self.tie_word_embeddings = tie_word_embeddings\n",
    "\n",
    "        super().__init__(\n",
    "            pad_token_id=pad_token_id,\n",
    "            bos_token_id=bos_token_id,\n",
    "            eos_token_id=eos_token_id,\n",
    "            **kwargs,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662fa425-a9b9-480d-8663-2ab92f6e6d82",
   "metadata": {},
   "source": [
    "### Displaying the configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dbaa24b-6907-4bbc-93ef-05fd19eb3cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HoloConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"d_model\": 768,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"expansion_factor\": 4,\n",
      "  \"hd_dim\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"holo\",\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config_small = HoloConfig(\n",
    "    vocab_size = 50257,\n",
    "    hidden_size = 768,       # Standard small width\n",
    "    hd_dim = 1536,           # 2x expansion for memory clarity\n",
    "    num_hidden_layers = 12,  # Standard depth\n",
    "    expansion_factor = 4     # Standard MLP width\n",
    ")\n",
    "print(config_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbbc3a14-0b4d-48a0-9480-c6dda515324b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HoloConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"d_model\": 1024,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"expansion_factor\": 4,\n",
      "  \"hd_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"holo\",\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config_medium = HoloConfig(\n",
    "    vocab_size=50257,\n",
    "    hidden_size=1024,       # Increased width\n",
    "    hd_dim=3072,            # 3x expansion (High fidelity memory)\n",
    "    num_hidden_layers=24,   # Deeper network for complex reasoning\n",
    "    expansion_factor=4\n",
    ")\n",
    "print(config_medium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2f88f80-8101-4dc1-a94a-682e39c73246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HoloConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"d_model\": 1600,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"expansion_factor\": 4,\n",
      "  \"hd_dim\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"holo\",\n",
      "  \"num_hidden_layers\": 48,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config_large = HoloConfig(\n",
    "    vocab_size=50257,\n",
    "    hidden_size=1600,       # GPT-2 XL width\n",
    "    hd_dim=4096,            # Massive holographic bus\n",
    "    num_hidden_layers=48,   # Very deep\n",
    "    expansion_factor=4\n",
    ")\n",
    "print(config_large)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff0e050-8676-41d2-81b6-73776ec99659",
   "metadata": {},
   "source": [
    "### Debugging the Model Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f3be5d-1a7c-4013-8c86-b3915a45089e",
   "metadata": {},
   "source": [
    "file functional.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ade20fe-d64e-4fe6-be18-3b8a81f66b7e",
   "metadata": {},
   "source": [
    "1. The Problem: \"Local Blur\" in Standard RoPE:\n",
    "   - In standard Rotary Positional Embeddings (RoPE), the frequencies are generated using a geometric progression (decreasing frequencies).\n",
    "   - Low Frequencies: Many dimensions in standard RoPE have very small frequencies (rotations).\n",
    "   - The Consequence: If the rotation speed is slow, the rotor for position $t$ is almost identical to the rotor for position $t+1$.\n",
    "   - Local Blur: When the model calculates attention (similarity) between tokens, nearby tokens (e.g., pos 50 and pos 51) have very high \"positional overlap.\" The model struggles to distinguish precise ordering among neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8516d401-5190-4454-8baa-7465101372f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Optional\n",
    "\n",
    "def generate_random_phasors(hd_dim: int, scaling_factor: float = 10.0, \n",
    "                            device: torch.device = None) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Generates fixed orthogonal high-frequency phases for the Holographic Key.\n",
    "    Replaces Standard RoPE to fix the 'Local Blur' issue.\n",
    "    \n",
    "    Args:\n",
    "        hd_dim: The holographic dimension (must be even if using view_as_complex logic later, \n",
    "                but here we assume complex64 tensors).\n",
    "        scaling_factor: The scaling factor to force the frequencies to spin faster\n",
    "                        (Preventing the Local Blur problem)\n",
    "    \n",
    "    Returns:\n",
    "        freqs: A tensor of shape (hd_dim,) containing random frequencies.\n",
    "    \"\"\"\n",
    "    # Drawn from a wide uniform distribution to ensure orthogonality across the spectrum\n",
    "    # High frequencies (> 1.0) are critical for \"Needle in a Haystack\" precision.\n",
    "    return torch.randn(hd_dim, device=device) * scaling_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3576c15d-bf3e-4fba-b656-f8bc7527ca67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rotors(\n",
    "    seq_len: int, \n",
    "    freqs: torch.Tensor, \n",
    "    offset: int = 0\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Creates the Holographic Rotors (Positional Encodings) in the Complex Plane.\n",
    "    Formula: Rotor_t = exp(i * t * theta)\n",
    "    \n",
    "    Args:\n",
    "        seq_len: Length of the sequence.\n",
    "        freqs: The fixed random frequencies (hd_dim,).\n",
    "        offset: Starting position index (for cache/inference steps).\n",
    "        \n",
    "    Returns:\n",
    "        rotors: Complex tensor (1, seq_len, hd_dim)\n",
    "    \"\"\"\n",
    "    # Create position indices [0, 1, 2, ...]\n",
    "    t = torch.arange(seq_len, device=freqs.device, dtype=torch.float32) + offset\n",
    "    \n",
    "    # Outer product: positions * frequencies\n",
    "    # Shape: (seq_len, hd_dim)\n",
    "    angles = torch.outer(t, freqs)\n",
    "    \n",
    "    # Polar to Rectangular: exp(i * theta) = cos(theta) + i*sin(theta)\n",
    "    rotors = torch.polar(torch.ones_like(angles), angles)\n",
    "    \n",
    "    return rotors.unsqueeze(0) # Add batch dim for broadcasting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f2a8b1-e30d-40c7-89ea-55fd56a7d265",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (main venv)",
   "language": "python",
   "name": "main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
