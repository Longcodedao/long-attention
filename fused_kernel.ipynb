{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9396367-33ee-4ccb-8cde-a1176f3532dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f831d126-58dd-4ba1-aa74-710d285a5918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warming up...\n",
      "Eager PyTorch:   23.697 ms\n",
      "torch.compile:   38.974 ms\n",
      "Speedup:         0.61x\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "B, T, H, D = 128, 2048, 16, 64\n",
    "\n",
    "# Inputs\n",
    "v = torch.randn(B, T, H, D, dtype=torch.complex64, device=device)\n",
    "k = torch.randn(B, T, H, D, dtype=torch.complex64, device=device)\n",
    "q = torch.randn(B, T, H, D, dtype=torch.complex64, device=device)\n",
    "\n",
    "def holo_op(v, k, q):\n",
    "    # The logic we want to fuse \n",
    "    mem = torch.cumsum(v * k, dim = 1)\n",
    "\n",
    "    # Scaling logic\n",
    "    T = mem.shape[1]\n",
    "    scale = torch.sqrt(torch.arange(1, T + 1, device=mem.device, dtype=torch.float32))\n",
    "    scale = scale.view(1, T, 1, 1)\n",
    "    \n",
    "    return (mem * q).real / scale\n",
    "\n",
    "# 2. Compile it\n",
    "# fullgraph=True ensures no python fallbacks. \n",
    "# mode=\"max-autotune\" enables the most aggressive Triton optimizations.\n",
    "compiled_holo = torch.compile(holo_op, mode=\"max-autotune\", fullgraph=True)\n",
    "\n",
    "# --- Warmup ---\n",
    "print(\"Warming up...\")\n",
    "for _ in range(5):\n",
    "    _ = holo_op(v, k, q)      # Eager\n",
    "    _ = compiled_holo(v, k, q) # Compiled\n",
    "\n",
    "# --- Benchmark Eager ---\n",
    "torch.cuda.synchronize()\n",
    "start = time.time()\n",
    "for _ in range(100):\n",
    "    _ = holo_op(v, k, q)\n",
    "torch.cuda.synchronize()\n",
    "eager_time = (time.time() - start) * 10\n",
    "\n",
    "# --- Benchmark Compile ---\n",
    "torch.cuda.synchronize()\n",
    "start = time.time()\n",
    "for _ in range(100):\n",
    "    _ = compiled_holo(v, k, q)\n",
    "torch.cuda.synchronize()\n",
    "compiled_time = (time.time() - start) * 10\n",
    "\n",
    "print(f\"Eager PyTorch:   {eager_time:.3f} ms\")\n",
    "print(f\"torch.compile:   {compiled_time:.3f} ms\")\n",
    "print(f\"Speedup:         {eager_time / compiled_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fdbdf56-874e-4f1a-b4b7-848bb1cc80b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "# ==========================================\n",
    "# 1. Fixed Triton Kernel\n",
    "# ==========================================\n",
    "@triton.jit\n",
    "def _holo_scan_fused_kernel(\n",
    "    v_ptr, k_ptr, q_ptr, out_ptr,\n",
    "    # Strides (Standard PyTorch strides)\n",
    "    stride_v_b, stride_v_t, stride_v_h, stride_v_d,\n",
    "    stride_k_b, stride_k_t, stride_k_h, stride_k_d,\n",
    "    stride_q_b, stride_q_t, stride_q_h, stride_q_d,\n",
    "    stride_out_b, stride_out_t, stride_out_h, stride_out_d,\n",
    "    T, H, D, \n",
    "    BLOCK_D: tl.constexpr\n",
    "):\n",
    "    # 1. Parallelize over (Batch * Head)\n",
    "    pid = tl.program_id(0)\n",
    "    \n",
    "    # 2. Decompose PID into Batch and Head indices\n",
    "    # logical grid is flattened, so we reconstruct dimensions\n",
    "    b_idx = pid // H\n",
    "    h_idx = pid % H\n",
    "    \n",
    "    # 3. Calculate Base Pointers for this (Batch, Head) specific sequence\n",
    "    # Logic: base = b * stride_b + h * stride_h\n",
    "    # This allows us to read directly from (B, T, H, D) without permuting!\n",
    "    \n",
    "    v_base = v_ptr + b_idx * stride_v_b + h_idx * stride_v_h\n",
    "    k_base = k_ptr + b_idx * stride_k_b + h_idx * stride_k_h\n",
    "    q_base = q_ptr + b_idx * stride_q_b + h_idx * stride_q_h\n",
    "    out_base = out_ptr + b_idx * stride_out_b + h_idx * stride_out_h\n",
    "\n",
    "    # 4. Setup D-dimension (contiguous block)\n",
    "    offs_d = tl.arange(0, BLOCK_D)\n",
    "    mask_d = offs_d < D\n",
    "    \n",
    "    # 5. Initialize Accumulators\n",
    "    acc_real = tl.zeros([BLOCK_D], dtype=tl.float32)\n",
    "    acc_imag = tl.zeros([BLOCK_D], dtype=tl.float32)\n",
    "\n",
    "    # 6. Sequential Loop over Time\n",
    "    for t in range(T):\n",
    "        # Stride logic: base + t * stride_t + d * stride_d\n",
    "        off_v = t * stride_v_t + offs_d * stride_v_d\n",
    "        off_k = t * stride_k_t + offs_d * stride_k_d\n",
    "        off_q = t * stride_q_t + offs_d * stride_q_d\n",
    "        \n",
    "        # Load\n",
    "        v_real = tl.load(v_base + off_v, mask=mask_d, other=0.0)\n",
    "        v_imag = tl.load(v_base + off_v + 1, mask=mask_d, other=0.0)\n",
    "        k_real = tl.load(k_base + off_k, mask=mask_d, other=0.0)\n",
    "        k_imag = tl.load(k_base + off_k + 1, mask=mask_d, other=0.0)\n",
    "        q_real = tl.load(q_base + off_q, mask=mask_d, other=0.0)\n",
    "        q_imag = tl.load(q_base + off_q + 1, mask=mask_d, other=0.0)\n",
    "\n",
    "        # Compute (v * k)\n",
    "        term_real = v_real * k_real - v_imag * k_imag\n",
    "        term_imag = v_real * k_imag + v_imag * k_real\n",
    "        \n",
    "        # Accumulate (Scan)\n",
    "        acc_real += term_real\n",
    "        acc_imag += term_imag\n",
    "        \n",
    "        # Retrieve (acc * q)\n",
    "        out_val = acc_real * q_real - acc_imag * q_imag\n",
    "        \n",
    "        # Scale\n",
    "        scale = tl.sqrt((t + 1).to(tl.float32))\n",
    "        out_val = out_val / scale\n",
    "\n",
    "        # Store\n",
    "        off_out = t * stride_out_t + offs_d * stride_out_d\n",
    "        tl.store(out_base + off_out, out_val, mask=mask_d)\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 2. Wrapper (No Copies!)\n",
    "# ==========================================\n",
    "\n",
    "def holo_op_triton(v, k, q):\n",
    "    # Input is (B, T, H, D)\n",
    "    # We DO NOT permute. We DO NOT make contiguous.\n",
    "    # We pass the original messy strides to the kernel.\n",
    "    \n",
    "    B, T, H, D = v.shape\n",
    "    \n",
    "    # View as Float32 to get float-pointer compatible strides\n",
    "    # We use a helper because .view(float32) requires contiguity on last dim usually,\n",
    "    # but since we are just passing pointers and calculating offsets manually,\n",
    "    # we can just cast the data_ptr if we are careful.\n",
    "    \n",
    "    # SAFER WAY:\n",
    "    # We still need the last dimension (D) to be contiguous for the block load `tl.load(ptr + range(0, D))`.\n",
    "    # Pytorch (B, T, H, D) is usually contiguous in D.\n",
    "    if v.stride(-1) != 1: v = v.contiguous()\n",
    "    if k.stride(-1) != 1: k = k.contiguous()\n",
    "    if q.stride(-1) != 1: q = q.contiguous()\n",
    "    \n",
    "    # Create Float32 Views (Only changes metadata, no copy if last dim is contiguous)\n",
    "    v_f = v.view(torch.float32)\n",
    "    k_f = k.view(torch.float32)\n",
    "    q_f = q.view(torch.float32)\n",
    "    \n",
    "    # Alloc Output\n",
    "    out = torch.empty((B, T, H, D), device=v.device, dtype=torch.float32)\n",
    "    \n",
    "    # Grid: B * H\n",
    "    grid = (B * H, )\n",
    "    BLOCK_D = triton.next_power_of_2(D)\n",
    "    \n",
    "    # Strides:\n",
    "    # Remember our float view doubled the last stride (because 1 complex = 2 floats).\n",
    "    # But wait, v.stride() returns stride in \"elements\".\n",
    "    # If v is complex64, v.stride(-1) is 1. \n",
    "    # v_f is float32, v_f.stride(-1) is 1.\n",
    "    # BUT we need to jump 2 floats to get to the next real number in our kernel logic?\n",
    "    # Actually:\n",
    "    # In Kernel: offs_d * stride_v_d.\n",
    "    # We want offs_d=1 to point to the next REAL number.\n",
    "    # In memory: [R0, I0, R1, I1].\n",
    "    # R0 is at 0. R1 is at 2.\n",
    "    # So stride_d MUST be 2.\n",
    "    \n",
    "    # However, v_f.stride(-1) is 1.\n",
    "    # So we must manually adjust strides for the kernel.\n",
    "    # Specifically, multiply all strides by 2 because we are treating complex* as float*.\n",
    "    # EXCEPT if the original stride was 1 (dense), it becomes 1 in float view... \n",
    "    # No, strictly speaking:\n",
    "    # 1 complex element = 8 bytes.\n",
    "    # 1 float element = 4 bytes.\n",
    "    # Pointer arithmetic in Triton is on the type (float32).\n",
    "    # To jump 1 complex element (8 bytes), we must jump 2 float elements.\n",
    "    # So YES, we multiply ALL input strides by 2.\n",
    "    \n",
    "    def get_strides(x):\n",
    "        return (x.stride(0)*2, x.stride(1)*2, x.stride(2)*2, x.stride(3)*2)\n",
    "\n",
    "    s_v = get_strides(v)\n",
    "    s_k = get_strides(k)\n",
    "    s_q = get_strides(q)\n",
    "    \n",
    "    # Output is float32, so its strides are native. \n",
    "    # BUT our kernel logic uses stride_out_d to jump output elements.\n",
    "    # Output is just Real numbers. [Out0, Out1].\n",
    "    # So stride is 1.\n",
    "    s_out = out.stride()\n",
    "    \n",
    "    _holo_scan_fused_kernel[grid](\n",
    "        v_f, k_f, q_f, out,\n",
    "        *s_v,\n",
    "        *s_k,\n",
    "        *s_q,\n",
    "        *s_out,\n",
    "        T=T, H=H, D=D,\n",
    "        BLOCK_D=BLOCK_D\n",
    "    )\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed62b5ea-ad0d-4625-8942-31090202354a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 1. Implementations\n",
    "# ==========================================\n",
    "\n",
    "# --- A. Eager Implementation ---\n",
    "def holo_op_eager(v, k, q):\n",
    "    # 1. Bind & Accumulate (The Bottleneck: Writes massive tensor to HBM)\n",
    "    mem = torch.cumsum(v * k, dim=1)\n",
    "    \n",
    "    # 2. Scaling\n",
    "    T = mem.shape[1]\n",
    "    scale = torch.sqrt(torch.arange(1, T + 1, device=mem.device, dtype=torch.float32))\n",
    "    scale = scale.view(1, T, 1, 1)\n",
    "    \n",
    "    # 3. Retrieve\n",
    "    return (mem * q).real / scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "879c7ead-71e7-4691-aa62-4b49f8b48fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking with B=64, T=2048, H=16, D=64\n",
      "Generating inputs...\n",
      "Compiling torch.compile version...\n",
      "Verifying correctness...\n",
      "Max Difference Eager vs Triton: 0.000003\n",
      "\n",
      "--- Starting Benchmark ---\n",
      "Eager PyTorch  : 11.820 ms\n",
      "Torch Compile  : 19.416 ms\n",
      "Triton Kernel  : 4.770 ms\n",
      "\n",
      "--- Results ---\n",
      "Compile Speedup vs Eager: 0.61x\n",
      "Triton Speedup vs Eager:  2.48x\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 2. Benchmark Setup\n",
    "# ==========================================\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# Setup Config (reduced slightly to fit typical VRAM for testing)\n",
    "B, T, H, D = 64, 2048, 16, 64  \n",
    "# NOTE: Total VRAM usage approx 4GB for inputs\n",
    "\n",
    "print(f\"Benchmarking with B={B}, T={T}, H={H}, D={D}\")\n",
    "print(\"Generating inputs...\")\n",
    "\n",
    "v = torch.randn(B, T, H, D, dtype=torch.complex64, device=device)\n",
    "k = torch.randn(B, T, H, D, dtype=torch.complex64, device=device)\n",
    "q = torch.randn(B, T, H, D, dtype=torch.complex64, device=device)\n",
    "\n",
    "# Compile the Eager version\n",
    "print(\"Compiling torch.compile version...\")\n",
    "holo_op_compiled = torch.compile(holo_op_eager, mode=\"max-autotune\", fullgraph=True)\n",
    "\n",
    "# Warmup Compilation\n",
    "try:\n",
    "    _ = holo_op_compiled(v, k, q)\n",
    "except Exception as e:\n",
    "    print(f\"Compilation failed (common with complex64 sometimes): {e}\")\n",
    "    # Fallback to default mode if max-autotune fails\n",
    "    holo_op_compiled = torch.compile(holo_op_eager)\n",
    "    _ = holo_op_compiled(v, k, q)\n",
    "\n",
    "# Verify Correctness (Quick check vs Triton)\n",
    "print(\"Verifying correctness...\")\n",
    "out_eager = holo_op_eager(v, k, q)\n",
    "out_triton = holo_op_triton(v, k, q)\n",
    "\n",
    "# Note: Triton float math accumulation order vs PyTorch Parallel reduction\n",
    "# can cause small differences. standard rtol=1e-3 is usually fine.\n",
    "diff = torch.max(torch.abs(out_eager - out_triton))\n",
    "print(f\"Max Difference Eager vs Triton: {diff:.6f}\")\n",
    "assert diff < 1e-2, \"Triton implementation mismatch!\"\n",
    "\n",
    "# ==========================================\n",
    "# 3. Run Benchmark\n",
    "# ==========================================\n",
    "\n",
    "def run_bench(name, func, iters=100):\n",
    "    # Warmup\n",
    "    for _ in range(5):\n",
    "        _ = func(v, k, q)\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    # Timing\n",
    "    start = time.time()\n",
    "    for _ in range(iters):\n",
    "        _ = func(v, k, q)\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    total_time_ms = (time.time() - start) * 1000\n",
    "    avg_time_ms = total_time_ms / iters\n",
    "    print(f\"{name:<15}: {avg_time_ms:.3f} ms\")\n",
    "    return avg_time_ms\n",
    "\n",
    "print(\"\\n--- Starting Benchmark ---\")\n",
    "t_eager = run_bench(\"Eager PyTorch\", holo_op_eager)\n",
    "t_compile = run_bench(\"Torch Compile\", holo_op_compiled)\n",
    "t_triton = run_bench(\"Triton Kernel\", holo_op_triton)\n",
    "\n",
    "print(\"\\n--- Results ---\")\n",
    "print(f\"Compile Speedup vs Eager: {t_eager / t_compile:.2f}x\")\n",
    "print(f\"Triton Speedup vs Eager:  {t_eager / t_triton:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59fab18-14d0-49ce-b3f8-35275eee879f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (my-project-kernel)",
   "language": "python",
   "name": "my-project-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
