{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9396367-33ee-4ccb-8cde-a1176f3532dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import triton\n",
    "import triton.language as tl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f831d126-58dd-4ba1-aa74-710d285a5918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warming up...\n",
      "Eager PyTorch:   24.372 ms\n",
      "torch.compile:   40.099 ms\n",
      "Speedup:         0.61x\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "B, T, H, D = 128, 2048, 16, 64\n",
    "\n",
    "# Inputs\n",
    "v = torch.randn(B, T, H, D, dtype=torch.complex64, device=device)\n",
    "k = torch.randn(B, T, H, D, dtype=torch.complex64, device=device)\n",
    "q = torch.randn(B, T, H, D, dtype=torch.complex64, device=device)\n",
    "\n",
    "def holo_op(v, k, q):\n",
    "    # The logic we want to fuse \n",
    "    mem = torch.cumsum(v * k, dim = 1)\n",
    "\n",
    "    # Scaling logic\n",
    "    T = mem.shape[1]\n",
    "    scale = torch.sqrt(torch.arange(1, T + 1, device=mem.device, dtype=torch.float32))\n",
    "    scale = scale.view(1, T, 1, 1)\n",
    "    \n",
    "    return (mem * q).real / scale\n",
    "\n",
    "# 2. Compile it\n",
    "# fullgraph=True ensures no python fallbacks. \n",
    "# mode=\"max-autotune\" enables the most aggressive Triton optimizations.\n",
    "compiled_holo = torch.compile(holo_op, mode=\"max-autotune\", fullgraph=True)\n",
    "\n",
    "# --- Warmup ---\n",
    "print(\"Warming up...\")\n",
    "for _ in range(5):\n",
    "    _ = holo_op(v, k, q)      # Eager\n",
    "    _ = compiled_holo(v, k, q) # Compiled\n",
    "\n",
    "# --- Benchmark Eager ---\n",
    "torch.cuda.synchronize()\n",
    "start = time.time()\n",
    "for _ in range(100):\n",
    "    _ = holo_op(v, k, q)\n",
    "torch.cuda.synchronize()\n",
    "eager_time = (time.time() - start) * 10\n",
    "\n",
    "# --- Benchmark Compile ---\n",
    "torch.cuda.synchronize()\n",
    "start = time.time()\n",
    "for _ in range(100):\n",
    "    _ = compiled_holo(v, k, q)\n",
    "torch.cuda.synchronize()\n",
    "compiled_time = (time.time() - start) * 10\n",
    "\n",
    "print(f\"Eager PyTorch:   {eager_time:.3f} ms\")\n",
    "print(f\"torch.compile:   {compiled_time:.3f} ms\")\n",
    "print(f\"Speedup:         {eager_time / compiled_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b738841-9f35-4bdb-936f-80d8c2ef0ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 1. Fixed Triton Kernel\n",
    "# ==========================================\n",
    "@triton.jit\n",
    "def _holo_scan_fused_kernel(\n",
    "    v_ptr, k_ptr, q_ptr, out_ptr,\n",
    "    # Strides (Standard PyTorch strides)\n",
    "    stride_v_b, stride_v_t, stride_v_h, stride_v_d,\n",
    "    stride_k_b, stride_k_t, stride_k_h, stride_k_d,\n",
    "    stride_q_b, stride_q_t, stride_q_h, stride_q_d,\n",
    "    stride_out_b, stride_out_t, stride_out_h, stride_out_d,\n",
    "    T, H, D, \n",
    "    BLOCK_D: tl.constexpr\n",
    "):\n",
    "    # 1. Parallelize over (Batch * Head)\n",
    "    pid = tl.program_id(0)\n",
    "    \n",
    "    # 2. Decompose PID into Batch and Head indices\n",
    "    # logical grid is flattened, so we reconstruct dimensions\n",
    "    b_idx = pid // H\n",
    "    h_idx = pid % H\n",
    "    \n",
    "    # 3. Calculate Base Pointers for this (Batch, Head) specific sequence\n",
    "    # Logic: base = b * stride_b + h * stride_h\n",
    "    # This allows us to read directly from (B, T, H, D) without permuting!\n",
    "    \n",
    "    v_base = v_ptr + b_idx * stride_v_b + h_idx * stride_v_h\n",
    "    k_base = k_ptr + b_idx * stride_k_b + h_idx * stride_k_h\n",
    "    q_base = q_ptr + b_idx * stride_q_b + h_idx * stride_q_h\n",
    "    out_base = out_ptr + b_idx * stride_out_b + h_idx * stride_out_h\n",
    "\n",
    "    # 4. Setup D-dimension (contiguous block)\n",
    "    offs_d = tl.arange(0, BLOCK_D)\n",
    "    mask_d = offs_d < D\n",
    "    \n",
    "    # 5. Initialize Accumulators\n",
    "    acc_real = tl.zeros([BLOCK_D], dtype=tl.float32)\n",
    "    acc_imag = tl.zeros([BLOCK_D], dtype=tl.float32)\n",
    "\n",
    "    # 6. Sequential Loop over Time\n",
    "    for t in range(T):\n",
    "        # Stride logic: base + t * stride_t + d * stride_d\n",
    "        off_v = t * stride_v_t + offs_d * stride_v_d\n",
    "        off_k = t * stride_k_t + offs_d * stride_k_d\n",
    "        off_q = t * stride_q_t + offs_d * stride_q_d\n",
    "        \n",
    "        # Load\n",
    "        v_real = tl.load(v_base + off_v, mask=mask_d, other=0.0)\n",
    "        v_imag = tl.load(v_base + off_v + 1, mask=mask_d, other=0.0)\n",
    "        k_real = tl.load(k_base + off_k, mask=mask_d, other=0.0)\n",
    "        k_imag = tl.load(k_base + off_k + 1, mask=mask_d, other=0.0)\n",
    "        q_real = tl.load(q_base + off_q, mask=mask_d, other=0.0)\n",
    "        q_imag = tl.load(q_base + off_q + 1, mask=mask_d, other=0.0)\n",
    "\n",
    "        # Compute (v * k)\n",
    "        term_real = v_real * k_real - v_imag * k_imag\n",
    "        term_imag = v_real * k_imag + v_imag * k_real\n",
    "        \n",
    "        # Accumulate (Scan)\n",
    "        acc_real += term_real\n",
    "        acc_imag += term_imag\n",
    "        \n",
    "        # Retrieve (acc * q)\n",
    "        out_val = acc_real * q_real - acc_imag * q_imag\n",
    "        \n",
    "        # Scale\n",
    "        scale = tl.sqrt((t + 1).to(tl.float32))\n",
    "        out_val = out_val / scale\n",
    "\n",
    "        # Store\n",
    "        off_out = t * stride_out_t + offs_d * stride_out_d\n",
    "        tl.store(out_base + off_out, out_val, mask=mask_d)\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 2. Wrapper (No Copies!)\n",
    "# ==========================================\n",
    "\n",
    "def holo_op_triton(v, k, q):\n",
    "    # Input is (B, T, H, D)\n",
    "    # We DO NOT permute. We DO NOT make contiguous.\n",
    "    # We pass the original messy strides to the kernel.\n",
    "    \n",
    "    B, T, H, D = v.shape\n",
    "    \n",
    "    # View as Float32 to get float-pointer compatible strides\n",
    "    # We use a helper because .view(float32) requires contiguity on last dim usually,\n",
    "    # but since we are just passing pointers and calculating offsets manually,\n",
    "    # we can just cast the data_ptr if we are careful.\n",
    "    \n",
    "    # SAFER WAY:\n",
    "    # We still need the last dimension (D) to be contiguous for the block load `tl.load(ptr + range(0, D))`.\n",
    "    # Pytorch (B, T, H, D) is usually contiguous in D.\n",
    "    if v.stride(-1) != 1: v = v.contiguous()\n",
    "    if k.stride(-1) != 1: k = k.contiguous()\n",
    "    if q.stride(-1) != 1: q = q.contiguous()\n",
    "    \n",
    "    # Create Float32 Views (Only changes metadata, no copy if last dim is contiguous)\n",
    "    v_f = v.view(torch.float32)\n",
    "    k_f = k.view(torch.float32)\n",
    "    q_f = q.view(torch.float32)\n",
    "    \n",
    "    # Alloc Output\n",
    "    out = torch.empty((B, T, H, D), device=v.device, dtype=torch.float32)\n",
    "    \n",
    "    # Grid: B * H\n",
    "    grid = (B * H, )\n",
    "    BLOCK_D = triton.next_power_of_2(D)\n",
    "    \n",
    "    # Strides:\n",
    "    # Remember our float view doubled the last stride (because 1 complex = 2 floats).\n",
    "    # But wait, v.stride() returns stride in \"elements\".\n",
    "    # If v is complex64, v.stride(-1) is 1. \n",
    "    # v_f is float32, v_f.stride(-1) is 1.\n",
    "    # BUT we need to jump 2 floats to get to the next real number in our kernel logic?\n",
    "    # Actually:\n",
    "    # In Kernel: offs_d * stride_v_d.\n",
    "    # We want offs_d=1 to point to the next REAL number.\n",
    "    # In memory: [R0, I0, R1, I1].\n",
    "    # R0 is at 0. R1 is at 2.\n",
    "    # So stride_d MUST be 2.\n",
    "    \n",
    "    # However, v_f.stride(-1) is 1.\n",
    "    # So we must manually adjust strides for the kernel.\n",
    "    # Specifically, multiply all strides by 2 because we are treating complex* as float*.\n",
    "    # EXCEPT if the original stride was 1 (dense), it becomes 1 in float view... \n",
    "    # No, strictly speaking:\n",
    "    # 1 complex element = 8 bytes.\n",
    "    # 1 float element = 4 bytes.\n",
    "    # Pointer arithmetic in Triton is on the type (float32).\n",
    "    # To jump 1 complex element (8 bytes), we must jump 2 float elements.\n",
    "    # So YES, we multiply ALL input strides by 2.\n",
    "    \n",
    "    def get_strides(x):\n",
    "        return (x.stride(0)*2, x.stride(1)*2, x.stride(2)*2, x.stride(3)*2)\n",
    "\n",
    "    s_v = get_strides(v)\n",
    "    s_k = get_strides(k)\n",
    "    s_q = get_strides(q)\n",
    "    \n",
    "    # Output is float32, so its strides are native. \n",
    "    # BUT our kernel logic uses stride_out_d to jump output elements.\n",
    "    # Output is just Real numbers. [Out0, Out1].\n",
    "    # So stride is 1.\n",
    "    s_out = out.stride()\n",
    "    \n",
    "    _holo_scan_fused_kernel[grid](\n",
    "        v_f, k_f, q_f, out,\n",
    "        *s_v,\n",
    "        *s_k,\n",
    "        *s_q,\n",
    "        *s_out,\n",
    "        T=T, H=H, D=D,\n",
    "        BLOCK_D=BLOCK_D\n",
    "    )\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fdbdf56-874e-4f1a-b4b7-848bb1cc80b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def _holo_scan_fused_pipelined_kernel(\n",
    "    v_ptr, k_ptr, q_ptr, out_ptr,\n",
    "    # Strides\n",
    "    stride_v_b, stride_v_h, stride_v_t, stride_v_d,\n",
    "    stride_k_b, stride_k_h, stride_k_t, stride_k_d,\n",
    "    stride_q_b, stride_q_h, stride_q_t, stride_q_d,\n",
    "    stride_out_b, stride_out_h, stride_out_t, stride_out_d,\n",
    "    T, H, D, \n",
    "    BLOCK_D: tl.constexpr,\n",
    "    UNROLL: tl.constexpr\n",
    "):\n",
    "    pid = tl.program_id(0)\n",
    "    b_idx = pid // H\n",
    "    h_idx = pid % H\n",
    "\n",
    "    # 1. Pointer Setup (Offset to Batch/Head)\n",
    "    # We maintain \"Base Pointers\" that point to the start of the current unroll block\n",
    "    off_v = b_idx * stride_v_b + h_idx * stride_v_h\n",
    "    off_k = b_idx * stride_k_b + h_idx * stride_k_h\n",
    "    off_q = b_idx * stride_q_b + h_idx * stride_q_h\n",
    "    off_out = b_idx * stride_out_b + h_idx * stride_out_h\n",
    "\n",
    "    v_ptr += off_v\n",
    "    k_ptr += off_k\n",
    "    q_ptr += off_q\n",
    "    out_ptr += off_out\n",
    "\n",
    "    # 2. Masks\n",
    "    offs_mem = tl.arange(0, BLOCK_D)\n",
    "    mask_mem = offs_mem < D\n",
    "    \n",
    "    offs_out = tl.arange(0, BLOCK_D)\n",
    "    mask_out = offs_out < D\n",
    "\n",
    "    # 3. Accumulators\n",
    "    acc_real = tl.zeros([BLOCK_D], dtype=tl.float32)\n",
    "    acc_imag = tl.zeros([BLOCK_D], dtype=tl.float32)\n",
    "\n",
    "    # 4. Cast pointers to int64 once for cleaner syntax\n",
    "    # Note: We will do pointer arithmetic on the base pointers (float32 type)\n",
    "    # and only cast to int64 right before loading.\n",
    "    \n",
    "    # 5. Main Pipelined Loop\n",
    "    for t in range(0, T - (T % UNROLL), UNROLL):\n",
    "        \n",
    "        # --- STAGE 1: ISSUE ALL LOADS (Latency Hiding) ---\n",
    "        # We calculate offsets for all UNROLL steps relative to the current base pointer\n",
    "        # This breaks the dependency chain.\n",
    "        \n",
    "        # Step 0\n",
    "        v_ptr_0 = v_ptr.to(tl.pointer_type(tl.int64))\n",
    "        k_ptr_0 = k_ptr.to(tl.pointer_type(tl.int64))\n",
    "        q_ptr_0 = q_ptr.to(tl.pointer_type(tl.int64))\n",
    "        \n",
    "        v_pack_0 = tl.load(v_ptr_0 + offs_mem, mask=mask_mem, other=0)\n",
    "        k_pack_0 = tl.load(k_ptr_0 + offs_mem, mask=mask_mem, other=0)\n",
    "        q_pack_0 = tl.load(q_ptr_0 + offs_mem, mask=mask_mem, other=0)\n",
    "\n",
    "        # Step 1 (Offset by stride_t)\n",
    "        v_ptr_1 = (v_ptr + stride_v_t).to(tl.pointer_type(tl.int64))\n",
    "        k_ptr_1 = (k_ptr + stride_k_t).to(tl.pointer_type(tl.int64))\n",
    "        q_ptr_1 = (q_ptr + stride_q_t).to(tl.pointer_type(tl.int64))\n",
    "\n",
    "        v_pack_1 = tl.load(v_ptr_1 + offs_mem, mask=mask_mem, other=0)\n",
    "        k_pack_1 = tl.load(k_ptr_1 + offs_mem, mask=mask_mem, other=0)\n",
    "        q_pack_1 = tl.load(q_ptr_1 + offs_mem, mask=mask_mem, other=0)\n",
    "\n",
    "        # Step 2\n",
    "        v_ptr_2 = (v_ptr + 2*stride_v_t).to(tl.pointer_type(tl.int64))\n",
    "        k_ptr_2 = (k_ptr + 2*stride_k_t).to(tl.pointer_type(tl.int64))\n",
    "        q_ptr_2 = (q_ptr + 2*stride_q_t).to(tl.pointer_type(tl.int64))\n",
    "\n",
    "        v_pack_2 = tl.load(v_ptr_2 + offs_mem, mask=mask_mem, other=0)\n",
    "        k_pack_2 = tl.load(k_ptr_2 + offs_mem, mask=mask_mem, other=0)\n",
    "        q_pack_2 = tl.load(q_ptr_2 + offs_mem, mask=mask_mem, other=0)\n",
    "\n",
    "        # Step 3\n",
    "        v_ptr_3 = (v_ptr + 3*stride_v_t).to(tl.pointer_type(tl.int64))\n",
    "        k_ptr_3 = (k_ptr + 3*stride_k_t).to(tl.pointer_type(tl.int64))\n",
    "        q_ptr_3 = (q_ptr + 3*stride_q_t).to(tl.pointer_type(tl.int64))\n",
    "\n",
    "        v_pack_3 = tl.load(v_ptr_3 + offs_mem, mask=mask_mem, other=0)\n",
    "        k_pack_3 = tl.load(k_ptr_3 + offs_mem, mask=mask_mem, other=0)\n",
    "        q_pack_3 = tl.load(q_ptr_3 + offs_mem, mask=mask_mem, other=0)\n",
    "\n",
    "        # --- STAGE 2: PROCESS & ACCUMULATE ---\n",
    "        # Now that loads are issued, we can process.\n",
    "        # The dependency on 'acc' forces this part to be sequential,\n",
    "        # but the memory loads for step 1, 2, 3 are arriving in the background.\n",
    "\n",
    "        # --- Iteration 0 ---\n",
    "        v_r = v_pack_0.to(tl.int32).to(tl.float32, bitcast=True)\n",
    "        v_i = (v_pack_0 >> 32).to(tl.int32).to(tl.float32, bitcast=True)\n",
    "        k_r = k_pack_0.to(tl.int32).to(tl.float32, bitcast=True)\n",
    "        k_i = (k_pack_0 >> 32).to(tl.int32).to(tl.float32, bitcast=True)\n",
    "        \n",
    "        acc_real += v_r * k_r - v_i * k_i\n",
    "        acc_imag += v_r * k_i + v_i * k_r\n",
    "        \n",
    "        q_r = q_pack_0.to(tl.int32).to(tl.float32, bitcast=True)\n",
    "        q_i = (q_pack_0 >> 32).to(tl.int32).to(tl.float32, bitcast=True)\n",
    "        \n",
    "        out_val = acc_real * q_r - acc_imag * q_i\n",
    "        scale = tl.rsqrt((t + 1).to(tl.float32))\n",
    "        tl.store(out_ptr + offs_out * stride_out_d, out_val * scale, mask=mask_out)\n",
    "\n",
    "        # --- Iteration 1 ---\n",
    "        v_r = v_pack_1.to(tl.int32).to(tl.float32, bitcast=True)\n",
    "        v_i = (v_pack_1 >> 32).to(tl.int32).to(tl.float32, bitcast=True)\n",
    "        k_r = k_pack_1.to(tl.int32).to(tl.float32, bitcast=True)\n",
    "        k_i = (k_pack_1 >> 32).to(tl.int32).to(tl.float32, bitcast=True)\n",
    "\n",
    "        acc_real += v_r * k_r - v_i * k_i\n",
    "        acc_imag += v_r * k_i + v_i * k_r\n",
    "        \n",
    "        q_r = q_pack_1.to(tl.int32).to(tl.float32, bitcast=True)\n",
    "        q_i = (q_pack_1 >> 32).to(tl.int32).to(tl.float32, bitcast=True)\n",
    "\n",
    "        out_val = acc_real * q_r - acc_imag * q_i\n",
    "        scale = tl.rsqrt((t + 2).to(tl.float32))\n",
    "        tl.store(out_ptr + stride_out_t + offs_out * stride_out_d, out_val * scale, mask=mask_out)\n",
    "\n",
    "        # --- Iteration 2 ---\n",
    "        v_r = v_pack_2.to(tl.int32).to(tl.float32, bitcast=True)\n",
    "        v_i = (v_pack_2 >> 32).to(tl.int32).to(tl.float32, bitcast=True)\n",
    "        k_r = k_pack_2.to(tl.int32).to(tl.float32, bitcast=True)\n",
    "        k_i = (k_pack_2 >> 32).to(tl.int32).to(tl.float32, bitcast=True)\n",
    "\n",
    "        acc_real += v_r * k_r - v_i * k_i\n",
    "        acc_imag += v_r * k_i + v_i * k_r\n",
    "        \n",
    "        q_r = q_pack_2.to(tl.int32).to(tl.float32, bitcast=True)\n",
    "        q_i = (q_pack_2 >> 32).to(tl.int32).to(tl.float32, bitcast=True)\n",
    "\n",
    "        out_val = acc_real * q_r - acc_imag * q_i\n",
    "        scale = tl.rsqrt((t + 3).to(tl.float32))\n",
    "        tl.store(out_ptr + 2*stride_out_t + offs_out * stride_out_d, out_val * scale, mask=mask_out)\n",
    "\n",
    "        # --- Iteration 3 ---\n",
    "        v_r = v_pack_3.to(tl.int32).to(tl.float32, bitcast=True)\n",
    "        v_i = (v_pack_3 >> 32).to(tl.int32).to(tl.float32, bitcast=True)\n",
    "        k_r = k_pack_3.to(tl.int32).to(tl.float32, bitcast=True)\n",
    "        k_i = (k_pack_3 >> 32).to(tl.int32).to(tl.float32, bitcast=True)\n",
    "\n",
    "        acc_real += v_r * k_r - v_i * k_i\n",
    "        acc_imag += v_r * k_i + v_i * k_r\n",
    "        \n",
    "        q_r = q_pack_3.to(tl.int32).to(tl.float32, bitcast=True)\n",
    "        q_i = (q_pack_3 >> 32).to(tl.int32).to(tl.float32, bitcast=True)\n",
    "\n",
    "        out_val = acc_real * q_r - acc_imag * q_i\n",
    "        scale = tl.rsqrt((t + 4).to(tl.float32))\n",
    "        tl.store(out_ptr + 3*stride_out_t + offs_out * stride_out_d, out_val * scale, mask=mask_out)\n",
    "\n",
    "        # --- STAGE 3: POINTER UPDATE ---\n",
    "        # Move pointers forward by UNROLL steps\n",
    "        v_ptr += UNROLL * stride_v_t\n",
    "        k_ptr += UNROLL * stride_k_t\n",
    "        q_ptr += UNROLL * stride_q_t\n",
    "        out_ptr += UNROLL * stride_out_t\n",
    "\n",
    "    # 6. Remainder Loop (Simple, no pipeline needed for last <4 items)\n",
    "    for t in range(T - (T % UNROLL), T):\n",
    "        v_ptr_i64 = v_ptr.to(tl.pointer_type(tl.int64))\n",
    "        k_ptr_i64 = k_ptr.to(tl.pointer_type(tl.int64))\n",
    "        q_ptr_i64 = q_ptr.to(tl.pointer_type(tl.int64))\n",
    "\n",
    "        v_packed = tl.load(v_ptr_i64 + offs_mem, mask=mask_mem, other=0)\n",
    "        k_packed = tl.load(k_ptr_i64 + offs_mem, mask=mask_mem, other=0)\n",
    "        q_packed = tl.load(q_ptr_i64 + offs_mem, mask=mask_mem, other=0)\n",
    "\n",
    "        v_r = v_packed.to(tl.int32).to(tl.float32, bitcast=True)\n",
    "        v_i = (v_packed >> 32).to(tl.int32).to(tl.float32, bitcast=True)\n",
    "        k_r = k_packed.to(tl.int32).to(tl.float32, bitcast=True)\n",
    "        k_i = (k_packed >> 32).to(tl.int32).to(tl.float32, bitcast=True)\n",
    "        q_r = q_packed.to(tl.int32).to(tl.float32, bitcast=True)\n",
    "        q_i = (q_packed >> 32).to(tl.int32).to(tl.float32, bitcast=True)\n",
    "\n",
    "        acc_real += v_r * k_r - v_i * k_i\n",
    "        acc_imag += v_r * k_i + v_i * k_r\n",
    "        \n",
    "        out_val = acc_real * q_r - acc_imag * q_i\n",
    "        scale = tl.rsqrt((t + 1).to(tl.float32))\n",
    "        tl.store(out_ptr + offs_out * stride_out_d, out_val * scale, mask=mask_out)\n",
    "\n",
    "        v_ptr += stride_v_t\n",
    "        k_ptr += stride_k_t\n",
    "        q_ptr += stride_q_t\n",
    "        out_ptr += stride_out_t\n",
    "\n",
    "def holo_triton_pipelined_run(v: torch.Tensor, k: torch.Tensor, q: torch.Tensor):\n",
    "    if not v.is_contiguous(): v = v.contiguous()\n",
    "    if not k.is_contiguous(): k = k.contiguous()\n",
    "    if not q.is_contiguous(): q = q.contiguous()\n",
    "    \n",
    "    B, T, H, D = v.shape\n",
    "    out = torch.empty((B, T, H, D), device=v.device, dtype=torch.float32)\n",
    "    \n",
    "    def get_float_strides(x):\n",
    "        return (x.stride(0) * 2, x.stride(1) * 2, x.stride(2) * 2, x.stride(3) * 2)\n",
    "\n",
    "    s_v = get_float_strides(v)\n",
    "    s_k = get_float_strides(k)\n",
    "    s_q = get_float_strides(q)\n",
    "    s_out = out.stride()\n",
    "\n",
    "    grid = (B * H, )\n",
    "    BLOCK_D = triton.next_power_of_2(D)\n",
    "    UNROLL = 4\n",
    "    num_warps = 4 if D <= 64 else 8\n",
    "\n",
    "    _holo_scan_fused_pipelined_kernel[grid](\n",
    "        v.view(torch.float32), \n",
    "        k.view(torch.float32), \n",
    "        q.view(torch.float32), \n",
    "        out,\n",
    "        s_v[0], s_v[2], s_v[1], s_v[3],\n",
    "        s_k[0], s_k[2], s_k[1], s_k[3],\n",
    "        s_q[0], s_q[2], s_q[1], s_q[3],\n",
    "        s_out[0], s_out[2], s_out[1], s_out[3],\n",
    "        T=T, H=H, D=D,\n",
    "        BLOCK_D=BLOCK_D,\n",
    "        UNROLL=UNROLL,\n",
    "        num_warps=num_warps,\n",
    "        num_stages=3\n",
    "    )\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d113c6cb-522d-4269-9c6d-bb55e91e9dbf",
   "metadata": {},
   "source": [
    "### Autotune "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3a6ca03-eeff-45e9-86e5-b1be4bca8d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# AUTOTUNE CONFIGURATION\n",
    "# -------------------------------------------------------------------------\n",
    "configs = [\n",
    "    triton.Config({'UNROLL': 2}, num_warps=4, num_stages=3),\n",
    "    triton.Config({'UNROLL': 4}, num_warps=4, num_stages=3),\n",
    "    triton.Config({'UNROLL': 4}, num_warps=8, num_stages=3),\n",
    "    triton.Config({'UNROLL': 8}, num_warps=4, num_stages=3),\n",
    "    triton.Config({'UNROLL': 8}, num_warps=8, num_stages=3),\n",
    "]\n",
    "\n",
    "@triton.autotune(configs=configs, key=['T', 'H', 'D'])\n",
    "@triton.jit\n",
    "def _holo_scan_fused_autotuned_kernel(\n",
    "    v_ptr, k_ptr, q_ptr, out_ptr,\n",
    "    stride_v_b, stride_v_h, stride_v_t, stride_v_d,\n",
    "    stride_k_b, stride_k_h, stride_k_t, stride_k_d,\n",
    "    stride_q_b, stride_q_h, stride_q_t, stride_q_d,\n",
    "    stride_out_b, stride_out_h, stride_out_t, stride_out_d,\n",
    "    T, H, D, \n",
    "    BLOCK_D: tl.constexpr,\n",
    "    UNROLL: tl.constexpr\n",
    "):\n",
    "    pid = tl.program_id(0)\n",
    "    b_idx = pid // H\n",
    "    h_idx = pid % H\n",
    "\n",
    "    # 1. Base Pointers\n",
    "    v_ptr += b_idx * stride_v_b + h_idx * stride_v_h\n",
    "    k_ptr += b_idx * stride_k_b + h_idx * stride_k_h\n",
    "    q_ptr += b_idx * stride_q_b + h_idx * stride_q_h\n",
    "    out_ptr += b_idx * stride_out_b + h_idx * stride_out_h\n",
    "\n",
    "    offs_mem = tl.arange(0, BLOCK_D)\n",
    "    mask_mem = offs_mem < D\n",
    "    \n",
    "    acc_real = tl.zeros([BLOCK_D], dtype=tl.float32)\n",
    "    acc_imag = tl.zeros([BLOCK_D], dtype=tl.float32)\n",
    "\n",
    "    # 2. Main Loop\n",
    "    for t in range(0, T - (T % UNROLL), UNROLL):\n",
    "        \n",
    "        # Unroll the Load-Compute-Store block\n",
    "        for j in range(UNROLL):\n",
    "            curr_t = t + j\n",
    "\n",
    "            # --- LOAD and UNPACK ---\n",
    "            # Use tl.advance or explicit pointer math with correct typing\n",
    "            # We cast to int64 pointer to ensure we load 8 bytes (real + imag)\n",
    "            p_v = (v_ptr + j * stride_v_t).to(tl.pointer_type(tl.int64))\n",
    "            p_k = (k_ptr + j * stride_k_t).to(tl.pointer_type(tl.int64))\n",
    "            p_q = (q_ptr + j * stride_q_t).to(tl.pointer_type(tl.int64))\n",
    "\n",
    "            # Load as int64\n",
    "            v_pack = tl.load(p_v + offs_mem, mask=mask_mem, other=0).to(tl.int64)\n",
    "            k_pack = tl.load(p_k + offs_mem, mask=mask_mem, other=0).to(tl.int64)\n",
    "            q_pack = tl.load(p_q + offs_mem, mask=mask_mem, other=0).to(tl.int64)\n",
    "\n",
    "            \n",
    "            # --- UNPACK ---\n",
    "            # Cast to int32 to isolate the bottom 32 bits (Real)\n",
    "            v_r = v_pack.to(tl.int32).to(tl.float32, bitcast=True)\n",
    "            # Shift and cast to isolate top 32 bits (Imag)\n",
    "            v_i = (v_pack >> 32).to(tl.int32).to(tl.float32, bitcast=True)\n",
    "            \n",
    "            k_r = k_pack.to(tl.int32).to(tl.float32, bitcast=True)\n",
    "            k_i = (k_pack >> 32).to(tl.int32).to(tl.float32, bitcast=True)\n",
    "            \n",
    "            q_r = q_pack.to(tl.int32).to(tl.float32, bitcast=True)\n",
    "            q_i = (q_pack >> 32).to(tl.int32).to(tl.float32, bitcast=True)\n",
    "\n",
    "            # --- MATH ---\n",
    "            # (acc_r + i*acc_i) += (v_r + i*v_i) * (k_r + i*k_i)\n",
    "            acc_real += v_r * k_r - v_i * k_i\n",
    "            acc_imag += v_r * k_i + v_i * k_r\n",
    "            \n",
    "            # out = (acc_r + i*acc_i) * (q_r + i*q_i)\n",
    "            out_val = acc_real * q_r - acc_imag * q_i\n",
    "            scale = tl.rsqrt((curr_t + 1).to(tl.float32))\n",
    "            \n",
    "            # --- STORE ---\n",
    "            # Note: out_ptr is float32, so stride_out_d is used for the D dimension\n",
    "            # and stride_out_t for the time dimension\n",
    "            tl.store(out_ptr + j * stride_out_t + offs_mem, out_val * scale, mask=mask_mem)\n",
    "            \n",
    "        # --- ADVANCE POINTERS ---\n",
    "        v_ptr += UNROLL * stride_v_t\n",
    "        k_ptr += UNROLL * stride_k_t\n",
    "        q_ptr += UNROLL * stride_q_t\n",
    "        out_ptr += UNROLL * stride_out_t\n",
    "\n",
    "    # 3. Remainder Loop (Exact same logic, single step)\n",
    "    for t in range(T - (T % UNROLL), T):\n",
    "        v_pack = tl.load(v_ptr + offs_mem, mask=mask_mem, other=0).to(tl.int64)\n",
    "        k_pack = tl.load(k_ptr + offs_mem, mask=mask_mem, other=0).to(tl.int64)\n",
    "        q_pack = tl.load(q_ptr + offs_mem, mask=mask_mem, other=0).to(tl.int64)\n",
    "        \n",
    "        v_r = v_pack.to(tl.int32).to(tl.float32, bitcast=True)\n",
    "        v_i = (v_pack >> 32).to(tl.int32).to(tl.float32, bitcast=True)\n",
    "        k_r = k_pack.to(tl.int32).to(tl.float32, bitcast=True)\n",
    "        k_i = (k_pack >> 32).to(tl.int32).to(tl.float32, bitcast=True)\n",
    "        q_r = q_pack.to(tl.int32).to(tl.float32, bitcast=True)\n",
    "        q_i = (q_pack >> 32).to(tl.int32).to(tl.float32, bitcast=True)\n",
    "\n",
    "        acc_real += v_r * k_r - v_i * k_i\n",
    "        acc_imag += v_r * k_i + v_i * k_r\n",
    "        out_val = acc_real * q_r - acc_imag * q_i\n",
    "        scale = tl.rsqrt((t + 1).to(tl.float32))\n",
    "        \n",
    "        tl.store(out_ptr + offs_mem, out_val * scale, mask=mask_mem)\n",
    "        \n",
    "        v_ptr += stride_v_t\n",
    "        k_ptr += stride_k_t\n",
    "        q_ptr += stride_q_t\n",
    "        out_ptr += stride_out_t        \n",
    "        \n",
    "\n",
    "def holo_triton_autotuned_run(v: torch.Tensor, k: torch.Tensor, q: torch.Tensor):\n",
    "    if not v.is_contiguous(): v = v.contiguous()\n",
    "    if not k.is_contiguous(): k = k.contiguous()\n",
    "    if not q.is_contiguous(): q = q.contiguous()\n",
    "    \n",
    "    B, T, H, D = v.shape\n",
    "    out = torch.empty((B, T, H, D), device=v.device, dtype=torch.float32)\n",
    "    \n",
    "    def get_float_strides(x):\n",
    "        return (x.stride(0) * 2, x.stride(1) * 2, x.stride(2) * 2, x.stride(3) * 2)\n",
    "\n",
    "    s_v = get_float_strides(v)\n",
    "    s_k = get_float_strides(k)\n",
    "    s_q = get_float_strides(q)\n",
    "    s_out = out.stride()\n",
    "\n",
    "    grid = (B * H, )\n",
    "    BLOCK_D = triton.next_power_of_2(D)\n",
    "    \n",
    "    _holo_scan_fused_autotuned_kernel[grid](\n",
    "        v.view(torch.float32), \n",
    "        k.view(torch.float32), \n",
    "        q.view(torch.float32), \n",
    "        out,\n",
    "        s_v[0], s_v[2], s_v[1], s_v[3],\n",
    "        s_k[0], s_k[2], s_k[1], s_k[3],\n",
    "        s_q[0], s_q[2], s_q[1], s_q[3],\n",
    "        s_out[0], s_out[2], s_out[1], s_out[3],\n",
    "        T=T, H=H, D=D,\n",
    "        BLOCK_D=BLOCK_D\n",
    "    )\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2390474f-89ca-4394-9238-c8f5b4380866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying correctness...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'holo_op_eager' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m k = torch.randn(B, T, H, D, dtype=torch.complex64, device=\u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      8\u001b[39m q = torch.randn(B, T, H, D, dtype=torch.complex64, device=\u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m out_ref = \u001b[43mholo_op_eager\u001b[49m(v, k, q)\n\u001b[32m     11\u001b[39m out_tri = holo_triton_autotuned_run(v, k, q)\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Diff check\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'holo_op_eager' is not defined"
     ]
    }
   ],
   "source": [
    "torch.set_float32_matmul_precision('high')\n",
    "    \n",
    "# 1. Verification\n",
    "print(\"Verifying correctness...\")\n",
    "B, T, H, D = 2, 128, 4, 32\n",
    "v = torch.randn(B, T, H, D, dtype=torch.complex64, device='cuda')\n",
    "k = torch.randn(B, T, H, D, dtype=torch.complex64, device='cuda')\n",
    "q = torch.randn(B, T, H, D, dtype=torch.complex64, device='cuda')\n",
    "\n",
    "out_ref = holo_op_eager(v, k, q)\n",
    "out_tri = holo_triton_autotuned_run(v, k, q)\n",
    "\n",
    "# Diff check\n",
    "diff = (out_ref - out_tri).abs().max()\n",
    "print(f\"Max Diff: {diff:.6f}\")\n",
    "assert diff < 1e-2, \"Triton logic is incorrect!\"\n",
    "print(\"Verification passed! Running benchmarks...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed62b5ea-ad0d-4625-8942-31090202354a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 1. Implementations\n",
    "# ==========================================\n",
    "\n",
    "# --- A. Eager Implementation ---\n",
    "def holo_op_eager(v, k, q):\n",
    "    # 1. Bind & Accumulate (The Bottleneck: Writes massive tensor to HBM)\n",
    "    mem = torch.cumsum(v * k, dim=1)\n",
    "    \n",
    "    # 2. Scaling\n",
    "    T = mem.shape[1]\n",
    "    scale = torch.sqrt(torch.arange(1, T + 1, device=mem.device, dtype=torch.float32))\n",
    "    scale = scale.view(1, T, 1, 1)\n",
    "    \n",
    "    # 3. Retrieve\n",
    "    return (mem * q).real / scale\n",
    "\n",
    "# Compiled version needs to be created dynamically or globally\n",
    "holo_compiled = torch.compile(holo_op_eager, mode=\"max-autotune\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741a0d11-dfca-4215-a5fa-9297742b738d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 3. Triton Benchmark Suite\n",
    "# ==========================================\n",
    "\n",
    "# We benchmark over Sequence Length (T) as it's the most critical dimension for Scans\n",
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=['T'],              # Argument to vary\n",
    "        x_vals=[512, 1024, 2048, 4096, 8192, 16384], # Different values for T\n",
    "        line_arg='provider',        # Argument to determine the line color\n",
    "        # line_vals=['eager', 'compiled', 'triton', 'flash-attention'],\n",
    "        # line_names=['PyTorch Eager', 'Torch Compile', 'Triton', 'Flash Attention'],\n",
    "        # styles=[('blue', '-'), ('green', '-'), ('red', '-'), ('orange', '-')],\n",
    "        line_vals=['eager', 'compiled', 'triton-old', 'triton-new', \"triton-super\"],\n",
    "        line_names=['PyTorch Eager', 'Torch Compile', 'Triton-Old', 'Triton-New', \"Triton-Super\"],\n",
    "        styles=[('blue', '-'), ('green', '-'), ('red', '-'), ('orange', '-'), (\"black\", \"-\")], \n",
    "        ylabel='Runtime (ms)', \n",
    "        plot_name='holo_scan_performance',\n",
    "        args={'B': 16, 'H': 12, 'D': 64} # Fixed arguments\n",
    "    )\n",
    ")\n",
    "def benchmark(B, T, H, D, provider):\n",
    "    # Generate Inputs\n",
    "    device = torch.device(\"cuda\")\n",
    "    v = torch.randn(B, T, H, D, dtype=torch.complex64, device=device)\n",
    "    k = torch.randn(B, T, H, D, dtype=torch.complex64, device=device)\n",
    "    q = torch.randn(B, T, H, D, dtype=torch.complex64, device=device)\n",
    "    \n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    \n",
    "    if provider == 'eager':\n",
    "        # do_bench automatically handles warmup and repetitions\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: holo_op_eager(v, k, q), quantiles=quantiles)\n",
    "    elif provider == 'compiled':\n",
    "        # do_bench will run it multiple times, so compilation overhead (first run) \n",
    "        # is amortized/ignored by warmup\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: holo_compiled(v, k, q), quantiles=quantiles)\n",
    "\n",
    "    elif provider == 'triton-old':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: holo_op_triton(v, k, q), quantiles=quantiles)\n",
    "\n",
    "    elif provider == 'triton-new':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: holo_triton_pipelined_run(v, k, q), quantiles=quantiles)\n",
    "\n",
    "    elif provider == 'triton-super':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: holo_triton_autotuned_run(v, k, q), quantiles=quantiles)\n",
    "\n",
    "    return ms, max_ms, min_ms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50e70c9-3991-4538-a186-1fa704df3fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('high')\n",
    "    \n",
    "# 1. Verification\n",
    "print(\"Verifying correctness...\")\n",
    "B, T, H, D = 2, 128, 4, 256\n",
    "v = torch.randn(B, T, H, D, dtype=torch.complex64, device='cuda')\n",
    "k = torch.randn(B, T, H, D, dtype=torch.complex64, device='cuda')\n",
    "q = torch.randn(B, T, H, D, dtype=torch.complex64, device='cuda')\n",
    "\n",
    "out_ref = holo_op_eager(v, k, q)\n",
    "out_tri = holo_triton_optimized(v, k, q)\n",
    "\n",
    "# Diff check\n",
    "diff = (out_ref - out_tri).abs().max()\n",
    "print(f\"Max Diff: {diff:.6f}\")\n",
    "assert diff < 1e-2, \"Triton logic is incorrect!\"\n",
    "print(\"Verification passed! Running benchmarks...\")\n",
    "\n",
    "# 2. Run Benchmark\n",
    "# This will run the benchmark and save a .png file locally\n",
    "benchmark.run(print_data=True, show_plots=True, save_path='.')\n",
    "print(\"Benchmark complete. Results saved to 'holo_scan_performance.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59fab18-14d0-49ce-b3f8-35275eee879f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My Project Name",
   "language": "python",
   "name": "my_project_name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
