{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55f41874-e95c-4420-983c-17b4fe343fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b418c5c-6a0e-4494-9f8c-38233cf207e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from model.holo import HoloConfig, HoloForCausalLM\n",
    "# from model.long import LongConfig, LongForCausalLM\n",
    "from model.long_new import LongConfig, LongHFModel\n",
    "from transformers import MambaConfig, MambaForCausalLM\n",
    "from transformers import GPT2Config, GPT2LMHeadModel\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm \n",
    "import numpy as np\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90228299-4680-48ed-95bc-75465a3477a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeedleHaystackDataset(Dataset):\n",
    "    def __init__(self, size=2000, min_len=32, max_len=64, vocab_size=128, depth=None):\n",
    "        \"\"\"\n",
    "        depth: Float between 0.0 and 1.0. \n",
    "               If None, depth is randomized for every sample (0% to 100%).\n",
    "               If set (e.g., 0.5), the needle is always placed at 50% context.\n",
    "        \"\"\"\n",
    "        self.size = size\n",
    "        self.min_len = min_len\n",
    "        self.max_len = max_len\n",
    "        self.vocab_size = vocab_size\n",
    "        self.depth = depth\n",
    "        \n",
    "        # We reserve the last token as the specific \"Prompt\" trigger\n",
    "        self.prompt_token = torch.tensor([vocab_size - 1]) \n",
    "        \n",
    "    def __len__(self): \n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 1. Determine total length of the sequence\n",
    "        curr_len = np.random.randint(self.min_len, self.max_len + 1)\n",
    "        \n",
    "        # 2. Generate the Key (Needle)\n",
    "        # Range: [1, vocab-2] to avoid padding (0) and prompt token (vocab-1)\n",
    "        key = torch.randint(1, self.vocab_size - 1, (1,))\n",
    "        \n",
    "        # 3. Generate Noise (Haystack)\n",
    "        # We need space for 2 prompt tokens and 2 key tokens (4 tokens total overhead)\n",
    "        noise_len = max(0, curr_len - 4)\n",
    "        noise = torch.randint(1, self.vocab_size - 1, (noise_len,))\n",
    "        \n",
    "        # 4. Determine Insertion Point (Depth)\n",
    "        if self.depth is not None:\n",
    "            # Fixed depth (e.g., 0.9 for 90% deep)\n",
    "            insert_idx = int(noise_len * self.depth)\n",
    "        else:\n",
    "            # Fully Random depth (0% to 100%)\n",
    "            # FIX: Previously this was noise_len // 2 (biased to start)\n",
    "            insert_idx = torch.randint(0, noise_len + 1, (1,)).item()\n",
    "        \n",
    "        # 5. Construct Sequence\n",
    "        # [Noise Part A] -> [Prompt] -> [Key] -> [Noise Part B] -> [Prompt] -> [Key (Target)]\n",
    "        input_ids = torch.cat([\n",
    "            noise[:insert_idx], \n",
    "            self.prompt_token, key,      \n",
    "            noise[insert_idx:], \n",
    "            self.prompt_token, key       \n",
    "        ])\n",
    "        \n",
    "        # 6. Create Labels (Mask everything except the final Key)\n",
    "        labels = input_ids.clone()\n",
    "        # Mask everything up to the final token\n",
    "        labels[:-1] = -100 \n",
    "        \n",
    "        return {\"input_ids\": input_ids, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dd2466b-d5f3-4dc1-b9cf-06a8f649f204",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiScaleLongAttention(nn.Module):\n",
    "    def __init__(self, dim, heads):\n",
    "        super().__init__()\n",
    "        self.h = heads\n",
    "        self.d = dim // heads\n",
    "        \n",
    "        self.q_proj = nn.Linear(dim, dim, bias=False)\n",
    "        self.k_proj = nn.Linear(dim, dim, bias=False)\n",
    "        self.v_proj = nn.Linear(dim, dim, bias=False)\n",
    "        self.o = nn.Linear(dim, dim, bias=False)\n",
    "        \n",
    "        # Local Conv for \"Trigger Detection\"\n",
    "        self.conv = nn.Conv1d(dim, dim, kernel_size=4, groups=dim, padding=3)\n",
    "        self.input_gate = nn.Linear(dim, dim, bias=True)\n",
    "        \n",
    "        # Spectral Gammas: [0.9 ... 1.0]\n",
    "        decays = torch.linspace(0.9, 1.0, heads)\n",
    "        decays[-1] = 1.0 # Ensure absolute stability\n",
    "        self.gamma = nn.Parameter(decays.view(1, 1, heads, 1), requires_grad=False)\n",
    "        \n",
    "        self.ln = nn.LayerNorm(dim)\n",
    "        nn.init.constant_(self.input_gate.bias, 1.0) # Start Open\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        # Short Convolution (Locality)\n",
    "        x_c = self.conv(x.transpose(1, 2))[:, :, :T].transpose(1, 2)\n",
    "        x_c = F.silu(x_c)\n",
    "        \n",
    "        q = self.q_proj(x).view(B, T, self.h, self.d)\n",
    "        k = self.k_proj(x).view(B, T, self.h, self.d)\n",
    "        v = self.v_proj(x).view(B, T, self.h, self.d)\n",
    "        \n",
    "        # Input Gate\n",
    "        i_gate = F.relu(self.input_gate(x_c)).view(B, T, self.h, self.d)\n",
    "        \n",
    "        k = F.normalize(k, p=2, dim=-1)\n",
    "        q = F.normalize(q, p=2, dim=-1)\n",
    "        \n",
    "        kv = (k * v) * i_gate \n",
    "        \n",
    "        # DUAL MODE SWITCHING\n",
    "        if T > 1024: \n",
    "            # RNN MODE (Inference - O(1) Memory)\n",
    "            out_list = []\n",
    "            state = torch.zeros(B, self.h, self.d, device=x.device)\n",
    "            gamma_sq = self.gamma.view(1, self.h, 1)\n",
    "            \n",
    "            for t in range(T):\n",
    "                kv_t = kv[:, t]\n",
    "                q_t = q[:, t]\n",
    "                state = state * gamma_sq + kv_t\n",
    "                out_t = (state * q_t).reshape(B, -1)\n",
    "                out_list.append(out_t)\n",
    "            out = torch.stack(out_list, dim=1)\n",
    "        else:\n",
    "            # PARALLEL MODE (Training - O(log N) Parallelism)\n",
    "            # This is the \"Cumsum\" trick that makes training stable\n",
    "            t_steps = torch.arange(T, device=x.device).view(1, T, 1, 1).float()\n",
    "            decay = torch.pow(self.gamma, t_steps)\n",
    "            D_inv = 1.0 / (decay + 1e-9)\n",
    "            \n",
    "            mem_unscaled = torch.cumsum(kv * D_inv, dim=1)\n",
    "            mem = mem_unscaled * decay\n",
    "            \n",
    "            mem_flat = mem.reshape(B, T, C)\n",
    "            out = (mem_flat * q.reshape(B, T, C))\n",
    "\n",
    "        return self.ln(self.o(out))\n",
    "\n",
    "class LongNet(nn.Module):\n",
    "    def __init__(self, vocab_size, dim, layers, heads):\n",
    "        super().__init__()\n",
    "        self.wte = nn.Embedding(vocab_size, dim)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.LayerNorm(dim), MultiScaleLongAttention(dim, heads), nn.LayerNorm(dim),\n",
    "                nn.Sequential(nn.Linear(dim, 4*dim), nn.GELU(), nn.Linear(4*dim, dim))\n",
    "            ) for _ in range(layers)])\n",
    "        self.head = nn.Linear(dim, vocab_size, bias=False)\n",
    "    def forward(self, idx):\n",
    "        x = self.wte(idx)\n",
    "        for block in self.blocks: \n",
    "            x = block[1](block[0](x)) + x\n",
    "            x = block[3](block[2](x)) + x\n",
    "        return self.head(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc22afc3-4ae6-4cfe-b006-d6be89f88621",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_collate_fn(batch):\n",
    "    inputs = rnn_utils.pad_sequence([x['input_ids'] for x in batch], batch_first=True, padding_value=0)\n",
    "    labels = rnn_utils.pad_sequence([x['labels'] for x in batch], batch_first=True, padding_value=-100)\n",
    "    return {\"input_ids\": inputs, \"labels\": labels}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6aa6b2b-a8a4-483f-8d09-9cec8a31c906",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 128\n",
    "DIM = 64\n",
    "LAYERS = 2\n",
    "HEADS = 4\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "config_kwargs = {\n",
    "     \"vocab_size\": VOCAB_SIZE, \n",
    "     \"ssm_cfg\": {\"dropout\": 0.0 }\n",
    "}\n",
    "mamba_config = MambaConfig(\n",
    "    hidden_size = DIM,\n",
    "    num_hidden_layers = LAYERS, \n",
    "    **config_kwargs\n",
    ")\n",
    "mamba_model = MambaForCausalLM(mamba_config).to(DEVICE)\n",
    "\n",
    "gpt2_config = GPT2Config(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    # 1. Context Window\n",
    "    # GPT-2 has a HARD limit. You must set this >= your max haystack length.\n",
    "    n_positions=8192, \n",
    "    \n",
    "    # 2. Dimensions (Matching your request)\n",
    "    n_embd=128,       # This is \"hidden_size\"\n",
    "    n_layer=2,        # This is \"num_hidden_layers\"\n",
    "    \n",
    "    # 3. Heads\n",
    "    # n_embd (128) must be divisible by n_head. \n",
    "    # 4 heads gives 32 dimension per head (standard).\n",
    "    n_head=4, \n",
    "    \n",
    "    # 4. Cleanup\n",
    "    bos_token_id=0,\n",
    "    eos_token_id=0,\n",
    "    \n",
    "    # Optional: Disable dropout for pure algorithmic testing (like your Mamba config)\n",
    "    resid_pdrop=0.0,\n",
    "    embd_pdrop=0.0,\n",
    "    attn_pdrop=0.0,\n",
    "    use_cache=False # False for training with Gradient Checkpointing, True for generation\n",
    ")\n",
    "\n",
    "model_gpt2 = GPT2LMHeadModel(gpt2_config).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9227e0d1-970b-41ca-ad3b-82a6d300b4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MambaBlock(nn.Module):\n",
    "    def __init__(self, d_model, d_state=16, d_conv=4, expand=2):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_inner = int(expand * d_model)\n",
    "        self.dt_rank = math.ceil(d_model / 16)\n",
    "        self.d_state = d_state\n",
    "        self.in_proj = nn.Linear(d_model, self.d_inner * 2, bias=False)\n",
    "        self.conv1d = nn.Conv1d(self.d_inner, self.d_inner, d_conv, groups=self.d_inner, padding=d_conv - 1)\n",
    "        self.x_proj = nn.Linear(self.d_inner, self.dt_rank + self.d_state * 2, bias=False)\n",
    "        self.dt_proj = nn.Linear(self.dt_rank, self.d_inner, bias=True)\n",
    "        \n",
    "        A = torch.arange(1, self.d_state + 1, dtype=torch.float32).repeat(self.d_inner, 1)\n",
    "        self.A_log = nn.Parameter(torch.log(A))\n",
    "        self.D = nn.Parameter(torch.ones(self.d_inner))\n",
    "        self.out_proj = nn.Linear(self.d_inner, d_model, bias=False)\n",
    "        nn.init.constant_(self.dt_proj.bias, -2.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, seqlen, dim = x.shape\n",
    "        x_and_z = self.in_proj(x)\n",
    "        x_src, z = x_and_z.chunk(2, dim=-1)\n",
    "        x_conv = self.conv1d(x_src.transpose(1, 2))[:, :, :seqlen]\n",
    "        x_conv = F.silu(x_conv)\n",
    "        x_dbl = self.x_proj(x_conv.transpose(1, 2))\n",
    "        dt, B, C = torch.split(x_dbl, [self.dt_rank, self.d_state, self.d_state], dim=-1)\n",
    "        dt = F.softplus(self.dt_proj(dt)).transpose(1, 2)\n",
    "        B = B.transpose(1, 2)\n",
    "        C = C.transpose(1, 2)\n",
    "        A = -torch.exp(self.A_log.float())\n",
    "        y = []\n",
    "        state = torch.zeros(batch, self.d_inner, self.d_state, device=x.device)\n",
    "        for t in range(seqlen):\n",
    "            dt_t = dt[:, :, t]\n",
    "            state = state * torch.exp(torch.einsum('bd, dn -> bdn', dt_t, A)) + \\\n",
    "                    torch.einsum('bd, bn -> bdn', dt_t, B[:,:,t]) * x_conv[:,:,t].unsqueeze(-1)\n",
    "            y.append(torch.einsum('bdn, bn -> bd', state, C[:,:,t]))\n",
    "        y = torch.stack(y, dim=1)\n",
    "        y = y + x_conv.transpose(1, 2) * self.D\n",
    "        return self.out_proj(y * F.silu(z))\n",
    "\n",
    "class MambaModel(nn.Module):\n",
    "    def __init__(self, vocab_size, dim, layers):\n",
    "        super().__init__()\n",
    "        self.wte = nn.Embedding(vocab_size, dim)\n",
    "        self.blocks = nn.ModuleList([nn.Sequential(MambaBlock(dim), nn.LayerNorm(dim)) for _ in range(layers)])\n",
    "        self.head = nn.Linear(dim, vocab_size, bias=False)\n",
    "    def forward(self, idx):\n",
    "        x = self.wte(idx)\n",
    "        for block in self.blocks: x = block(x) + x\n",
    "        return self.head(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4190da15-718f-4b04-8bdc-f8f3b869cf9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LongHFModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LongHFModel(\n",
      "  (wte): Embedding(128, 64)\n",
      "  (layers): ModuleList(\n",
      "    (0-1): 2 x ModuleDict(\n",
      "      (attention): LongAttention(\n",
      "        (q_proj): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (k_proj): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (v_proj): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (o_proj): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(2,), groups=64)\n",
      "        (gate_proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
      "      )\n",
      "      (ln_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (ln_f): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "  (lm_head): Linear(in_features=64, out_features=128, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "long_config = LongConfig(\n",
    "    vocab_size = VOCAB_SIZE, \n",
    "    hidden_size = 64, \n",
    "    num_hidden_layers = 2, \n",
    "    num_heads = 4,\n",
    "    hybrid_ratio = 0\n",
    ")\n",
    "long_llm = LongHFModel(long_config).to(DEVICE)\n",
    "\n",
    "print(long_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40ae6a64-c24c-4c30-97dc-08544147a51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LongNet(\n",
      "  (wte): Embedding(128, 64)\n",
      "  (blocks): ModuleList(\n",
      "    (0-1): 2 x Sequential(\n",
      "      (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (1): MultiScaleLongAttention(\n",
      "        (q_proj): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (k_proj): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (v_proj): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (o): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (conv): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)\n",
      "        (input_gate): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (3): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (head): Linear(in_features=64, out_features=128, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "long_llm = LongNet(VOCAB_SIZE, DIM, LAYERS, HEADS).to(DEVICE)\n",
    "print(long_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d455a8c-dd6b-4b87-b982-7b778b2f1dff",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LongConfig' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 124\u001b[39m\n\u001b[32m    119\u001b[39m                 results[name].append(\u001b[32m0.0\u001b[39m)\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# --- 3. Visualization ---\u001b[39;00m\n\u001b[32m    122\u001b[39m     \u001b[38;5;66;03m# (Same as your provided code, just use results dictionary)\u001b[39;00m\n\u001b[32m    123\u001b[39m     \u001b[38;5;66;03m# ...\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m \u001b[43mrun_benchmark\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mrun_benchmark\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      7\u001b[39m BATCH_SIZE = \u001b[32m64\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# --- 1. Model Setup (Hugging Face Style) ---\u001b[39;00m\n\u001b[32m     10\u001b[39m \n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Setup Long-LLM Config\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Assuming LongConfig is your custom config class\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m long_config = \u001b[43mLongConfig\u001b[49m(\n\u001b[32m     14\u001b[39m     vocab_size=VOCAB_SIZE,\n\u001b[32m     15\u001b[39m     hidden_size=DIM,\n\u001b[32m     16\u001b[39m     num_hidden_layers=LAYERS,\n\u001b[32m     17\u001b[39m     num_heads=HEADS,\n\u001b[32m     18\u001b[39m     hybrid_ratio=\u001b[32m0\u001b[39m, \u001b[38;5;66;03m# Example: every 2nd layer is RoPE\u001b[39;00m\n\u001b[32m     19\u001b[39m )\n\u001b[32m     20\u001b[39m long_llm_hf = LongHFModel(long_config).to(DEVICE)\n\u001b[32m     22\u001b[39m long_llm = LongNet(VOCAB_SIZE, DIM, LAYERS, HEADS).to(DEVICE)\n",
      "\u001b[31mNameError\u001b[39m: name 'LongConfig' is not defined"
     ]
    }
   ],
   "source": [
    "def run_benchmark():\n",
    "    VOCAB_SIZE = 1000\n",
    "    DIM = 64\n",
    "    LAYERS = 2\n",
    "    HEADS = 4\n",
    "    EPOCHS = 10\n",
    "    BATCH_SIZE = 64\n",
    "    \n",
    "    # --- 1. Model Setup (Hugging Face Style) ---\n",
    "    \n",
    "    # Setup Long-LLM Config\n",
    "    # Assuming LongConfig is your custom config class\n",
    "    long_config = LongConfig(\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        hidden_size=DIM,\n",
    "        num_hidden_layers=LAYERS,\n",
    "        num_heads=HEADS,\n",
    "        hybrid_ratio=0, # Example: every 2nd layer is RoPE\n",
    "    )\n",
    "    long_llm_hf = LongHFModel(long_config).to(DEVICE)\n",
    "\n",
    "    long_llm = LongNet(VOCAB_SIZE, DIM, LAYERS, HEADS).to(DEVICE)\n",
    "\n",
    "    # Setup Mamba HF\n",
    "    mamba_config = MambaConfig(\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        hidden_size=DIM,\n",
    "        num_hidden_layers=LAYERS,\n",
    "        ssm_cfg={\"dropout\": 0.0}\n",
    "    )\n",
    "    mamba_hf = MambaForCausalLM(mamba_config).to(DEVICE)\n",
    "\n",
    "    # Setup GPT-2 HF\n",
    "    gpt2_config = GPT2Config(\n",
    "        vocab_size=VOCAB_SIZE, n_positions=8192, n_embd=DIM, n_layer=LAYERS, n_head=HEADS,\n",
    "        resid_pdrop=0.0, embd_pdrop=0.0, attn_pdrop=0.0, use_cache=False\n",
    "    )\n",
    "    gpt2_model = GPT2LMHeadModel(gpt2_config).to(DEVICE)\n",
    "\n",
    "    models = {\n",
    "        \"Long-LLM (HF)\": long_llm_hf,\n",
    "        \"Long-LLM (NET)\": long_llm,\n",
    "        \"Mamba (HF)\": mamba_hf,\n",
    "        \"GPT-2\": gpt2_model\n",
    "    }\n",
    "    \n",
    "    test_lengths = [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768]\n",
    "    results = {name: [] for name in models}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nðŸš€ Training {name}...\")\n",
    "        \n",
    "        opt = optim.AdamW(model.parameters(), lr=0.001)\n",
    "        train_ds = NeedleHaystackDataset(size=10000, min_len=32, max_len=128, vocab_size=VOCAB_SIZE)\n",
    "        train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, collate_fn=pad_collate_fn, shuffle=True)\n",
    "        \n",
    "        model.train()\n",
    "        for ep in range(EPOCHS):\n",
    "            loss_list = []\n",
    "            for batch in train_loader:\n",
    "                x, y = batch['input_ids'].to(DEVICE), batch['labels'].to(DEVICE)\n",
    "                \n",
    "                # --- HF-STYLE FORWARD ---\n",
    "                # HF models can calculate loss internally if labels are passed\n",
    "                outputs = model(input_ids=x, labels=y)\n",
    "                \n",
    "                if hasattr(outputs, 'loss') and outputs.loss is not None:\n",
    "                    loss = outputs.loss\n",
    "                else:\n",
    "                    # Fallback for models that don't compute loss internally\n",
    "                    logits = outputs.logits if hasattr(outputs, 'logits') else outputs\n",
    "                    shift_logits = logits[:, :-1, :].contiguous()\n",
    "                    shift_labels = y[:, 1:].contiguous()\n",
    "                    loss = F.cross_entropy(shift_logits.view(-1, VOCAB_SIZE), shift_labels.view(-1))\n",
    "                \n",
    "                opt.zero_grad()\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                loss_list.append(loss.item())\n",
    "                \n",
    "            if (ep+1) % 5 == 0:\n",
    "                print(f\"   Ep {ep+1}: Loss {np.mean(loss_list):.4f}\")\n",
    "\n",
    "        # --- 2. Benchmark Evaluation ---\n",
    "        print(f\"   ðŸ§ª Benchmarking {name}...\")\n",
    "        model.eval()\n",
    "        for L in test_lengths:\n",
    "            gc.collect(); torch.cuda.empty_cache()\n",
    "            eval_batch = 64 if L <= 1024 else (8 if L <= 8192 else 1)\n",
    "            \n",
    "            test_ds = NeedleHaystackDataset(size=100, min_len=L, max_len=L, vocab_size=VOCAB_SIZE, depth=0.5)\n",
    "            test_loader = DataLoader(test_ds, batch_size=eval_batch, collate_fn=pad_collate_fn)\n",
    "            \n",
    "            hits, total = 0, 0\n",
    "            try:\n",
    "                with torch.no_grad():\n",
    "                    for batch in test_loader:\n",
    "                        x, y = batch['input_ids'].to(DEVICE), batch['labels'].to(DEVICE)\n",
    "                        outputs = model(x)\n",
    "                        logits = outputs.logits if hasattr(outputs, 'logits') else outputs\n",
    "                        \n",
    "                        for i in range(x.size(0)):\n",
    "                            valid = (y[i] != -100).nonzero(as_tuple=True)[0]\n",
    "                            if len(valid) == 0: continue\n",
    "                            \n",
    "                            target_pos = valid[-1].item()\n",
    "                            pred = logits[i, target_pos-1].argmax().item()\n",
    "                            if pred == y[i, target_pos].item(): hits += 1\n",
    "                            total += 1\n",
    "                            \n",
    "                acc = hits / total if total > 0 else 0\n",
    "                results[name].append(acc)\n",
    "                print(f\"      Len {L}: {acc:.1%}\")\n",
    "            except RuntimeError as e:\n",
    "                if \"out of memory\" in str(e).lower():\n",
    "                    print(f\"      Len {L}: FAILED (OOM)\")\n",
    "                else:\n",
    "                    print(f\"      Len {L}: FAILED ({e})\")\n",
    "                results[name].append(0.0)\n",
    "\n",
    "    # --- 3. Visualization ---\n",
    "    # (Same as your provided code, just use results dictionary)\n",
    "    # ...\n",
    "run_benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2522e5a5-089b-4c31-b05b-c67466731b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_benchmark():\n",
    "    VOCAB_SIZE = 1000\n",
    "    DIM = 64\n",
    "    LAYERS = 2\n",
    "    HEADS = 4\n",
    "    EPOCHS = 10\n",
    "    BATCH_SIZE = 64\n",
    "    \n",
    "    # --- Model Setup ---\n",
    "    # 1. Custom Mamba (Reference Implementation)\n",
    "    mamba_custom = MambaModel(VOCAB_SIZE, DIM, LAYERS).to(DEVICE)\n",
    "    \n",
    "    # 2. Hugging Face Mamba\n",
    "    # Note: Ensure MambaConfig/MambaForCausalLM are imported from transformers or mamba_ssm\n",
    "    config_kwargs = {\n",
    "        \"vocab_size\": VOCAB_SIZE, \n",
    "        \"ssm_cfg\": {\"dropout\": 0.0 }\n",
    "    }\n",
    "    mamba_config = MambaConfig(\n",
    "        hidden_size=DIM,\n",
    "        num_hidden_layers=LAYERS, \n",
    "        **config_kwargs\n",
    "    )\n",
    "    mamba_hf = MambaForCausalLM(mamba_config).to(DEVICE)\n",
    "\n",
    "    # 3. GPT-2\n",
    "    # Ensure model_gpt2 is defined or re-instantiate here\n",
    "    # (Using the config provided in your previous context)\n",
    "    gpt2_config = GPT2Config(\n",
    "        vocab_size=VOCAB_SIZE, n_positions=8192, n_embd=DIM, n_layer=LAYERS, n_head=HEADS,\n",
    "        bos_token_id=0, eos_token_id=0, resid_pdrop=0.0, embd_pdrop=0.0, attn_pdrop=0.0, use_cache=False\n",
    "    )\n",
    "    gpt2_model = GPT2LMHeadModel(gpt2_config).to(DEVICE)\n",
    "    \n",
    "    # 4. Long-LLM\n",
    "    long_llm = LongNet(VOCAB_SIZE, DIM, LAYERS, HEADS).to(DEVICE)\n",
    "\n",
    "    models = {\n",
    "        \"Long-LLM (Fixed)\": long_llm,\n",
    "        # \"Mamba (Our Implementation)\": mamba_custom,\n",
    "        \"Mamba (Hugging Face)\": mamba_hf,\n",
    "        \"GPT-2\": gpt2_model\n",
    "    }\n",
    "    \n",
    "    # Scale up to 32k tokens\n",
    "    test_lengths = [64, 128, 256, 512, 1024, 2048, 4096, 8192]\n",
    "    results = {name: [] for name in models}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nðŸš€ Training {name}...\")\n",
    "        \n",
    "        lr = 0.005 if \"Mamba\" in name else 0.003\n",
    "        opt = optim.AdamW(model.parameters(), lr=lr)\n",
    "        \n",
    "        # Train on Short Context (Max 64)\n",
    "        train_ds = NeedleHaystackDataset(size=10000, min_len=32, max_len=128, vocab_size=VOCAB_SIZE, depth=None)\n",
    "        train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, collate_fn=pad_collate_fn, shuffle=True)\n",
    "        \n",
    "        model.train()\n",
    "        for ep in range(EPOCHS):\n",
    "            loss_list = []\n",
    "            for batch in train_loader:\n",
    "                x, y = batch['input_ids'].to(DEVICE), batch['labels'].to(DEVICE)\n",
    "                \n",
    "                # --- FIX: Handle Output Polymorphism ---\n",
    "                outputs = model(x)\n",
    "                if hasattr(outputs, 'logits'):\n",
    "                    logits = outputs.logits\n",
    "                else:\n",
    "                    logits = outputs\n",
    "                # ---------------------------------------\n",
    "\n",
    "                # Shift logits and labels\n",
    "                shift_logits = logits[:, :-1, :].contiguous()\n",
    "                shift_labels = y[:, 1:].contiguous()\n",
    "                \n",
    "                loss = F.cross_entropy(\n",
    "                    shift_logits.view(-1, VOCAB_SIZE), \n",
    "                    shift_labels.view(-1)\n",
    "                )\n",
    "                \n",
    "                opt.zero_grad()\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                loss_list.append(loss.item())\n",
    "                \n",
    "            if (ep+1) % 5 == 0:\n",
    "                print(f\"   Ep {ep+1}: Loss {np.mean(loss_list):.4f}\")\n",
    "\n",
    "        print(f\"   ðŸ§ª Benchmarking {name}...\")\n",
    "        model.eval()\n",
    "        for L in test_lengths:\n",
    "            gc.collect(); torch.cuda.empty_cache()\n",
    "            \n",
    "            if L <= 1024: eval_batch = 64\n",
    "            elif L <= 8192: eval_batch = 8\n",
    "            else: eval_batch = 1\n",
    "            \n",
    "            # Use fixed depth (0.5) for fair comparison\n",
    "            test_ds = NeedleHaystackDataset(size=100, min_len=L, max_len=L, vocab_size=VOCAB_SIZE, depth=0.5)\n",
    "            test_loader = DataLoader(test_ds, batch_size=eval_batch, collate_fn=pad_collate_fn)\n",
    "            \n",
    "            hits = 0; total = 0\n",
    "            try:\n",
    "                with torch.no_grad():\n",
    "                    for batch in test_loader:\n",
    "                        x, y = batch['input_ids'].to(DEVICE), batch['labels'].to(DEVICE)\n",
    "                        \n",
    "                        try:\n",
    "                            # --- FIX: Handle Inference Polymorphism ---\n",
    "                            outputs = model(x)\n",
    "                            if hasattr(outputs, 'logits'):\n",
    "                                logits = outputs.logits\n",
    "                            else:\n",
    "                                logits = outputs\n",
    "                            # ------------------------------------------\n",
    "                        except RuntimeError as e:\n",
    "                            if \"out of memory\" in str(e): raise e\n",
    "                            else: raise e\n",
    "                            \n",
    "                        for i in range(x.size(0)):\n",
    "                            valid = (y[i] != -100).nonzero(as_tuple=True)[0]\n",
    "                            if len(valid) == 0: continue\n",
    "                            \n",
    "                            target_pos = valid[-1].item()\n",
    "                            pred = logits[i, target_pos-1].argmax().item()\n",
    "                            \n",
    "                            if pred == y[i, target_pos].item(): hits += 1\n",
    "                            total += 1\n",
    "                            \n",
    "                acc = hits / total if total > 0 else 0\n",
    "                results[name].append(acc)\n",
    "                print(f\"      Len {L}: {acc:.1%}\")\n",
    "            except RuntimeError:\n",
    "                print(f\"      Len {L}: FAILED (OOM)\")\n",
    "                results[name].append(0.0)\n",
    "\n",
    "    # VISUALIZATION\n",
    "    print(\"\\nðŸ“Š Generating Figure...\")\n",
    "    plt.figure(figsize=(10, 6), dpi=150)\n",
    "    \n",
    "    # --- UPDATED MARKERS & COLORS ---\n",
    "    markers = {\n",
    "        'Long-LLM (Fixed)': 'o', \n",
    "        # 'Mamba (Our Implementation)': 'D', # Diamond|\n",
    "        'Mamba (Hugging Face)': 's',       # Square\n",
    "        'GPT-2': 'x'\n",
    "    }\n",
    "    colors = {\n",
    "        'Long-LLM (Fixed)': '#1f77b4',     # Blue\n",
    "        # 'Mamba (Our Implementation)': '#2ca02c', # |Green\n",
    "        'Mamba (Hugging Face)': '#ff7f0e', # Orange\n",
    "        'GPT-2': '#d62728'                 # Red\n",
    "    }\n",
    "    \n",
    "    for name, accs in results.items():\n",
    "        plt.plot(test_lengths, accs, marker=markers[name], color=colors[name], linewidth=2.5, label=name)\n",
    "        \n",
    "    plt.xscale('log', base=2)\n",
    "    plt.xticks(test_lengths, [str(l) if l<1024 else f\"{l//1024}k\" for l in test_lengths])\n",
    "    plt.xlabel(\"Context Length (Tokens)\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"FINAL: Multi-Scale Long-LLM vs Mamba (Custom & HF) vs GPT-2\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, which=\"both\", alpha=0.3)\n",
    "    plt.savefig(\"figure5_final_fix.png\")\n",
    "    print(\"âœ… Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb23c736-5c0a-41ba-8f08-c1c44c9fb519",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5422ae-7bb4-4aa4-887f-3ebf8a48d9f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (MyProject)",
   "language": "python",
   "name": "myproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
