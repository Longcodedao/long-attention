{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bdfb447-cead-4eb9-9b32-fa475fbf61c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import torch._dynamo\n",
    "torch._dynamo.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d056bf2e-2a79-4b98-8552-c19d93f5c823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking on cuda...\n",
      "[Model Loader] Identified Vocab Size: 50257\n",
      "[Model Loader] Initializing Long LLM (small)...\n",
      "\n",
      "--- üèéÔ∏è Starting Race (Generation Speed) ---\n",
      "Seq Len    | Yours (ms)      | GPT-2 (ms)      | Speedup   \n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/long-attention/.venv/lib/python3.12/site-packages/torch/_inductor/compile_fx.py:236: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n",
      "W0128 10:31:48.029000 49838 torch/_inductor/utils.py:1250] [0/0] Not enough SMs to use max_autotune_gemm mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100        | 29865.10          | 112.45          | 0.00x\n",
      "500        | 57.76          | 21.87          | 0.38x\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from model.long import LongConfig, LongForCausalLM\n",
    "from transformers import GPT2Config, GPT2LMHeadModel\n",
    "from transformers import AutoTokenizer\n",
    "from model import model_loader\n",
    "\n",
    "def benchmark():\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Benchmarking on {device}...\")\n",
    "\n",
    "    # # 1. Setup Your Model (Linear Attention)\n",
    "    # my_config = LongConfig(\n",
    "    #     vocab_size=50304,\n",
    "    #     hidden_size=768,\n",
    "    #     num_hidden_layers=18, # Increased depth\n",
    "    #     num_heads=12,\n",
    "    #     expansion_ratio=8/3,   # Ensures intermediate_size = 2048\n",
    "    #     conv_kernel=4,\n",
    "    #     hybrid_ratio=0         # Pure Linear Attention (fastest)\n",
    "    # )\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "    # if tokenizer.pad_token is None:\n",
    "    #     tokenizer.pad_token = tokenizer.eos_token\n",
    "    #     tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    \n",
    "    # my_model = LongForCausalLM(my_config).to(device)\n",
    "    # my_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    # # Compilation helps specifically with the scan operations\n",
    "    # my_model = torch.compile(my_model, mode = \"default\")\n",
    "    # my_model.eval()\n",
    "    my_model, tokenizer = model_loader.get_model_and_tokenizer(\n",
    "        model_type = \"long\",\n",
    "        model_size = \"small\",\n",
    "        device = \"cuda\"\n",
    "    )\n",
    "\n",
    "    # 2. Setup Standard Transformer (GPT-2)\n",
    "    gpt_config = GPT2Config(\n",
    "        vocab_size=50257,\n",
    "        n_embd=256,\n",
    "        n_layer=4,\n",
    "        n_head=8,\n",
    "        n_positions=2048\n",
    "    )\n",
    "    gpt_model = GPT2LMHeadModel(gpt_config).to(device)\n",
    "    gpt_model.eval()\n",
    "\n",
    "    # 3. The Race\n",
    "    lengths = [100, 500, 1000, 2000]\n",
    "    my_times = []\n",
    "    gpt_times = []\n",
    "\n",
    "    print(\"\\n--- üèéÔ∏è Starting Race (Generation Speed) ---\")\n",
    "    print(f\"{'Seq Len':<10} | {'Yours (ms)':<15} | {'GPT-2 (ms)':<15} | {'Speedup':<10}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for seq_len in lengths:\n",
    "        # Create a prompt of size 'seq_len'\n",
    "        input_ids = torch.randint(0, 50000, (1, seq_len)).to(device)\n",
    "\n",
    "        # --- NEW: Create Attention Mask ---\n",
    "        # Since we have full valid data (no padding), the mask is all 1s.\n",
    "        attention_mask = torch.ones_like(input_ids).to(device)\n",
    "        \n",
    "        # --- Benchmark Yours (Recurrent Step) ---\n",
    "        # Warmup\n",
    "        for _ in range(10):\n",
    "            with torch.no_grad():\n",
    "                _ = my_model(input_ids, attention_mask = attention_mask)\n",
    "            \n",
    "        torch.cuda.synchronize()\n",
    "        start = time.time()\n",
    "        \n",
    "        # Generate 10 tokens\n",
    "        # For your model, we simulate the Recurrent State update 10 times\n",
    "        # (This is how you would use it in production)\n",
    "        curr_input = input_ids[:, -1:]\n",
    "        past_kv = None\n",
    "        \n",
    "        # Pre-fill state (the \"prompt processing\")\n",
    "        with torch.no_grad():\n",
    "             out = my_model(input_ids, attention_mask = attention_mask)\n",
    "             past_kv = out.past_key_values\n",
    "            \n",
    "\n",
    "        # 2. Step-by-step Generation\n",
    "        # For the step input, the mask is just [1] (valid token)\n",
    "        step_mask = torch.ones((input_ids.shape[0], 1), dtype=torch.long, device=device)\n",
    "        \n",
    "        # Generate loop\n",
    "        for _ in range(10):\n",
    "            with torch.no_grad():\n",
    "                out = my_model(curr_input, \n",
    "                               past_key_values=past_kv, \n",
    "                               attention_mask=step_mask)\n",
    "                past_kv = out.past_key_values\n",
    "                curr_input = torch.argmax(out.logits, dim=-1)\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        my_time = (time.time() - start) * 1000 # ms\n",
    "        my_times.append(my_time)\n",
    "\n",
    "        # --- Benchmark GPT-2 (KV Cache) ---\n",
    "        # GPT-2 uses KV caching, but attention matrix still grows\n",
    "        with torch.no_grad():\n",
    "             _ = gpt_model(input_ids, attention_mask=attention_mask)\n",
    "             \n",
    "        torch.cuda.synchronize()\n",
    "        start = time.time()\n",
    "        \n",
    "        # We use generate() which handles KV cache efficiently\n",
    "        with torch.no_grad():\n",
    "            gpt_model.generate(\n",
    "                input_ids, \n",
    "                attention_mask=attention_mask, # Pass mask!\n",
    "                max_new_tokens=10, \n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.pad_token_id # Explicitly pass pad token\n",
    "            )\n",
    "            \n",
    "        torch.cuda.synchronize()\n",
    "        gpt_time = (time.time() - start) * 1000\n",
    "        gpt_times.append(gpt_time)\n",
    "\n",
    "        print(f\"{seq_len:<10} | {my_time:.2f}          | {gpt_time:.2f}          | {gpt_time/my_time:.2f}x\")\n",
    "\n",
    "    print(\"\\n‚úÖ Benchmark Complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa78af84-304e-4e72-be3d-1e91e9a2e1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from model.long import LongConfig, LongForCausalLM  # Ensure this matches your file name\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"Counts valid trainable parameters.\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def test_model_size():\n",
    "    print(\"--- üß™ Testing Model Configuration (Target: ~187M) ---\")\n",
    "\n",
    "    # 1. Instantiate Config\n",
    "    config = LongConfig(\n",
    "        vocab_size=50304,\n",
    "        hidden_size=768,\n",
    "        num_hidden_layers=18, # Increased depth\n",
    "        num_heads=12,\n",
    "        expansion_ratio=8/3,   # Ensures intermediate_size = 2048\n",
    "        conv_kernel=4,\n",
    "        hybrid_ratio=4         # Pure Linear Attention (fastest)\n",
    "    )\n",
    "\n",
    "    print(f\"Config: L={config.num_hidden_layers}, H={config.hidden_size}, Vocab={config.vocab_size}\")\n",
    "\n",
    "    # 2. Instantiate Model\n",
    "    print(\"Instantiating model... \", end=\"\")\n",
    "    try:\n",
    "        model = LongForCausalLM(config)\n",
    "        print(\"‚úÖ Success!\")\n",
    "    except ImportError:\n",
    "        print(\"\\n‚ùå Error: Could not import LongForCausalLM. Make sure modeling_long.py is accessible.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error initializing model: {e}\")\n",
    "        return\n",
    "\n",
    "    # 3. Calculate Parameters\n",
    "    total_params = count_parameters(model)\n",
    "    \n",
    "    # Calculate Embedding vs Non-Embedding params\n",
    "    # (Useful to know how much compute goes to \"thinking\" vs \"looking up words\")\n",
    "    embed_params = config.vocab_size * config.hidden_size\n",
    "    non_embed_params = total_params - embed_params\n",
    "\n",
    "    print(\"\\n--- üìä Parameter Breakdown ---\")\n",
    "    print(f\"Total Parameters:      {total_params:,} ({total_params/1e6:.2f}M)\")\n",
    "    print(f\"Embedding Parameters:  {embed_params:,} ({embed_params/1e6:.2f}M)\")\n",
    "    print(f\"Layer Parameters:      {non_embed_params:,} ({non_embed_params/1e6:.2f}M)\")\n",
    "    \n",
    "    # 4. Verification\n",
    "    target = 187_000_000\n",
    "    diff = abs(total_params - target)\n",
    "    print(f\"\\nDifference from 187M:  {diff:,} params\")\n",
    "    \n",
    "    if 180_000_000 <= total_params <= 195_000_000:\n",
    "        print(\"‚úÖ Result: PERFECT MATCH for the 187M category.\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Result: Slightly off target (adjust layers/hidden_size if needed).\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_model_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbc6ca1-9186-46d2-ad19-f06757060a80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (MyProject)",
   "language": "python",
   "name": "myproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
