{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bdfb447-cead-4eb9-9b32-fa475fbf61c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d056bf2e-2a79-4b98-8552-c19d93f5c823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking on cuda...\n",
      "\n",
      "--- üèéÔ∏è Starting Race (Generation Speed) ---\n",
      "Seq Len    | Yours (ms)      | GPT-2 (ms)      | Speedup   \n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100        | 280.00          | 158.13          | 0.56x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500        | 200.41          | 82.44          | 0.41x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000       | 253.22          | 84.63          | 0.33x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000       | 314.23          | 94.74          | 0.30x\n",
      "\n",
      "‚úÖ Benchmark Complete.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from model.long import LongConfig, LongForCausalLM\n",
    "from transformers import GPT2Config, GPT2LMHeadModel\n",
    "\n",
    "def benchmark():\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Benchmarking on {device}...\")\n",
    "\n",
    "    # 1. Setup Your Model (Linear Attention)\n",
    "    my_config = LongConfig(\n",
    "        vocab_size=50304,\n",
    "        hidden_size=768,\n",
    "        num_hidden_layers=18, # Increased depth\n",
    "        num_heads=12,\n",
    "        expansion_ratio=8/3,   # Ensures intermediate_size = 2048\n",
    "        conv_kernel=4,\n",
    "        hybrid_ratio=0         # Pure Linear Attention (fastest)\n",
    "    )\n",
    "    my_model = LongForCausalLM(my_config).to(device)\n",
    "    my_model.eval()\n",
    "\n",
    "    # 2. Setup Standard Transformer (GPT-2)\n",
    "    gpt_config = GPT2Config(\n",
    "        vocab_size=50257,\n",
    "        n_embd=256,\n",
    "        n_layer=4,\n",
    "        n_head=8,\n",
    "        n_positions=2048\n",
    "    )\n",
    "    gpt_model = GPT2LMHeadModel(gpt_config).to(device)\n",
    "    gpt_model.eval()\n",
    "\n",
    "    # 3. The Race\n",
    "    lengths = [100, 500, 1000, 2000]\n",
    "    my_times = []\n",
    "    gpt_times = []\n",
    "\n",
    "    print(\"\\n--- üèéÔ∏è Starting Race (Generation Speed) ---\")\n",
    "    print(f\"{'Seq Len':<10} | {'Yours (ms)':<15} | {'GPT-2 (ms)':<15} | {'Speedup':<10}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for seq_len in lengths:\n",
    "        # Create a prompt of size 'seq_len'\n",
    "        input_ids = torch.randint(0, 50000, (1, seq_len)).to(device)\n",
    "        \n",
    "        # --- Benchmark Yours (Recurrent Step) ---\n",
    "        # Warmup\n",
    "        with torch.no_grad():\n",
    "            _ = my_model(input_ids)\n",
    "            \n",
    "        torch.cuda.synchronize()\n",
    "        start = time.time()\n",
    "        \n",
    "        # Generate 10 tokens\n",
    "        # For your model, we simulate the Recurrent State update 10 times\n",
    "        # (This is how you would use it in production)\n",
    "        curr_input = input_ids[:, -1:]\n",
    "        past_kv = None\n",
    "        \n",
    "        # Pre-fill state (the \"prompt processing\")\n",
    "        with torch.no_grad():\n",
    "             out = my_model(input_ids)\n",
    "             past_kv = out.past_key_values\n",
    "        \n",
    "        # Generate loop\n",
    "        for _ in range(10):\n",
    "            with torch.no_grad():\n",
    "                out = my_model(curr_input, past_key_values=past_kv)\n",
    "                past_kv = out.past_key_values\n",
    "                curr_input = torch.argmax(out.logits, dim=-1)\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        my_time = (time.time() - start) * 1000 # ms\n",
    "        my_times.append(my_time)\n",
    "\n",
    "        # --- Benchmark GPT-2 (KV Cache) ---\n",
    "        # GPT-2 uses KV caching, but attention matrix still grows\n",
    "        with torch.no_grad():\n",
    "             _ = gpt_model(input_ids)\n",
    "             \n",
    "        torch.cuda.synchronize()\n",
    "        start = time.time()\n",
    "        \n",
    "        # We use generate() which handles KV cache efficiently\n",
    "        with torch.no_grad():\n",
    "            gpt_model.generate(input_ids, max_new_tokens=10, do_sample=False)\n",
    "            \n",
    "        torch.cuda.synchronize()\n",
    "        gpt_time = (time.time() - start) * 1000\n",
    "        gpt_times.append(gpt_time)\n",
    "\n",
    "        print(f\"{seq_len:<10} | {my_time:.2f}          | {gpt_time:.2f}          | {gpt_time/my_time:.2f}x\")\n",
    "\n",
    "    print(\"\\n‚úÖ Benchmark Complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa78af84-304e-4e72-be3d-1e91e9a2e1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üß™ Testing Model Configuration (Target: ~187M) ---\n",
      "Config: L=18, H=768, Vocab=50304\n",
      "Instantiating model... ‚úÖ Success!\n",
      "\n",
      "--- üìä Parameter Breakdown ---\n",
      "Total Parameters:      187,671,768 (187.67M)\n",
      "Embedding Parameters:  38,633,472 (38.63M)\n",
      "Layer Parameters:      149,038,296 (149.04M)\n",
      "\n",
      "Difference from 187M:  671,768 params\n",
      "‚úÖ Result: PERFECT MATCH for the 187M category.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from model.long import LongConfig, LongForCausalLM  # Ensure this matches your file name\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"Counts valid trainable parameters.\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def test_model_size():\n",
    "    print(\"--- üß™ Testing Model Configuration (Target: ~187M) ---\")\n",
    "\n",
    "    # 1. Instantiate Config\n",
    "    config = LongConfig(\n",
    "        vocab_size=50304,\n",
    "        hidden_size=768,\n",
    "        num_hidden_layers=18, # Increased depth\n",
    "        num_heads=12,\n",
    "        expansion_ratio=8/3,   # Ensures intermediate_size = 2048\n",
    "        conv_kernel=4,\n",
    "        hybrid_ratio=0         # Pure Linear Attention (fastest)\n",
    "    )\n",
    "\n",
    "    print(f\"Config: L={config.num_hidden_layers}, H={config.hidden_size}, Vocab={config.vocab_size}\")\n",
    "\n",
    "    # 2. Instantiate Model\n",
    "    print(\"Instantiating model... \", end=\"\")\n",
    "    try:\n",
    "        model = LongForCausalLM(config)\n",
    "        print(\"‚úÖ Success!\")\n",
    "    except ImportError:\n",
    "        print(\"\\n‚ùå Error: Could not import LongForCausalLM. Make sure modeling_long.py is accessible.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error initializing model: {e}\")\n",
    "        return\n",
    "\n",
    "    # 3. Calculate Parameters\n",
    "    total_params = count_parameters(model)\n",
    "    \n",
    "    # Calculate Embedding vs Non-Embedding params\n",
    "    # (Useful to know how much compute goes to \"thinking\" vs \"looking up words\")\n",
    "    embed_params = config.vocab_size * config.hidden_size\n",
    "    non_embed_params = total_params - embed_params\n",
    "\n",
    "    print(\"\\n--- üìä Parameter Breakdown ---\")\n",
    "    print(f\"Total Parameters:      {total_params:,} ({total_params/1e6:.2f}M)\")\n",
    "    print(f\"Embedding Parameters:  {embed_params:,} ({embed_params/1e6:.2f}M)\")\n",
    "    print(f\"Layer Parameters:      {non_embed_params:,} ({non_embed_params/1e6:.2f}M)\")\n",
    "    \n",
    "    # 4. Verification\n",
    "    target = 187_000_000\n",
    "    diff = abs(total_params - target)\n",
    "    print(f\"\\nDifference from 187M:  {diff:,} params\")\n",
    "    \n",
    "    if 180_000_000 <= total_params <= 195_000_000:\n",
    "        print(\"‚úÖ Result: PERFECT MATCH for the 187M category.\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Result: Slightly off target (adjust layers/hidden_size if needed).\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_model_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbc6ca1-9186-46d2-ad19-f06757060a80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (MyProject)",
   "language": "python",
   "name": "myproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
