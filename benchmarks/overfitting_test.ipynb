{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04469649-f1d0-475a-9d6b-ff4aba64344a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60c9c91d-06a9-4824-95ce-41aa6febfa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from model.long import LongConfig, LongForCausalLM\n",
    "from model.long import chunked_parallel_scan, recurrent_scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7e86983-5461-4a24-9a32-ca70f36505f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 30\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Tiny Character Tokenizer ---\n",
    "class TinyTokenizer:\n",
    "    def __init__(self, text):\n",
    "        chars = sorted(list(set(text)))\n",
    "        self.vocab_size = len(chars) + 1 # +1 for padding\n",
    "        self.stoi = {ch: i+1 for i, ch in enumerate(chars)}\n",
    "        self.itos = {i+1: ch for i, ch in enumerate(chars)}\n",
    "        self.pad_token_id = 0\n",
    "    \n",
    "    def encode(self, text):\n",
    "        return [self.stoi[c] for c in text]\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        return \"\".join([self.itos[i] for i in ids if i > 0])\n",
    "\n",
    "# --- 2. Setup Data ---\n",
    "text = \"The quick brown fox jumps over the lazy dog. \" * 10\n",
    "tokenizer = TinyTokenizer(text)\n",
    "print(f\"Vocab Size: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d2c1df6-6a0e-4da2-8af6-44f28efe0ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Parameters: 599048\n"
     ]
    }
   ],
   "source": [
    "# Create Batch\n",
    "data = torch.tensor(tokenizer.encode(text), dtype=torch.long).unsqueeze(0) # [1, T]\n",
    "inputs = data[:, :-1]\n",
    "targets = data[:, 1:]\n",
    "\n",
    "# --- 3. Setup Model ---\n",
    "config = LongConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    hidden_size=128,       # Small size for speed\n",
    "    num_hidden_layers=2,   # 2 layers is enough to prove flow\n",
    "    num_heads=4,\n",
    "    conv_kernel=3,\n",
    "    max_position_embeddings=512\n",
    ")\n",
    "model = LongForCausalLM(config)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3) # High LR for fast overfitting\n",
    "\n",
    "print(f\"Model Parameters: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0a7f342-4054-4406-8954-975af61c5afe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3, 11,  8,  1, 20, 24, 12,  6, 14,  1,  5, 21, 18, 26, 17,  1,  9, 18,\n",
      "         27,  1, 13, 24, 16, 19, 22,  1, 18, 25,  8, 21,  1, 23, 11,  8,  1, 15,\n",
      "          4, 29, 28,  1,  7, 18, 10,  2,  1,  3, 11,  8,  1, 20, 24, 12,  6, 14,\n",
      "          1,  5, 21, 18, 26, 17,  1,  9, 18, 27,  1, 13, 24, 16, 19, 22,  1, 18,\n",
      "         25,  8, 21,  1, 23, 11,  8,  1, 15,  4, 29, 28,  1,  7, 18, 10,  2,  1,\n",
      "          3, 11,  8,  1, 20, 24, 12,  6, 14,  1,  5, 21, 18, 26, 17,  1,  9, 18,\n",
      "         27,  1, 13, 24, 16, 19, 22,  1, 18, 25,  8, 21,  1, 23, 11,  8,  1, 15,\n",
      "          4, 29, 28,  1,  7, 18, 10,  2,  1,  3, 11,  8,  1, 20, 24, 12,  6, 14,\n",
      "          1,  5, 21, 18, 26, 17,  1,  9, 18, 27,  1, 13, 24, 16, 19, 22,  1, 18,\n",
      "         25,  8, 21,  1, 23, 11,  8,  1, 15,  4, 29, 28,  1,  7, 18, 10,  2,  1,\n",
      "          3, 11,  8,  1, 20, 24, 12,  6, 14,  1,  5, 21, 18, 26, 17,  1,  9, 18,\n",
      "         27,  1, 13, 24, 16, 19, 22,  1, 18, 25,  8, 21,  1, 23, 11,  8,  1, 15,\n",
      "          4, 29, 28,  1,  7, 18, 10,  2,  1,  3, 11,  8,  1, 20, 24, 12,  6, 14,\n",
      "          1,  5, 21, 18, 26, 17,  1,  9, 18, 27,  1, 13, 24, 16, 19, 22,  1, 18,\n",
      "         25,  8, 21,  1, 23, 11,  8,  1, 15,  4, 29, 28,  1,  7, 18, 10,  2,  1,\n",
      "          3, 11,  8,  1, 20, 24, 12,  6, 14,  1,  5, 21, 18, 26, 17,  1,  9, 18,\n",
      "         27,  1, 13, 24, 16, 19, 22,  1, 18, 25,  8, 21,  1, 23, 11,  8,  1, 15,\n",
      "          4, 29, 28,  1,  7, 18, 10,  2,  1,  3, 11,  8,  1, 20, 24, 12,  6, 14,\n",
      "          1,  5, 21, 18, 26, 17,  1,  9, 18, 27,  1, 13, 24, 16, 19, 22,  1, 18,\n",
      "         25,  8, 21,  1, 23, 11,  8,  1, 15,  4, 29, 28,  1,  7, 18, 10,  2,  1,\n",
      "          3, 11,  8,  1, 20, 24, 12,  6, 14,  1,  5, 21, 18, 26, 17,  1,  9, 18,\n",
      "         27,  1, 13, 24, 16, 19, 22,  1, 18, 25,  8, 21,  1, 23, 11,  8,  1, 15,\n",
      "          4, 29, 28,  1,  7, 18, 10,  2,  1,  3, 11,  8,  1, 20, 24, 12,  6, 14,\n",
      "          1,  5, 21, 18, 26, 17,  1,  9, 18, 27,  1, 13, 24, 16, 19, 22,  1, 18,\n",
      "         25,  8, 21,  1, 23, 11,  8,  1, 15,  4, 29, 28,  1,  7, 18, 10,  2]])\n",
      "tensor([[11,  8,  1, 20, 24, 12,  6, 14,  1,  5, 21, 18, 26, 17,  1,  9, 18, 27,\n",
      "          1, 13, 24, 16, 19, 22,  1, 18, 25,  8, 21,  1, 23, 11,  8,  1, 15,  4,\n",
      "         29, 28,  1,  7, 18, 10,  2,  1,  3, 11,  8,  1, 20, 24, 12,  6, 14,  1,\n",
      "          5, 21, 18, 26, 17,  1,  9, 18, 27,  1, 13, 24, 16, 19, 22,  1, 18, 25,\n",
      "          8, 21,  1, 23, 11,  8,  1, 15,  4, 29, 28,  1,  7, 18, 10,  2,  1,  3,\n",
      "         11,  8,  1, 20, 24, 12,  6, 14,  1,  5, 21, 18, 26, 17,  1,  9, 18, 27,\n",
      "          1, 13, 24, 16, 19, 22,  1, 18, 25,  8, 21,  1, 23, 11,  8,  1, 15,  4,\n",
      "         29, 28,  1,  7, 18, 10,  2,  1,  3, 11,  8,  1, 20, 24, 12,  6, 14,  1,\n",
      "          5, 21, 18, 26, 17,  1,  9, 18, 27,  1, 13, 24, 16, 19, 22,  1, 18, 25,\n",
      "          8, 21,  1, 23, 11,  8,  1, 15,  4, 29, 28,  1,  7, 18, 10,  2,  1,  3,\n",
      "         11,  8,  1, 20, 24, 12,  6, 14,  1,  5, 21, 18, 26, 17,  1,  9, 18, 27,\n",
      "          1, 13, 24, 16, 19, 22,  1, 18, 25,  8, 21,  1, 23, 11,  8,  1, 15,  4,\n",
      "         29, 28,  1,  7, 18, 10,  2,  1,  3, 11,  8,  1, 20, 24, 12,  6, 14,  1,\n",
      "          5, 21, 18, 26, 17,  1,  9, 18, 27,  1, 13, 24, 16, 19, 22,  1, 18, 25,\n",
      "          8, 21,  1, 23, 11,  8,  1, 15,  4, 29, 28,  1,  7, 18, 10,  2,  1,  3,\n",
      "         11,  8,  1, 20, 24, 12,  6, 14,  1,  5, 21, 18, 26, 17,  1,  9, 18, 27,\n",
      "          1, 13, 24, 16, 19, 22,  1, 18, 25,  8, 21,  1, 23, 11,  8,  1, 15,  4,\n",
      "         29, 28,  1,  7, 18, 10,  2,  1,  3, 11,  8,  1, 20, 24, 12,  6, 14,  1,\n",
      "          5, 21, 18, 26, 17,  1,  9, 18, 27,  1, 13, 24, 16, 19, 22,  1, 18, 25,\n",
      "          8, 21,  1, 23, 11,  8,  1, 15,  4, 29, 28,  1,  7, 18, 10,  2,  1,  3,\n",
      "         11,  8,  1, 20, 24, 12,  6, 14,  1,  5, 21, 18, 26, 17,  1,  9, 18, 27,\n",
      "          1, 13, 24, 16, 19, 22,  1, 18, 25,  8, 21,  1, 23, 11,  8,  1, 15,  4,\n",
      "         29, 28,  1,  7, 18, 10,  2,  1,  3, 11,  8,  1, 20, 24, 12,  6, 14,  1,\n",
      "          5, 21, 18, 26, 17,  1,  9, 18, 27,  1, 13, 24, 16, 19, 22,  1, 18, 25,\n",
      "          8, 21,  1, 23, 11,  8,  1, 15,  4, 29, 28,  1,  7, 18, 10,  2,  1]])\n"
     ]
    }
   ],
   "source": [
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26485c59-9b96-4fca-9dd8-b85c35a780d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Overfit Run...\n",
      "Step 000 | Loss: 103.222633\n",
      "Step 025 | Loss: 0.000634\n",
      "Step 050 | Loss: 0.000004\n",
      "Step 075 | Loss: 0.000001\n",
      "Step 100 | Loss: 0.000001\n",
      "Step 125 | Loss: 0.000000\n",
      "Step 150 | Loss: 0.000000\n",
      "Step 175 | Loss: 0.000000\n",
      "Step 200 | Loss: 0.000000\n",
      "Step 225 | Loss: 0.000000\n",
      "\n",
      "Final Loss: 2.075517073762967e-07\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Training Loop ---\n",
    "print(\"\\nStarting Overfit Run...\")\n",
    "model.train()\n",
    "\n",
    "for step in range(250):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    outputs = model(inputs, labels=targets)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    \n",
    "    # 3. Tighter clipping (0.5 instead of 1.0)\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    if step % 25 == 0:\n",
    "        print(f\"Step {step:03d} | Loss: {loss.item():.6f}\")\n",
    "        \n",
    "print(\"\\nFinal Loss:\", loss.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16635c23-cdb5-4a7f-b280-1dd7851db4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating from Prompt 'The quick' ---\n",
      "Output: The quickbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb\n",
      "\n",
      "‚úÖ SUCCESS: Model overfit successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Generation Test (Inference Mode) ---\n",
    "print(\"\\n--- Generating from Prompt 'The quick' ---\")\n",
    "model.eval()\n",
    "\n",
    "# Start with \"The quick\"\n",
    "prompt = \"The quick\"\n",
    "input_ids = torch.tensor([tokenizer.encode(prompt)], dtype=torch.long)\n",
    "\n",
    "# Generate 50 tokens\n",
    "# We implement a simple loop here to use your model's native .forward\n",
    "generated = input_ids.tolist()[0]\n",
    "past_key_values = None\n",
    "current_input = input_ids\n",
    "\n",
    "for _ in range(50):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(current_input, past_key_values=past_key_values)\n",
    "        \n",
    "    logits = outputs.logits[:, -1, :] # Last token logits\n",
    "    next_token = torch.argmax(logits, dim=-1).unsqueeze(0) # Greedy decode\n",
    "    \n",
    "    # Update History\n",
    "    past_key_values = outputs.past_key_values\n",
    "    current_input = next_token\n",
    "    \n",
    "    generated.append(next_token.item())\n",
    "\n",
    "print(f\"Output: {tokenizer.decode(generated)}\")\n",
    "\n",
    "if loss.item() < 0.1:\n",
    "    print(\"\\n‚úÖ SUCCESS: Model overfit successfully.\")\n",
    "else:\n",
    "    print(\"\\n‚ùå FAILURE: Model failed to converge.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb4f8694-5f44-4259-bd8f-bcd8475e8552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The quick brown fox jumps over the lazy dog. '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"The quick brown fox jumps over the lazy dog. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "846ae800-d61b-4435-a186-f0b71ddeb71a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- üïµÔ∏è‚Äç‚ôÄÔ∏è Needle in a Haystack Test ---\n",
      "Sequence Length: 500\n",
      "Needle '101 -> 999' hidden at index 50\n",
      "Trigger '101' placed at end (index 499)\n",
      "\n",
      "Training...\n",
      "Step 000: Loss 72.3350\n",
      "Step 030: Loss 0.0000\n",
      "Step 060: Loss 0.0000\n",
      "Step 090: Loss 0.0000\n",
      "Step 120: Loss 0.0000\n",
      "\n",
      "--- Testing Inference (Generation) ---\n",
      "Expected: 999\n",
      "Predicted: 999\n",
      "\n",
      "‚úÖ SUCCESS: Found the needle! Your Linear Attention is working.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from model.long import LongConfig, LongForCausalLM\n",
    "\n",
    "def test_needle_retrieval():\n",
    "    print(\"\\n--- üïµÔ∏è‚Äç‚ôÄÔ∏è Needle in a Haystack Test ---\")\n",
    "    \n",
    "    # 1. Setup Model (Long Context)\n",
    "    config = LongConfig(\n",
    "        vocab_size=1000,\n",
    "        hidden_size=64,\n",
    "        num_hidden_layers=2,\n",
    "        num_heads=4,\n",
    "        max_position_embeddings=2048 \n",
    "    )\n",
    "    model = LongForCausalLM(config)\n",
    "    \n",
    "    # 2. Construct the Haystack\n",
    "    seq_len = 500     # Length of sequence\n",
    "    needle_pos = 50   # Where we hide the secret\n",
    "    \n",
    "    key_token = 101   # The \"Question\"\n",
    "    val_token = 999   # The \"Answer\"\n",
    "    trigger = 101     # We ask this at the end\n",
    "    \n",
    "    # Create random noise\n",
    "    input_ids = torch.randint(10, 90, (1, seq_len)) \n",
    "    \n",
    "    # Hide the needle: \"When you see 101, the next token is 999\"\n",
    "    input_ids[0, needle_pos] = key_token\n",
    "    input_ids[0, needle_pos+1] = val_token\n",
    "    \n",
    "    # Place trigger at the very end\n",
    "    input_ids[0, -1] = trigger\n",
    "    \n",
    "    print(f\"Sequence Length: {seq_len}\")\n",
    "    print(f\"Needle '{key_token} -> {val_token}' hidden at index {needle_pos}\")\n",
    "    print(f\"Trigger '{trigger}' placed at end (index {seq_len-1})\")\n",
    "    \n",
    "    # 3. Train on this ONE example\n",
    "    # We want the model to learn: \"If I saw 101 earlier, output 999 now.\"\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.01)\n",
    "    model.train()\n",
    "    \n",
    "    # Target: Predict the token AFTER the trigger\n",
    "    target = torch.tensor([val_token], device=input_ids.device)\n",
    "    \n",
    "    print(\"\\nTraining...\")\n",
    "    for i in range(150):\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids)\n",
    "        \n",
    "        # We only care about the prediction at the very last step\n",
    "        last_token_logits = outputs.logits[0, -1, :] # [Vocab]\n",
    "        \n",
    "        # Calculate loss manually for just the last token\n",
    "        loss = nn.CrossEntropyLoss()(last_token_logits.unsqueeze(0), target)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 30 == 0:\n",
    "            print(f\"Step {i:03d}: Loss {loss.item():.4f}\")\n",
    "\n",
    "    # 4. Inference Test (Recurrent Mode)\n",
    "    print(\"\\n--- Testing Inference (Generation) ---\")\n",
    "    model.eval()\n",
    "    \n",
    "    # We feed the sequence up to the trigger, then generate ONE token\n",
    "    # This forces the model to use its Recurrent State to remember the needle\n",
    "    with torch.no_grad():\n",
    "        # Pass the whole sequence\n",
    "        outputs = model(input_ids)\n",
    "        \n",
    "        # Look at the prediction for the step *after* the trigger\n",
    "        logits = outputs.logits[0, -1, :]\n",
    "        predicted_id = torch.argmax(logits).item()\n",
    "    \n",
    "    print(f\"Expected: {val_token}\")\n",
    "    print(f\"Predicted: {predicted_id}\")\n",
    "    \n",
    "    if predicted_id == val_token:\n",
    "        print(\"\\n‚úÖ SUCCESS: Found the needle! Your Linear Attention is working.\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå FAILURE: Predicted {predicted_id}. Memory lost.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_needle_retrieval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c5505c-56f0-4a1a-a85d-ab9bbd16f7a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feea6a1a-2ce3-4d79-9f2e-e34e868eec0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (MyProject)",
   "language": "python",
   "name": "myproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
