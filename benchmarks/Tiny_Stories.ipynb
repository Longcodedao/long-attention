{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa24b9d8-83e8-4c80-96dd-408ae8eaa0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60924523-244a-4959-9ce0-6af3356b55ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import torch.optim as optim\n",
    "from model.long import LongConfig, LongForCausalLM\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1444c5be-f468-48d1-a260-15b067d83d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Class for Streaming ---\n",
    "class StreamDataset(IterableDataset):\n",
    "    def __init__(self, hf_dataset, tokenizer, max_len):\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __iter__(self):\n",
    "        # We iterate over the infinite/streaming HF dataset\n",
    "        for item in self.hf_dataset:\n",
    "            text = item['text']\n",
    "            # Tokenize\n",
    "            enc = self.tokenizer(\n",
    "                text, \n",
    "                truncation=True, \n",
    "                max_length=self.max_len, \n",
    "                padding=\"max_length\", \n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            # Yield just the input_ids (squeeze to remove batch dim from tokenizer)\n",
    "            yield enc['input_ids'].squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83834ad5-3702-4a68-9a29-a97b308b61d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "BATCH_SIZE = 8        # Small batch for GPU memory safety\n",
    "SEQ_LEN = 512         # Decent context length\n",
    "LEARNING_RATE = 5e-4  # Standard transformer LR\n",
    "MAX_STEPS = 1000      # Short run to prove it works (increase later)\n",
    "SAVE_EVERY = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9b3d1bf-c080-43dc-8bc2-63aa9d27c0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    print(\"--- ðŸš€ Initializing Linear Attention Training ---\")\n",
    "    \n",
    "    # 1. Load Data & Tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    print(\"Loading TinyStories dataset (Streaming)...\")\n",
    "    # streaming=True downloads data on the fly, no huge HDD space needed\n",
    "    hf_dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train\", streaming=True)\n",
    "    \n",
    "    # Wrap in our PyTorch IterableDataset\n",
    "    train_dataset = StreamDataset(hf_dataset, tokenizer, SEQ_LEN)\n",
    "\n",
    "    # 2. Model Setup\n",
    "    config = LongConfig(\n",
    "        vocab_size=tokenizer.vocab_size,\n",
    "        hidden_size=256,      \n",
    "        num_hidden_layers=4,  \n",
    "        num_heads=8,\n",
    "        max_position_embeddings=SEQ_LEN\n",
    "    )\n",
    "    model = LongForCausalLM(config).cuda()\n",
    "    \n",
    "    print(f\"Model Parameters: {sum(p.numel() for p in model.parameters())/1e6:.2f}M\")\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    # 3. Training Loop\n",
    "    model.train()\n",
    "    # PyTorch automatically handles IterableDataset correctly (no sampler needed)\n",
    "    loader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    step = 0\n",
    "    print(\"\\nStarting Training...\")\n",
    "    \n",
    "    for batch in loader:\n",
    "        if step >= MAX_STEPS: break\n",
    "        \n",
    "        # Move to GPU\n",
    "        batch = batch.cuda()\n",
    "        \n",
    "        # Forward\n",
    "        outputs = model(batch, labels=batch)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        step += 1\n",
    "        \n",
    "        if step % 10 == 0:\n",
    "            print(f\"Step {step:04d} | Loss: {loss.item():.4f}\")\n",
    "            \n",
    "        if step % SAVE_EVERY == 0:\n",
    "            print(f\"Saving checkpoint at step {step}...\")\n",
    "            generate_sample(model, tokenizer)\n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "541109d9-85cd-4f77-9521-f2f64e171e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ðŸš€ Initializing Linear Attention Training ---\n",
      "Loading TinyStories dataset (Streaming)...\n",
      "Model Parameters: 16.56M\n",
      "\n",
      "Starting Training...\n",
      "Step 0010 | Loss: 6.9726\n",
      "Step 0020 | Loss: 5.1524\n",
      "Step 0030 | Loss: 3.3083\n",
      "Step 0040 | Loss: 2.5144\n",
      "Step 0050 | Loss: 2.2211\n",
      "Step 0060 | Loss: 2.0560\n",
      "Step 0070 | Loss: 1.6017\n",
      "Step 0080 | Loss: 1.7025\n",
      "Step 0090 | Loss: 4.4261\n",
      "Step 0100 | Loss: 3.4892\n",
      "Step 0110 | Loss: 1.4607\n",
      "Step 0120 | Loss: 3.4764\n",
      "Step 0130 | Loss: 1.3430\n",
      "Step 0140 | Loss: 1.5170\n",
      "Step 0150 | Loss: 1.4277\n",
      "Step 0160 | Loss: 1.3733\n",
      "Step 0170 | Loss: 1.2521\n",
      "Step 0180 | Loss: 1.2470\n",
      "Step 0190 | Loss: 1.5670\n",
      "Step 0200 | Loss: 1.4887\n",
      "Saving checkpoint at step 200...\n",
      "\n",
      "--- Generating Sample ---\n",
      "Result: Once upon a time, there was a little girl named Tim. He was so happy that he was so happy.\n",
      "\n",
      "The little girl was so happy that he was so happy.\n",
      "\n",
      "The little girl was so happy that he was so happy.\n",
      "\n",
      "The little\n",
      "-------------------------\n",
      "Step 0210 | Loss: 1.6974\n",
      "Step 0220 | Loss: 1.5112\n",
      "Step 0230 | Loss: 1.7173\n",
      "Step 0240 | Loss: 1.6836\n",
      "Step 0250 | Loss: 1.5902\n",
      "Step 0260 | Loss: 1.7046\n",
      "Step 0270 | Loss: 3.0361\n",
      "Step 0280 | Loss: 1.0758\n",
      "Step 0290 | Loss: 2.3116\n",
      "Step 0300 | Loss: 1.1375\n",
      "Step 0310 | Loss: 1.2953\n",
      "Step 0320 | Loss: 1.8029\n",
      "Step 0330 | Loss: 1.6364\n",
      "Step 0340 | Loss: 1.3859\n",
      "Step 0350 | Loss: 1.2917\n",
      "Step 0360 | Loss: 1.3203\n",
      "Step 0370 | Loss: 1.0779\n",
      "Step 0380 | Loss: 1.0341\n",
      "Step 0390 | Loss: 1.4926\n",
      "Step 0400 | Loss: 1.5108\n",
      "Saving checkpoint at step 400...\n",
      "\n",
      "--- Generating Sample ---\n",
      "Result: Once upon a time, there was a little girl named Lily. She was so happy that he had made a special toy. He was so happy that he had made a special toy.\n",
      "\n",
      "The little girl was so happy that he had made a special toy. He was\n",
      "-------------------------\n",
      "Step 0410 | Loss: 1.2799\n",
      "Step 0420 | Loss: 1.1695\n",
      "Step 0430 | Loss: 1.3692\n",
      "Step 0440 | Loss: 1.2990\n",
      "Step 0450 | Loss: 1.1355\n",
      "Step 0460 | Loss: 1.4930\n",
      "Step 0470 | Loss: 1.6201\n",
      "Step 0480 | Loss: 1.5050\n",
      "Step 0490 | Loss: 1.3224\n",
      "Step 0500 | Loss: 1.0545\n",
      "Step 0510 | Loss: 1.2002\n",
      "Step 0520 | Loss: 1.2098\n",
      "Step 0530 | Loss: 1.5113\n",
      "Step 0540 | Loss: 1.4054\n",
      "Step 0550 | Loss: 1.5082\n",
      "Step 0560 | Loss: 1.4839\n",
      "Step 0570 | Loss: 1.0162\n",
      "Step 0580 | Loss: 1.2717\n",
      "Step 0590 | Loss: 1.4266\n",
      "Step 0600 | Loss: 1.4018\n",
      "Saving checkpoint at step 600...\n",
      "\n",
      "--- Generating Sample ---\n",
      "Result: Once upon a time, there was a little girl named Lily. She was very excited and he was very happy. He wanted to go home and saw a big, big, big, shiny car. He was very happy and he wanted to go home.\n",
      "\n",
      "The little\n",
      "-------------------------\n",
      "Step 0610 | Loss: 1.4874\n",
      "Step 0620 | Loss: 1.4207\n",
      "Step 0630 | Loss: 1.2920\n",
      "Step 0640 | Loss: 1.6985\n",
      "Step 0650 | Loss: 1.3799\n",
      "Step 0660 | Loss: 1.0564\n",
      "Step 0670 | Loss: 1.3440\n",
      "Step 0680 | Loss: 1.2999\n",
      "Step 0690 | Loss: 1.0543\n",
      "Step 0700 | Loss: 1.2494\n",
      "Step 0710 | Loss: 2.3533\n",
      "Step 0720 | Loss: 1.1870\n",
      "Step 0730 | Loss: 1.1960\n",
      "Step 0740 | Loss: 1.3040\n",
      "Step 0750 | Loss: 0.8309\n",
      "Step 0760 | Loss: 1.0035\n",
      "Step 0770 | Loss: 0.9849\n",
      "Step 0780 | Loss: 1.0953\n",
      "Step 0790 | Loss: 1.3460\n",
      "Step 0800 | Loss: 1.2578\n",
      "Saving checkpoint at step 800...\n",
      "\n",
      "--- Generating Sample ---\n",
      "Result: Once upon a time, there was a little girl named Lily. She was very excited and wanted to help her. She asked her mom, \"What are you doing?\"\n",
      "\n",
      "The little girl smiled and said, \"I'm sorry, Mom. I'm so proud of\n",
      "-------------------------\n",
      "Step 0810 | Loss: 1.5405\n",
      "Step 0820 | Loss: 1.0686\n",
      "Step 0830 | Loss: 0.9422\n",
      "Step 0840 | Loss: 1.0151\n",
      "Step 0850 | Loss: 0.8627\n",
      "Step 0860 | Loss: 1.1142\n",
      "Step 0870 | Loss: 2.2090\n",
      "Step 0880 | Loss: 2.6295\n",
      "Step 0890 | Loss: 1.1801\n",
      "Step 0900 | Loss: 2.2883\n",
      "Step 0910 | Loss: 0.9974\n",
      "Step 0920 | Loss: 2.5095\n",
      "Step 0930 | Loss: 2.5013\n",
      "Step 0940 | Loss: 0.8942\n",
      "Step 0950 | Loss: 1.4798\n",
      "Step 0960 | Loss: 1.4613\n",
      "Step 0970 | Loss: 1.2842\n",
      "Step 0980 | Loss: 1.2087\n",
      "Step 0990 | Loss: 0.8943\n",
      "Step 1000 | Loss: 0.9289\n",
      "Saving checkpoint at step 1000...\n",
      "\n",
      "--- Generating Sample ---\n",
      "Result: Once upon a time, there was a little girl named Lily. She loved to play with her toys and her friends. One day, Lily's mommy said, \"I'm sorry, Lily. I can help you find your friend.\"\n",
      "\n",
      "Lily was very happy\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "def generate_sample(model, tokenizer):\n",
    "    print(\"\\n--- Generating Sample ---\")\n",
    "    model.eval()\n",
    "    prompt = \"Once upon a time,\"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").cuda()\n",
    "    \n",
    "    generated = input_ids.tolist()[0]\n",
    "    curr_in = input_ids\n",
    "    past_key_values = None\n",
    "    \n",
    "    for _ in range(50):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(curr_in, past_key_values=past_key_values)\n",
    "            \n",
    "            # Simple Greedy Decoding\n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "            next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(0)\n",
    "            \n",
    "            past_key_values = outputs.past_key_values\n",
    "            curr_in = next_token\n",
    "            generated.append(next_token.item())\n",
    "            \n",
    "    text = tokenizer.decode(generated, skip_special_tokens=True)\n",
    "    print(f\"Result: {text}\\n-------------------------\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12bb09e-990f-4e58-9a27-bd27b945b6f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (MyProject)",
   "language": "python",
   "name": "myproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
