{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa24b9d8-83e8-4c80-96dd-408ae8eaa0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60924523-244a-4959-9ce0-6af3356b55ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import torch.optim as optim\n",
    "from model.long import LongConfig, LongForCausalLM\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1444c5be-f468-48d1-a260-15b067d83d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Class for Streaming ---\n",
    "class StreamDataset(IterableDataset):\n",
    "    def __init__(self, hf_dataset, tokenizer, max_len):\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __iter__(self):\n",
    "        # We iterate over the infinite/streaming HF dataset\n",
    "        for item in self.hf_dataset:\n",
    "            text = item['text']\n",
    "            # Tokenize\n",
    "            enc = self.tokenizer(\n",
    "                text, \n",
    "                truncation=True, \n",
    "                max_length=self.max_len, \n",
    "                padding=\"max_length\", \n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            # Yield just the input_ids (squeeze to remove batch dim from tokenizer)\n",
    "            yield enc['input_ids'].squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83834ad5-3702-4a68-9a29-a97b308b61d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "BATCH_SIZE = 16        # Small batch for GPU memory safety\n",
    "SEQ_LEN = 512         # Decent context length\n",
    "LEARNING_RATE = 5e-4  # Standard transformer LR\n",
    "MAX_STEPS = 10000      # Short run to prove it works (increase later)\n",
    "SAVE_EVERY = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9b3d1bf-c080-43dc-8bc2-63aa9d27c0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    print(\"--- ðŸš€ Initializing Linear Attention Training ---\")\n",
    "    \n",
    "    # 1. Load Data & Tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    print(\"Loading TinyStories dataset (Streaming)...\")\n",
    "    # streaming=True downloads data on the fly, no huge HDD space needed\n",
    "    hf_dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train\", streaming=True)\n",
    "    \n",
    "    # Wrap in our PyTorch IterableDataset\n",
    "    train_dataset = StreamDataset(hf_dataset, tokenizer, SEQ_LEN)\n",
    "\n",
    "    # 2. Model Setup\n",
    "    # config = LongConfig(\n",
    "    #     vocab_size=tokenizer.vocab_size,\n",
    "    #     hidden_size=256,      \n",
    "    #     num_hidden_layers=4,  \n",
    "    #     num_heads=8,\n",
    "    #     max_position_embeddings=SEQ_LEN\n",
    "    # )\n",
    "    config = LongConfig(\n",
    "        vocab_size=tokenizer.vocab_size,\n",
    "        hidden_size=128,\n",
    "        num_hidden_layers=4, # Increased depth\n",
    "        num_heads=8,\n",
    "        expansion_ratio=8/3,   # Ensures intermediate_size = 2048\n",
    "        conv_kernel=4,\n",
    "        hybrid_ratio=0, # Pure Linear Attention (fastest)\n",
    "        max_position_embeddings=SEQ_LEN\n",
    "    )\n",
    "    model = LongForCausalLM(config).cuda()\n",
    "    \n",
    "    print(f\"Model Parameters: {sum(p.numel() for p in model.parameters())/1e6:.2f}M\")\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    # 3. Training Loop\n",
    "    model.train()\n",
    "    # PyTorch automatically handles IterableDataset correctly (no sampler needed)\n",
    "    loader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    step = 0\n",
    "    print(\"\\nStarting Training...\")\n",
    "    \n",
    "    for batch in loader:\n",
    "        if step >= MAX_STEPS: break\n",
    "        \n",
    "        # Move to GPU\n",
    "        batch = batch.cuda()\n",
    "        \n",
    "        # Forward\n",
    "        outputs = model(batch, labels=batch)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        step += 1\n",
    "        \n",
    "        if step % 10 == 0:\n",
    "            print(f\"Step {step:04d} | Loss: {loss.item():.4f}\")\n",
    "            \n",
    "        if step % SAVE_EVERY == 0:\n",
    "            print(f\"Saving checkpoint at step {step}...\")\n",
    "            generate_sample(model, tokenizer, temperature = 0.8)\n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541109d9-85cd-4f77-9521-f2f64e171e7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_sample(model, tokenizer, max_new_tokens=80, temperature=0.8, top_k=40):\n",
    "    print(f\"\\n--- Generating Sample (Temp: {temperature}, Top-K: {top_k}) ---\")\n",
    "    model.eval()\n",
    "    \n",
    "    prompt = \"Once upon a time,\"\n",
    "    # Encode prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").cuda()\n",
    "    \n",
    "    # We keep track of the full sequence for decoding\n",
    "    generated = input_ids.tolist()[0]\n",
    "    \n",
    "    # For the first step, we pass the whole prompt to fill the KV cache/RNN state\n",
    "    outputs = model(input_ids)\n",
    "    past_key_values = outputs.past_key_values\n",
    "    \n",
    "    # Get the first predicted token\n",
    "    next_token_logits = outputs.logits[:, -1, :] / temperature\n",
    "    \n",
    "    # Filter Top-K\n",
    "    indices_to_remove = next_token_logits < torch.topk(next_token_logits, top_k)[0][..., -1, None]\n",
    "    next_token_logits[indices_to_remove] = -float('Inf')\n",
    "    \n",
    "    # Sample\n",
    "    probs = F.softmax(next_token_logits, dim=-1)\n",
    "    next_token = torch.multinomial(probs, num_samples=1)\n",
    "    \n",
    "    generated.append(next_token.item())\n",
    "    curr_in = next_token # Now we only feed the new token\n",
    "    \n",
    "    # Generation Loop\n",
    "    for _ in range(max_new_tokens - 1):\n",
    "        outputs = model(curr_in, past_key_values=past_key_values)\n",
    "        \n",
    "        logits = outputs.logits[:, -1, :] / temperature\n",
    "        \n",
    "        # Apply Top-K\n",
    "        v, _ = torch.topk(logits, top_k)\n",
    "        logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "        \n",
    "        # Sample next token\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        \n",
    "        # Update for next iteration\n",
    "        past_key_values = outputs.past_key_values\n",
    "        curr_in = next_token\n",
    "        generated.append(next_token.item())\n",
    "        \n",
    "        if next_token.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "            \n",
    "    text = tokenizer.decode(generated, skip_special_tokens=True)\n",
    "    print(f\"Result: {text}\\n-------------------------\")\n",
    "    # model.train() # Switch back to training mode\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12bb09e-990f-4e58-9a27-bd27b945b6f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5e8352-6779-478c-81ae-427f173baf18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (MyProject)",
   "language": "python",
   "name": "myproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
