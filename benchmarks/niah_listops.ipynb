{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b9decb2-9850-49ac-8d75-0ccf50753889",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7800b71-a63d-4ac0-aa79-9659dd1c64ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from model.holo import HoloConfig, HoloForCausalLM\n",
    "from model.long import LongConfig, LongForCausalLM\n",
    "from model.long_new import LongConfig, LongHFModel\n",
    "from transformers import MambaConfig, MambaForCausalLM\n",
    "from transformers import GPT2Config, GPT2LMHeadModel\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm \n",
    "import numpy as np\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6803546-3711-4e8e-9b04-8e5e6f1101c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeedleHaystackDataset(Dataset):\n",
    "    def __init__(self, size=2000, min_len=32, max_len=64, vocab_size=128, depth=None):\n",
    "        \"\"\"\n",
    "        depth: Float between 0.0 and 1.0. \n",
    "               If None, depth is randomized for every sample (0% to 100%).\n",
    "               If set (e.g., 0.5), the needle is always placed at 50% context.\n",
    "        \"\"\"\n",
    "        self.size = size\n",
    "        self.min_len = min_len\n",
    "        self.max_len = max_len\n",
    "        self.vocab_size = vocab_size\n",
    "        self.depth = depth\n",
    "        \n",
    "        # We reserve the last token as the specific \"Prompt\" trigger\n",
    "        self.prompt_token = torch.tensor([vocab_size - 1]) \n",
    "        \n",
    "    def __len__(self): \n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 1. Determine total length of the sequence\n",
    "        curr_len = np.random.randint(self.min_len, self.max_len + 1)\n",
    "        \n",
    "        # 2. Generate the Key (Needle)\n",
    "        # Range: [1, vocab-2] to avoid padding (0) and prompt token (vocab-1)\n",
    "        key = torch.randint(1, self.vocab_size - 1, (1,))\n",
    "        \n",
    "        # 3. Generate Noise (Haystack)\n",
    "        # We need space for 2 prompt tokens and 2 key tokens (4 tokens total overhead)\n",
    "        noise_len = max(0, curr_len - 4)\n",
    "        noise = torch.randint(1, self.vocab_size - 1, (noise_len,))\n",
    "        \n",
    "        # 4. Determine Insertion Point (Depth)\n",
    "        if self.depth is not None:\n",
    "            # Fixed depth (e.g., 0.9 for 90% deep)\n",
    "            insert_idx = int(noise_len * self.depth)\n",
    "        else:\n",
    "            # Fully Random depth (0% to 100%)\n",
    "            # FIX: Previously this was noise_len // 2 (biased to start)\n",
    "            insert_idx = torch.randint(0, noise_len + 1, (1,)).item()\n",
    "        \n",
    "        # 5. Construct Sequence\n",
    "        # [Noise Part A] -> [Prompt] -> [Key] -> [Noise Part B] -> [Prompt] -> [Key (Target)]\n",
    "        input_ids = torch.cat([\n",
    "            noise[:insert_idx], \n",
    "            self.prompt_token, key,      \n",
    "            noise[insert_idx:], \n",
    "            self.prompt_token, key       \n",
    "        ])\n",
    "        \n",
    "        # 6. Create Labels (Mask everything except the final Key)\n",
    "        labels = input_ids.clone()\n",
    "        # Mask everything up to the final token\n",
    "        labels[:-1] = -100 \n",
    "        \n",
    "        return {\"input_ids\": input_ids, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f68c51a-e10e-4316-bd22-4a149add585f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 20 \n",
    "VOCAB_SIZE = 1000\n",
    "\n",
    "train_dataset = NeedleHaystackDataset(size=50000, min_len = 10, max_len = 30,  vocab_size=VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d19eb87-f33a-4a8a-b50c-fc58b1abb230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the input_ids: torch.Size([21])\n",
      "Shape of the labels: torch.Size([21])\n",
      "\n",
      "\n",
      "Input_ids: tensor([798, 618, 524,   2, 479,  52, 854, 920, 999, 268, 259, 783, 286, 141,\n",
      "        936, 641, 388,  44, 337, 999, 268])\n",
      "Labels: tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100,  268])\n"
     ]
    }
   ],
   "source": [
    "examples = train_dataset[0]\n",
    "input_ids, labels = examples['input_ids'], examples['labels']\n",
    "\n",
    "print(f\"Shape of the input_ids: {input_ids.shape}\")\n",
    "print(f\"Shape of the labels: {labels.shape}\")\n",
    "print(\"\\n\")\n",
    "print(f\"Input_ids: {input_ids}\")\n",
    "print(f\"Labels: {labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88adf2e0-9931-4bc2-b93e-daf712677d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_collate_fn(batch):\n",
    "    inputs = rnn_utils.pad_sequence([x['input_ids'] for x in batch], batch_first=True, padding_value=0)\n",
    "    labels = rnn_utils.pad_sequence([x['labels'] for x in batch], batch_first=True, padding_value=-100)\n",
    "    return {\"input_ids\": inputs, \"labels\": labels}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c52c5675-21d1-4eb7-a060-f7c5f72bdbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(logits, labels, top_k=5):\n",
    "    \"\"\"\n",
    "    Calculates Top-1 and Top-K accuracy for the final token prediction.\n",
    "    Assumes NO padding (all sequences in batch end at the last index).\n",
    "    \"\"\"\n",
    "    # 1. Get the actual target token (the very last token in the sequence)\n",
    "    # Shape: [Batch]\n",
    "    targets = labels[:, -1]\n",
    "\n",
    "    # 2. Get the logits used to predict that target (from the second-to-last position)\n",
    "    # Shape: [Batch, Vocab_Size]\n",
    "    # Note: Logic is \"token at T-2 predicts token at T-1\" (0-indexed)\n",
    "    predictions = logits[:, -2, :]\n",
    "\n",
    "    # --- Top-1 Accuracy ---\n",
    "    # Check if the highest probability token matches the target\n",
    "    top1 = (predictions.argmax(dim=-1) == targets).float().mean().item()\n",
    "\n",
    "    # --- Top-K Accuracy ---\n",
    "    # Check if the target is within the top K probability tokens\n",
    "    _, top_indices = predictions.topk(top_k, dim=-1) # [Batch, K]\n",
    "    topk = (top_indices == targets.unsqueeze(1)).any(dim=1).float().mean().item()\n",
    "\n",
    "    return top1, topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee732967-f2be-4c29-93f0-9506a9b4169d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_pipeline(model_name, model, train_loader, eval_seq_lens, vocab_size, device):\n",
    "    \"\"\"\n",
    "    Updated to run for 10 Epochs.\n",
    "    \"\"\"\n",
    "    print(f\"\\n[{model_name}] Starting Training...\")\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.003, weight_decay=0.01)\n",
    "    \n",
    "    # --- NEW: Define Epochs ---\n",
    "    NUM_EPOCHS = 10\n",
    "    model.train()\n",
    "\n",
    "    # --- TRAINING PHASE ---\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        epoch_loss = []\n",
    "        # Wrap the loader in a progress bar for each epoch\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [{model_name}]\", leave=True)\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=input_ids)\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            # Shift for Causal LM loss\n",
    "            shift_logits = logits[:, :-1, :].contiguous()\n",
    "            shift_labels = labels[:, 1:].contiguous()\n",
    "            \n",
    "            loss = F.cross_entropy(\n",
    "                shift_logits.view(-1, vocab_size), \n",
    "                shift_labels.view(-1)\n",
    "            )\n",
    "            \n",
    "            # Backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update progress bar\n",
    "            epoch_loss.append(loss.item())\n",
    "            progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "        \n",
    "        print(f\"Epoch {epoch+1} Average Loss: {np.mean(epoch_loss):.4f}\")\n",
    "\n",
    "    # --- EVALUATION PHASE (Length Generalization) ---\n",
    "    # (Remains 1 pass per length as discussed, to test the model's final state)\n",
    "    print(f\"\\n[{model_name}] Evaluating Length Generalization...\")\n",
    "    results = {\"lengths\": [], \"acc\": [], \"top5\": []}\n",
    "    \n",
    "    model.eval()\n",
    "    for length in eval_seq_lens:\n",
    "        eval_ds = NeedleHaystackDataset(size=100, min_len=length, max_len=length, vocab_size=vocab_size)\n",
    "        eval_loader = DataLoader(eval_ds, batch_size=1, shuffle=False)\n",
    "        \n",
    "        batch_accs, batch_top5s = [], []\n",
    "        inner_pbar = tqdm(eval_loader, desc=f\"Eval L={length}\", leave=False)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in inner_pbar:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                try:\n",
    "                    output = model(input_ids=input_ids)\n",
    "                    t1, t5 = calculate_accuracy(output.logits, input_ids)\n",
    "                    batch_accs.append(t1)\n",
    "                    batch_top5s.append(t5)\n",
    "                    del output, input_ids\n",
    "                except RuntimeError as e:\n",
    "                    if \"out of memory\" in str(e).lower():\n",
    "                        print(f\"OOM at length {length}!\")\n",
    "                        torch.cuda.empty_cache()\n",
    "                        batch_accs.append(0.0)\n",
    "                        batch_top5s.append(0.0)\n",
    "                        break\n",
    "                    else: raise e\n",
    "\n",
    "        avg_acc = np.mean(batch_accs) if batch_accs else 0.0\n",
    "        avg_top5 = np.mean(batch_top5s) if batch_top5s else 0.0\n",
    "        \n",
    "        print(f\"  Length {length}: Top-1={avg_acc:.1%} | Top-5={avg_top5:.1%}\")\n",
    "        results[\"lengths\"].append(length)\n",
    "        results[\"acc\"].append(avg_acc)\n",
    "        results[\"top5\"].append(avg_top5)\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "739ce14d-9ea6-46ea-9ddb-79dc53b3dd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "VOCAB_SIZE = 128\n",
    "MIN_SEQ_LEN = 32\n",
    "MAX_SEQ_LEN = 128\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26d4ba3d-c5a2-4131-9732-08511cd03ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = NeedleHaystackDataset(size=10000, \n",
    "                            min_len = MIN_SEQ_LEN, \n",
    "                            max_len = MAX_SEQ_LEN,  \n",
    "                            vocab_size=VOCAB_SIZE)\n",
    "\n",
    "# 2. Create the DataLoader\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    collate_fn=pad_collate_fn\n",
    ")\n",
    "\n",
    "\n",
    "config_kwargs = {\n",
    "     \"vocab_size\": VOCAB_SIZE, \n",
    "     \"ssm_cfg\": {\"dropout\": 0.0 }\n",
    "}\n",
    "mamba_config = MambaConfig(\n",
    "    hidden_size = 64,\n",
    "    num_hidden_layers = 2, \n",
    "    **config_kwargs\n",
    ")\n",
    "mamba_model = MambaForCausalLM(mamba_config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a57aed0b-a1aa-486d-8289-1f94c9195835",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LongHFModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Long-Attention] Starting Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "131c689a251d4cb89889ba0ac44fd039",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/10 [Long-Attention]:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Average Loss: 4.9013\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7dbb13a6c66498fb4a6718c52e33eaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/10 [Long-Attention]:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Average Loss: 3.7384\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6553f41343314545a2fc7323073d5f1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/10 [Long-Attention]:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Average Loss: 0.0216\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4b466ffc23d42db8abbfd48b3da64f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/10 [Long-Attention]:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Average Loss: 0.0038\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c171e0ee2d784317afb4553ad4f24707",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/10 [Long-Attention]:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      1\u001b[39m long_config = LongConfig(\n\u001b[32m      2\u001b[39m     vocab_size = VOCAB_SIZE, \n\u001b[32m      3\u001b[39m     hidden_size = \u001b[32m64\u001b[39m, \n\u001b[32m   (...)\u001b[39m\u001b[32m      6\u001b[39m     num_heads = \u001b[32m4\u001b[39m\n\u001b[32m      7\u001b[39m )\n\u001b[32m      8\u001b[39m long_model = LongHFModel(long_config).to(device)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m results_long = \u001b[43mtrain_eval_pipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mLong-Attention\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlong_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m        \u001b[49m\u001b[43meval_seq_lens\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m18\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mVOCAB_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mtrain_eval_pipeline\u001b[39m\u001b[34m(model_name, model, train_loader, eval_seq_lens, vocab_size, device)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Wrap the loader in a progress bar for each epoch\u001b[39;00m\n\u001b[32m     16\u001b[39m progress_bar = tqdm(train_loader, desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_EPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m, leave=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput_ids\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlabels\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/long-attention/.venv/lib/python3.12/site-packages/tqdm/notebook.py:250\u001b[39m, in \u001b[36mtqdm_notebook.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    249\u001b[39m     it = \u001b[38;5;28msuper\u001b[39m().\u001b[34m__iter__\u001b[39m()\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# return super(tqdm...) will not catch exception\u001b[39;49;00m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/long-attention/.venv/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/long-attention/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/long-attention/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/long-attention/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mNeedleHaystackDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     40\u001b[39m     insert_idx = torch.randint(\u001b[32m0\u001b[39m, noise_len + \u001b[32m1\u001b[39m, (\u001b[32m1\u001b[39m,)).item()\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# 5. Construct Sequence\u001b[39;00m\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# [Noise Part A] -> [Prompt] -> [Key] -> [Noise Part B] -> [Prompt] -> [Key (Target)]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m input_ids = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43minsert_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprompt_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m[\u001b[49m\u001b[43minsert_idx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprompt_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m       \u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# 6. Create Labels (Mask everything except the final Key)\u001b[39;00m\n\u001b[32m     52\u001b[39m labels = input_ids.clone()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "long_config = LongConfig(\n",
    "    vocab_size = VOCAB_SIZE, \n",
    "    hidden_size = 64, \n",
    "    expansion_ratio = 4, \n",
    "    num_hidden_layers = 2, \n",
    "    num_heads = 4\n",
    ")\n",
    "long_model = LongHFModel(long_config).to(device)\n",
    "\n",
    "\n",
    "\n",
    "results_long = train_eval_pipeline(\n",
    "        \"Long-Attention\", \n",
    "        long_model, \n",
    "        train_loader, \n",
    "        eval_seq_lens=[2**i for i in range(8, 18)],\n",
    "        vocab_size=VOCAB_SIZE, \n",
    "        device=device\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8336a5ed-8632-4035-82b2-7965aa01e4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "holo_config =  HoloConfig(\n",
    "    d_model=128, \n",
    "    num_hidden_layers = 2,\n",
    "    num_heads = 4,\n",
    "    vocab_size=VOCAB_SIZE, \n",
    "    resid_dropout = 0.0, \n",
    "    dropout = 0.0,\n",
    "    use_version=2\n",
    ")\n",
    "holo_model = HoloForCausalLM(holo_config).to(device)\n",
    "\n",
    "results_holo = train_eval_pipeline(\n",
    "        \"Holo-Attention\", \n",
    "        holo_model, \n",
    "        train_loader, \n",
    "        eval_seq_lens=[2**i for i in range(8, 18)],\n",
    "        vocab_size=VOCAB_SIZE, \n",
    "        device=device\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe55b9e-5caa-4070-b169-a2ff7edb3873",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_mamba = train_eval_pipeline(\n",
    "        \"Mamba\", \n",
    "        mamba_model, \n",
    "        train_loader, \n",
    "        eval_seq_lens=[2**i for i in range(8, 18)], \n",
    "        vocab_size=VOCAB_SIZE, \n",
    "        device=device\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef5408d-c34e-4436-994b-10e560e08eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_seq_lens=[2**i for i in range(8, 21)]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plotting Mamba\n",
    "plt.plot(eval_seq_lens, results_mamba, marker='o', linestyle='-', linewidth=2, label='Mamba')\n",
    "\n",
    "# Plotting Holo\n",
    "# plt.plot(eval_seq_lens, results_holo1, marker='s', linestyle='--', linewidth=2, label='Holo')\n",
    "plt.plot(eval_seq_lens, results_holo, marker='s', linestyle='-', linewidth=2, label='Holo')\n",
    "\n",
    "# Formatting the X-axis to be Logarithmic (essential for exponential lengths)\n",
    "plt.xscale('log')\n",
    "plt.xticks(eval_seq_lens, labels=eval_seq_lens) # Set specific ticks for our lengths\n",
    "\n",
    "# Labels and Title\n",
    "plt.xlabel('Sequence Length', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.title('Needle In A Haystack: Length Generalization', fontsize=14)\n",
    "plt.ylim(-0.05, 1.05) # Keep y-axis range clean\n",
    "plt.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "# Save the plot\n",
    "plt.tight_layout()\n",
    "plt.savefig('niah_results_plot.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21b4a9c-6c43-4d10-be46-8f60e964058d",
   "metadata": {},
   "source": [
    "### Play around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f88c4c-cd0d-464f-89c3-1a529852ce1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (MyProject)",
   "language": "python",
   "name": "myproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
