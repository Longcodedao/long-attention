{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b9decb2-9850-49ac-8d75-0ccf50753889",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7800b71-a63d-4ac0-aa79-9659dd1c64ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from model.holo import HoloConfig, HoloForCausalLM\n",
    "from model.long import LongConfig, LongForCausalLM\n",
    "from transformers import MambaConfig, MambaForCausalLM\n",
    "from transformers import GPT2Config, GPT2LMHeadModel\n",
    "\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm \n",
    "import numpy as np\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6803546-3711-4e8e-9b04-8e5e6f1101c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NIAHDataset(Dataset):\n",
    "    def __init__(self, size, min_len, max_len, vocab_size):\n",
    "        self.size = size\n",
    "        self.min_len = min_len\n",
    "        self.max_len = max_len\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # Special Tokens\n",
    "        self.start_tokens = torch.tensor([3, 4])\n",
    "        self.trigger_tokens = torch.tensor([5, 4, 3, 6]) # e.g. \"The answer is\"\n",
    "        self.flag_token = torch.tensor([2]) \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 1. Randomize Length for this specific sample\n",
    "        # This prevents the model from overfitting to a specific horizon\n",
    "        curr_len = np.random.randint(self.min_len, self.max_len + 1)\n",
    "        \n",
    "        # 2. Generate Key\n",
    "        key = torch.randint(10, self.vocab_size, (1,))\n",
    "\n",
    "        # 3. Calculate Noise\n",
    "        # Overhead: Start(2) + Flag(1) + Key(1) + Trigger(4) + Target(1) = 9\n",
    "        noise_len = curr_len - 9\n",
    "        if noise_len < 0: noise_len = 0 # Safety clipping\n",
    "\n",
    "        noise = torch.randint(10, self.vocab_size, (noise_len,))\n",
    "\n",
    "        # 4. Insert Key Randomly\n",
    "        insert_idx = torch.randint(0, noise_len + 1, (1,)).item()\n",
    "        \n",
    "        input_ids = torch.cat([\n",
    "            self.start_tokens,\n",
    "            noise[:insert_idx],\n",
    "            self.flag_token,     # The Flag\n",
    "            key,                 # The Needle\n",
    "            noise[insert_idx:],\n",
    "            self.trigger_tokens,\n",
    "            key\n",
    "        ])\n",
    "\n",
    "        labels = input_ids.clone()\n",
    "\n",
    "        # Mask everything but unmask the last token (label)\n",
    "        labels[:-1] = -100 \n",
    "        labels[-1] = input_ids[-1]\n",
    "\n",
    "        # UNMASK THE TRIGGER (Optional but recommended for stability)\n",
    "        # This teaches the model to recognize \"The question is coming\"\n",
    "        trigger_len = len(self.trigger_tokens)\n",
    "        labels[-(trigger_len + 1): -1] = self.trigger_tokens\n",
    "        \n",
    "        return {\"input_ids\": input_ids, \"labels\": labels}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f68c51a-e10e-4316-bd22-4a149add585f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 20 \n",
    "VOCAB_SIZE = 1000\n",
    "\n",
    "train_dataset = NIAHDataset(size=50000, min_len = 10, max_len = 30,  vocab_size=VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d19eb87-f33a-4a8a-b50c-fc58b1abb230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the input_ids: torch.Size([15])\n",
      "Shape of the labels: torch.Size([15])\n",
      "\n",
      "\n",
      "Input_ids: tensor([  3,   4, 127, 159, 903, 832, 895,   2, 168, 461,   5,   4,   3,   6,\n",
      "        168])\n",
      "Labels: tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100,    5,    4,\n",
      "           3,    6,  168])\n"
     ]
    }
   ],
   "source": [
    "examples = train_dataset[0]\n",
    "input_ids, labels = examples['input_ids'], examples['labels']\n",
    "\n",
    "print(f\"Shape of the input_ids: {input_ids.shape}\")\n",
    "print(f\"Shape of the labels: {labels.shape}\")\n",
    "print(\"\\n\")\n",
    "print(f\"Input_ids: {input_ids}\")\n",
    "print(f\"Labels: {labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88adf2e0-9931-4bc2-b93e-daf712677d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_collate_fn(batch):\n",
    "    # Extract inputs and labels\n",
    "    inputs = [item['input_ids'] for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "    \n",
    "    # Pad sequences to the longest in the batch (dynamic padding)\n",
    "    # batch_first=True makes shape [Batch, Seq_Len]\n",
    "    inputs_padded = rnn_utils.pad_sequence(inputs, batch_first=True, padding_value=0)\n",
    "    labels_padded = rnn_utils.pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": inputs_padded,\n",
    "        \"labels\": labels_padded\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c52c5675-21d1-4eb7-a060-f7c5f72bdbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(target_logits, labels, top_k=5):\n",
    "    \"\"\"\n",
    "    Calculates accuracy for the final token prediction, ignoring padding.\n",
    "    Assumes padding in labels is 0 (or invalid tokens), and computes per-sequence lengths.\n",
    "    \"\"\"\n",
    "    # Compute sequence lengths (assuming padding_value=0 in input_ids/labels)\n",
    "    # This finds the last non-padding position\n",
    "    mask = labels != 0  # [Batch, Seq_len]\n",
    "    lengths = mask.sum(dim=1)  # [Batch] effective lengths\n",
    "\n",
    "    # Ensure lengths are at least 2 (for -2 and -1 positions)\n",
    "    valid_mask = lengths >= 2  # [Batch] boolean mask for valid sequences\n",
    "\n",
    "    if not valid_mask.any():\n",
    "        return 0.0, 0.0  # No valid sequences\n",
    "\n",
    "    # Filter to only valid batches\n",
    "    valid_indices = valid_mask.nonzero(as_tuple=False).squeeze(1)  # Indices of valid batches\n",
    "    valid_lengths = lengths[valid_indices]\n",
    "    valid_labels = labels[valid_indices]\n",
    "    valid_logits = target_logits[valid_indices]\n",
    "\n",
    "    # Compute target positions (last token) and pred positions (token before last)\n",
    "    target_pos = valid_lengths - 1  # [Valid_Batch]\n",
    "    pred_pos = valid_lengths - 2    # [Valid_Batch]\n",
    "\n",
    "    # Gather target tokens\n",
    "    target_token = valid_labels.gather(dim=1, index=target_pos.unsqueeze(1)).squeeze(1)  # [Valid_Batch]\n",
    "\n",
    "    # Gather key logits (predictions for the target)\n",
    "    batch_indices = torch.arange(len(valid_indices), device=labels.device)  # [Valid_Batch]\n",
    "    key_logit = valid_logits[batch_indices, pred_pos, :]  # [Valid_Batch, Vocab]\n",
    "\n",
    "    # --- Top-1 Accuracy ---\n",
    "    pred_token = torch.argmax(key_logit, dim=-1)  # [Valid_Batch]\n",
    "    top1 = (pred_token == target_token).float().mean().item()\n",
    "\n",
    "    # --- Top-K Accuracy ---\n",
    "    _, top_k_indices = torch.topk(key_logit, k=top_k, dim=-1)  # [Valid_Batch, k]\n",
    "    target_expanded = target_token.unsqueeze(1)  # [Valid_Batch, 1]\n",
    "    topk_acc = (top_k_indices == target_expanded).any(dim=1).float().mean().item()\n",
    "\n",
    "    return top1, topk_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee732967-f2be-4c29-93f0-9506a9b4169d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_pipeline(model_name, model, train_loader, eval_seq_lens, vocab_size, device):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        model_name (str): Name for logging.\n",
    "        model (nn.Module): The HF-style model (returns outputs.loss).\n",
    "        train_loader (DataLoader): Loader for the fixed-length training data.\n",
    "        eval_seq_lens (list): List of lengths to test generalization on (e.g. [256, 512, 1024]).\n",
    "        vocab_size (int): Vocab size for generating eval data on the fly.\n",
    "        device (str): 'cuda' or 'cpu'.\n",
    "    \"\"\"\n",
    "    print(f\"\\n[{model_name}] Starting Training...\")\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.01)\n",
    "    model.train()\n",
    "\n",
    "    # --- TRAINING PHASE ---\n",
    "    # We use the train_loader which usually has a fixed sequence length (e.g. 256)\n",
    "    # This replaces the \"variable length loop\" from the old code, \n",
    "    # relying on the model to learn the mechanism from the fixed length dataset.\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=f\"Training {model_name}\", leave=True)\n",
    "    \n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        # Move batch to device\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Forward pass (HF models compute loss automatically if labels are passed)\n",
    "        outputs = model(input_ids=input_ids, labels = labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    # --- EVALUATION PHASE (Length Generalization) ---\n",
    "    print(f\"\\n[{model_name}] Evaluating Length Generalization...\")\n",
    "    results = {\"lengths\": [], \"acc\": [], \"top5\": []}\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    for length in eval_seq_lens:\n",
    "        # Generate a small temporary dataset for this specific length\n",
    "        # This mirrors the 'generator_fn' logic from the old code\n",
    "        # eval_ds = NIAHDataset(size=100, seq_len=length, vocab_size=vocab_size)\n",
    "        eval_ds = NIAHDataset(size=100,\n",
    "                              min_len = length, \n",
    "                              max_len = length,  \n",
    "                              vocab_size=vocab_size)\n",
    "        eval_loader = DataLoader(eval_ds, batch_size=1, shuffle=False)\n",
    "        \n",
    "        batch_accs = []\n",
    "        batch_top5s = []\n",
    "\n",
    "        # Inner loop progress bar (Eval can be slow on long seqs)\n",
    "        inner_pbar = tqdm(eval_loader, desc=f\"Eval L={length}\", leave=False)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in inner_pbar:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "\n",
    "                try:\n",
    "                    # Forward (No labels, we just want logits)\n",
    "                    output = model(input_ids=input_ids)\n",
    "                    \n",
    "                    # Calculate Metrics\n",
    "                    t1, t5 = calculate_accuracy(output.logits, input_ids)\n",
    "                    batch_accs.append(t1)\n",
    "                    batch_top5s.append(t5)\n",
    "                    \n",
    "                    # CRITICAL: Free memory immediately\n",
    "                    del output, input_ids\n",
    "                    \n",
    "                except RuntimeError as e:\n",
    "                    if \"out of memory\" in str(e).lower():\n",
    "                        print(f\"OOM at length {length}!\")\n",
    "                        torch.cuda.empty_cache()\n",
    "                        batch_accs.append(0.0) # Penalty for OOM\n",
    "                        batch_top5s.append(0.0)\n",
    "                        break # Skip rest of this length\n",
    "                    else:\n",
    "                        raise e\n",
    "\n",
    "        # Aggregate results\n",
    "        avg_acc = np.mean(batch_accs) if batch_accs else 0.0\n",
    "        avg_top5 = np.mean(batch_top5s) if batch_top5s else 0.0\n",
    "        \n",
    "        print(f\"  Length {length}: Top-1={avg_acc:.1%} | Top-5={avg_top5:.1%}\")\n",
    "        \n",
    "        results[\"lengths\"].append(length)\n",
    "        results[\"acc\"].append(avg_acc)\n",
    "        results[\"top5\"].append(avg_top5)\n",
    "        \n",
    "        # Clean cache between lengths\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "739ce14d-9ea6-46ea-9ddb-79dc53b3dd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "VOCAB_SIZE = 1000\n",
    "MIN_SEQ_LEN = 32\n",
    "MAX_SEQ_LEN = 256\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01fa096b-1746-4fa0-8abc-a595e7f37d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  3,   4, 662, 767, 675, 214, 117, 862, 618, 832,   2, 168, 860, 122,\n",
      "         873, 303, 889,  84, 450, 540, 673, 281, 576, 691, 986,   5,   4,   3,\n",
      "           6, 168],\n",
      "        [  3,   4, 987, 907, 288, 819, 282, 275, 253, 976,  80, 574, 384, 839,\n",
      "         126,   2, 617, 845, 606, 701, 510, 774, 180,   5,   4,   3,   6, 617,\n",
      "           0,   0],\n",
      "        [  3,   4, 946, 661,  50, 392, 603, 346,  69, 379, 354, 704, 527, 850,\n",
      "         631,   2, 402, 897, 101,   5,   4,   3,   6, 402,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [  3,   4, 333,   2, 586,   5,   4,   3,   6, 586,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0]])\n",
      "tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100,    5,    4,    3,    6,  168],\n",
      "        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,    5,\n",
      "            4,    3,    6,  617, -100, -100],\n",
      "        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100,    5,    4,    3,    6,  402,\n",
      "         -100, -100, -100, -100, -100, -100],\n",
      "        [-100, -100, -100, -100, -100,    5,    4,    3,    6,  586, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100]])\n"
     ]
    }
   ],
   "source": [
    "train_dataset = NIAHDataset(size=50000, min_len = 10, max_len = 30,  vocab_size=VOCAB_SIZE)\n",
    "\n",
    "# 2. Create the DataLoader\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=4, \n",
    "    shuffle=True, \n",
    "    num_workers=2,# Can use parallel loading now\n",
    "    collate_fn=pad_collate_fn\n",
    ")\n",
    "\n",
    "iter_loader = next(iter(train_loader))\n",
    "inputs_ids, labels = iter_loader['input_ids'], iter_loader['labels']\n",
    "\n",
    "print(inputs_ids)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26d4ba3d-c5a2-4131-9732-08511cd03ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LongForCausalLM(\n",
      "  (long_model): LongModel(\n",
      "    (wte): Embedding(1000, 128)\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x LongBlock(\n",
      "        (attn): LongAttention(\n",
      "          (q_proj): Linear(in_features=128, out_features=128, bias=False)\n",
      "          (k_proj): Linear(in_features=128, out_features=128, bias=False)\n",
      "          (v_proj): Linear(in_features=128, out_features=128, bias=False)\n",
      "          (conv): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)\n",
      "          (input_gate_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (output_gate_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (gamma_proj): Linear(in_features=128, out_features=4, bias=True)\n",
      "          (o_proj): Linear(in_features=128, out_features=128, bias=False)\n",
      "          (grp_norm): GroupNorm(4, 128, eps=1e-05, affine=True)\n",
      "          (mem_norm): GroupNorm(4, 128, eps=1e-05, affine=True)\n",
      "        )\n",
      "        (mlp): LongMLP(\n",
      "          (fc1): Linear(in_features=128, out_features=512, bias=False)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=512, out_features=128, bias=False)\n",
      "        )\n",
      "        (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=128, out_features=1000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "train_dataset = NIAHDataset(size=200000, \n",
    "                            min_len = MIN_SEQ_LEN, \n",
    "                            max_len = MAX_SEQ_LEN,  \n",
    "                            vocab_size=VOCAB_SIZE)\n",
    "\n",
    "# 2. Create the DataLoader\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    num_workers=2, # Can use parallel loading now\n",
    "    collate_fn=pad_collate_fn\n",
    ")\n",
    "\n",
    "# Use version 2 meaning that using the newest LongAttention inside the HOLOBlock\n",
    "# holo_config =  HoloConfig(\n",
    "#     d_model=128, \n",
    "#     num_hidden_layers = 2,\n",
    "#     num_heads = 4,\n",
    "#     vocab_size=VOCAB_SIZE, \n",
    "#     resid_dropout = 0.0, \n",
    "#     dropout = 0.0,\n",
    "#     use_version=2\n",
    "# )\n",
    "# holo_model = HoloForCausalLM(holo_config).to(device)\n",
    "long_config = LongConfig(\n",
    "    vocab_size = VOCAB_SIZE, \n",
    "    hidden_size = 128, \n",
    "    expansion_ratio = 4, \n",
    "    num_hidden_layers = 2, \n",
    "    num_heads = 4\n",
    ")\n",
    "long_model = LongForCausalLM(long_config).to(device)\n",
    "\n",
    "\n",
    "config_kwargs = {\n",
    "     \"vocab_size\": VOCAB_SIZE, \n",
    "     \"ssm_cfg\": {\"dropout\": 0.0 }\n",
    "}\n",
    "mamba_config = MambaConfig(\n",
    "    hidden_size = 128,\n",
    "    num_hidden_layers = 2, \n",
    "    **config_kwargs\n",
    ")\n",
    "print(long_model)\n",
    "mamba_model = MambaForCausalLM(mamba_config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49f773c1-92d9-49c5-b989-7af174423761",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# MAX_LENGTH = 8192  # CRITICAL for Needle-in-Haystack\n",
    "\n",
    "# gpt2_config = GPT2Config(\n",
    "#     vocab_size=VOCAB_SIZE,\n",
    "#     # 1. Context Window\n",
    "#     # GPT-2 has a HARD limit. You must set this >= your max haystack length.\n",
    "#     n_positions=MAX_LENGTH, \n",
    "    \n",
    "#     # 2. Dimensions (Matching your request)\n",
    "#     n_embd=128,       # This is \"hidden_size\"\n",
    "#     n_layer=2,        # This is \"num_hidden_layers\"\n",
    "    \n",
    "#     # 3. Heads\n",
    "#     # n_embd (128) must be divisible by n_head. \n",
    "#     # 4 heads gives 32 dimension per head (standard).\n",
    "#     n_head=4, \n",
    "    \n",
    "#     # 4. Cleanup\n",
    "#     bos_token_id=0,\n",
    "#     eos_token_id=0,\n",
    "    \n",
    "#     # Optional: Disable dropout for pure algorithmic testing (like your Mamba config)\n",
    "#     resid_pdrop=0.0,\n",
    "#     embd_pdrop=0.0,\n",
    "#     attn_pdrop=0.0,\n",
    "#     use_cache=False # False for training with Gradient Checkpointing, True for generation\n",
    "# )\n",
    "\n",
    "# model_gpt2 = GPT2LMHeadModel(gpt2_config).to(device)\n",
    "\n",
    "# results_gpt2 = train_eval_pipeline(\n",
    "#         \"GPT2-Attention\", \n",
    "#         model_gpt2, \n",
    "#         train_loader, \n",
    "#         eval_seq_lens=[2**i for i in range(8, 14)],\n",
    "#         vocab_size=VOCAB_SIZE, \n",
    "#         device=device\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea94855a-270c-4708-9288-76f7efaf0f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import LlamaConfig, LlamaForCausalLM\n",
    "\n",
    "# # 1. Use Llama Configuration (Uses RoPE by default)\n",
    "# config_llama = LlamaConfig(\n",
    "#     vocab_size=VOCAB_SIZE,\n",
    "#     hidden_size=128,        # Match your dims\n",
    "#     intermediate_size=512,  # MLP size (usually 4x hidden)\n",
    "#     num_hidden_layers=2,    # Match your layers\n",
    "#     num_attention_heads=4,  # Match your heads\n",
    "#     max_position_embeddings=16384, # RoPE can handle this easily\n",
    "    \n",
    "#     # 2. Critical Modern Settings\n",
    "#     hidden_act=\"silu\",      # Better than GeLU\n",
    "#     attention_bias=False,   # Flash Attention friendly\n",
    "#     rms_norm_eps=1e-5,\n",
    "    \n",
    "#     # 3. Disable Dropout for pure algorithmic test\n",
    "#     attention_dropout=0.0,\n",
    "#     hidden_dropout=0.0,\n",
    "# )\n",
    "\n",
    "# # 4. Initialize\n",
    "# model_llama = LlamaForCausalLM(config_llama).to(device)\n",
    "\n",
    "# # 5. Run your pipeline\n",
    "# results_llama = train_eval_pipeline(\n",
    "#     \"Llama-RoPE\", \n",
    "#     model_llama, \n",
    "#     train_loader, \n",
    "#     eval_seq_lens=[2**i for i in range(8, 14)], # Up to 8k\n",
    "#     vocab_size=VOCAB_SIZE, \n",
    "#     device=device\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57aed0b-a1aa-486d-8289-1f94c9195835",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Long-Attention] Starting Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd208b4d44bb440b91f37fdaa5dbf7f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Long-Attention:   0%|          | 0/6250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_long = train_eval_pipeline(\n",
    "        \"Long-Attention\", \n",
    "        long_model, \n",
    "        train_loader, \n",
    "        eval_seq_lens=[2**i for i in range(8, 18)],\n",
    "        vocab_size=VOCAB_SIZE, \n",
    "        device=device\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8336a5ed-8632-4035-82b2-7965aa01e4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "holo_config =  HoloConfig(\n",
    "    d_model=128, \n",
    "    num_hidden_layers = 2,\n",
    "    num_heads = 4,\n",
    "    vocab_size=VOCAB_SIZE, \n",
    "    resid_dropout = 0.0, \n",
    "    dropout = 0.0,\n",
    "    use_version=2\n",
    ")\n",
    "holo_model = HoloForCausalLM(holo_config).to(device)\n",
    "\n",
    "results_holo = train_eval_pipeline(\n",
    "        \"Holo-Attention\", \n",
    "        holo_model, \n",
    "        train_loader, \n",
    "        eval_seq_lens=[2**i for i in range(8, 18)],\n",
    "        vocab_size=VOCAB_SIZE, \n",
    "        device=device\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe55b9e-5caa-4070-b169-a2ff7edb3873",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_mamba = train_eval_pipeline(\n",
    "        \"Mamba\", \n",
    "        mamba_model, \n",
    "        train_loader, \n",
    "        eval_seq_lens=[2**i for i in range(8, 18)], \n",
    "        vocab_size=VOCAB_SIZE, \n",
    "        device=device\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef5408d-c34e-4436-994b-10e560e08eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_seq_lens=[2**i for i in range(8, 21)]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plotting Mamba\n",
    "plt.plot(eval_seq_lens, results_mamba, marker='o', linestyle='-', linewidth=2, label='Mamba')\n",
    "\n",
    "# Plotting Holo\n",
    "# plt.plot(eval_seq_lens, results_holo1, marker='s', linestyle='--', linewidth=2, label='Holo')\n",
    "plt.plot(eval_seq_lens, results_holo, marker='s', linestyle='-', linewidth=2, label='Holo')\n",
    "\n",
    "# Formatting the X-axis to be Logarithmic (essential for exponential lengths)\n",
    "plt.xscale('log')\n",
    "plt.xticks(eval_seq_lens, labels=eval_seq_lens) # Set specific ticks for our lengths\n",
    "\n",
    "# Labels and Title\n",
    "plt.xlabel('Sequence Length', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.title('Needle In A Haystack: Length Generalization', fontsize=14)\n",
    "plt.ylim(-0.05, 1.05) # Keep y-axis range clean\n",
    "plt.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "# Save the plot\n",
    "plt.tight_layout()\n",
    "plt.savefig('niah_results_plot.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21b4a9c-6c43-4d10-be46-8f60e964058d",
   "metadata": {},
   "source": [
    "### Play around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f88c4c-cd0d-464f-89c3-1a529852ce1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (MyProject)",
   "language": "python",
   "name": "myproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
