{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "740f9a35-ecce-463c-87df-153dfc6af075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1fc2787-e72e-450f-93d9-af343dae5a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from model.long import LongConfig, LongForCausalLM\n",
    "from model.long import chunked_parallel_scan, recurrent_scan\n",
    "# from transformers import MambaConfig, MambaForCausalLM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6eff33-f128-4c1e-8a37-616dd2b83d1b",
   "metadata": {},
   "source": [
    "Scan Equivalence: Parallel_Output == Recurrent_Output (down to the decimal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3daa54cb-614c-4dc2-aea9-5201b1650ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Difference: 0.00000024\n",
      "✅ SUCCESS: Parallel and Recurrent Scans match!\n"
     ]
    }
   ],
   "source": [
    "def test_kernel_equivalence():\n",
    "    \"\"\"\n",
    "    CRITICAL TEST: \n",
    "    Does the Parallel Scan (Training) match the Recurrent Scan (Inference)?\n",
    "    \"\"\"\n",
    "    torch.manual_seed(42)\n",
    "    B, T, H, D = 2, 64, 4, 32\n",
    "\n",
    "    k = torch.randn(B, T, H, D)\n",
    "    v = torch.randn(B, T, H, D)\n",
    "    gate = torch.sigmoid(torch.randn(B, T, H, D))\n",
    "    gamma = torch.sigmoid(torch.randn(B, T, H, 1)) # Decay\n",
    "\n",
    "    # 1. Run Parallel (Training Mode)\n",
    "    out_parallel = chunked_parallel_scan(k, v, gate, gamma, chunk_size=16)\n",
    "\n",
    "    # 2. Run Recurrent (Inference Mode)\n",
    "    # We initialize state as zeros\n",
    "    state = torch.zeros(B, H, D)\n",
    "    out_recurrent, _ = recurrent_scan(k, v, gate, gamma, state)\n",
    "\n",
    "    # 3. Compare\n",
    "    # We use a relaxed tolerance because chunked_parallel uses float64 internally\n",
    "    # while recurrent might use float32 depending on input.\n",
    "    diff = (out_parallel - out_recurrent).abs().max()\n",
    "    print(f\"Max Difference: {diff.item():.8f}\")\n",
    "\n",
    "    if diff < 1e-4:\n",
    "        print(\"✅ SUCCESS: Parallel and Recurrent Scans match!\")\n",
    "    else:\n",
    "        print(\"❌ FAILURE: Scans diverge. Check padding or float precision.\")\n",
    "\n",
    "test_kernel_equivalence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7702cc0-7370-4547-a435-ee9e329481b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_mixed_generation_consistency():\n",
    "    \"\"\"\n",
    "    Tests if the model produces the same results when:\n",
    "    A) Processing the whole sequence at once (Prompt Processing)\n",
    "    B) Processing token-by-token (Generation)\n",
    "    \n",
    "    This verifies that your mixed State/KV-Cache logic is correct.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- 2. Testing Generation Consistency (Hybrid Cache) ---\")\n",
    "    \n",
    "    # Config with Hybrid Layers (Anchor every 2 layers)\n",
    "    config = LongConfig(\n",
    "        vocab_size=100, \n",
    "        hidden_size=64, \n",
    "        num_hidden_layers=4, \n",
    "        num_heads=4, \n",
    "        hybrid_ratio=2 # Layers 1, 3=Long; 2, 4=Anchor\n",
    "    )\n",
    "    model = LongForCausalLM(config)\n",
    "    model.eval()\n",
    "    \n",
    "    # Dummy Input: [Batch, SeqLen]\n",
    "    input_ids = torch.randint(0, 100, (1, 10))\n",
    "    \n",
    "    # A. Full Forward Pass (Prompt)\n",
    "    with torch.no_grad():\n",
    "        out_full = model(input_ids)\n",
    "    logits_full = out_full.logits\n",
    "    \n",
    "    # B. Step-by-Step Generation (Simulating model.generate)\n",
    "    past_key_values = None\n",
    "    generated_logits = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for t in range(input_ids.shape[1]):\n",
    "            # Feed one token at a time\n",
    "            token = input_ids[:, t:t+1]\n",
    "            \n",
    "            outputs = model(token, past_key_values=past_key_values)\n",
    "            \n",
    "            # Update history\n",
    "            past_key_values = outputs.past_key_values\n",
    "            \n",
    "            # Store logit for this step\n",
    "            generated_logits.append(outputs.logits)\n",
    "\n",
    "    # Concatenate step outputs\n",
    "    logits_step = torch.cat(generated_logits, dim=1)\n",
    "    \n",
    "    # Compare only the last few tokens (early tokens might vary slightly due to warm-up)\n",
    "    diff = (logits_full - logits_step).abs().max()\n",
    "    print(f\"Generation Max Logic Diff: {diff.item():.6f}\")\n",
    "    \n",
    "    if diff < 1e-4:\n",
    "        print(\"✅ SUCCESS: Step-by-step generation matches full forward pass!\")\n",
    "    else:\n",
    "        print(\"❌ FAILURE: Generation drift detected. Check State/KV alignment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa22083f-c1fd-4c1b-b3f8-0d7db2c124d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_backward_pass():\n",
    "    \"\"\"\n",
    "    Ensures gradients flow through the custom kernels without error.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- 3. Testing Backward Pass (Training) ---\")\n",
    "    config = LongConfig(\n",
    "        vocab_size=100, \n",
    "        hidden_size=64, \n",
    "        num_hidden_layers=2, \n",
    "        num_heads=4  # <--- ADDED THIS (64 / 4 = 16)\n",
    "    )\n",
    "    model = LongForCausalLM(config)\n",
    "    \n",
    "    input_ids = torch.randint(0, 100, (2, 32))\n",
    "    labels = input_ids.clone()\n",
    "    \n",
    "    # Forward\n",
    "    outputs = model(input_ids, labels=labels)\n",
    "    loss = outputs.loss\n",
    "    print(f\"Initial Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    # Backward\n",
    "    try:\n",
    "        loss.backward()\n",
    "        print(\"✅ SUCCESS: Gradients computed successfully.\")\n",
    "        \n",
    "        # Check for NaNs in gradients\n",
    "        has_nan = False\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None and torch.isnan(param.grad).any():\n",
    "                print(f\"❌ NaN detected in {name}\")\n",
    "                has_nan = True\n",
    "        \n",
    "        if not has_nan:\n",
    "            print(\"✅ SUCCESS: No NaNs in gradients.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ FAILURE: Backward pass crashed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58bd4264-9952-4c31-bfcd-27a3e02c847d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Difference: 0.00000024\n",
      "✅ SUCCESS: Parallel and Recurrent Scans match!\n",
      "\n",
      "--- 2. Testing Generation Consistency (Hybrid Cache) ---\n",
      "Generation Max Logic Diff: 0.280876\n",
      "❌ FAILURE: Generation drift detected. Check State/KV alignment.\n",
      "\n",
      "--- 3. Testing Backward Pass (Training) ---\n",
      "Initial Loss: 4.6158\n",
      "✅ SUCCESS: Gradients computed successfully.\n",
      "✅ SUCCESS: No NaNs in gradients.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    test_kernel_equivalence()\n",
    "    test_mixed_generation_consistency()\n",
    "    test_backward_pass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c95aa2-f8ba-472b-883f-503aa586ea63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (MyProject)",
   "language": "python",
   "name": "myproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
