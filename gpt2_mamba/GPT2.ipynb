{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13e5376d-63e6-4b05-9128-b4e0ebfe4483",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import math\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import GPT2Config, GPT2LMHeadModel, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "# from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel\n",
    "# from mamba_ssm.utils.hf import load_config_hf, load_state_dict_hf\n",
    "\n",
    "# --- Configuration ---\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.manual_seed(42)\n",
    "\n",
    "CONF = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"batch_size\": 1,           # Adjust based on VRAM\n",
    "    \"grad_accum\": 4,           # Effective batch size = 32\n",
    "    \"lr\": 3e-4,\n",
    "    \"train_steps\": 1000,       # Short run for demo (increase to >10k for real results)\n",
    "    \"eval_every\": 500,\n",
    "    \"dataset_name\": \"DKYoon/SlimPajama-6B\",\n",
    "    \"dataset_subset\": \"default\", \n",
    "}\n",
    "\n",
    "# --- Model Configs for ~370M Parameters ---\n",
    "# GPT-2 Medium-like\n",
    "gpt2_config = GPT2Config(\n",
    "    vocab_size=CONF[\"vocab_size\"],\n",
    "    n_positions=CONF[\"context_length\"],\n",
    "    n_embd=1024,\n",
    "    n_layer=24,\n",
    "    n_head=16,\n",
    "    bos_token_id=50256,\n",
    "    eos_token_id=50256,\n",
    ")\n",
    "\n",
    "# Mamba-2 (approx 370M)\n",
    "# d_model=1024, n_layer=48 is standard for ~370M in Mamba papers\n",
    "mamba2_config_dict = {\n",
    "    \"d_model\": 1024,\n",
    "    \"n_layer\": 48,\n",
    "    \"vocab_size\": CONF[\"vocab_size\"],\n",
    "    \"ssm_cfg\": {\"layer\": \"Mamba2\"}, \n",
    "    \"rms_norm\": True,\n",
    "    \"residual_in_fp32\": True,\n",
    "    \"fused_add_norm\": True,\n",
    "    \"pad_vocab_size_multiple\": 8\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2326e22-a92e-463e-9f0c-802b4567b897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8773f22cde1a4b87811974c7328eb8ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e18071c3d3944433bde39a38841d83f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets initialized (Streaming Mode).\n"
     ]
    }
   ],
   "source": [
    "# Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def get_dataloader(split=\"train\"):\n",
    "    \"\"\"Streams SlimPajama, tokenizes, and returns a DataLoader.\"\"\"\n",
    "    # Streaming=True avoids downloading the whole 20GB+ dataset\n",
    "    dataset = load_dataset(CONF[\"dataset_name\"], split=split, streaming=True)\n",
    "    \n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples[\"text\"], \n",
    "            truncation=True, \n",
    "            max_length=CONF[\"context_length\"], \n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "\n",
    "    # Only keep input_ids to save memory\n",
    "    tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\", \"meta\"])\n",
    "    tokenized_dataset = tokenized_dataset.with_format(\"torch\")\n",
    "    \n",
    "    return DataLoader(tokenized_dataset, batch_size=CONF[\"batch_size\"])\n",
    "\n",
    "# Initialize Loaders\n",
    "train_loader = get_dataloader(\"train\")\n",
    "lambada_dataset = load_dataset(\"cimec/lambada\", split=\"validation\")\n",
    "print(\"Datasets initialized (Streaming Mode).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed48cc2a-0cf8-4ca7-9347-2c92dc4213a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Trainer:\n",
    "    def __init__(self, config):\n",
    "        self.model = GPT2LMHeadModel(config).to(device)\n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=CONF[\"lr\"])\n",
    "        # Store metrics to save later\n",
    "        self.metrics = {\n",
    "            \"model_name\": \"GPT-2 (350M)\",\n",
    "            \"loss\": [], \n",
    "            \"memory_gb\": [], \n",
    "            \"throughput_tok_sec\": [], \n",
    "            \"lambada_acc\": 0.0,\n",
    "            \"inference_speed\": 0.0\n",
    "        }\n",
    "\n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "\n",
    "    def train(self, steps):\n",
    "        self.model.train()\n",
    "        data_iter = iter(train_loader)\n",
    "        progress_bar = tqdm(range(steps), desc=\"Training GPT-2\")\n",
    "        \n",
    "        for i in progress_bar:\n",
    "            try:\n",
    "                batch = next(data_iter)\n",
    "            except StopIteration:\n",
    "                data_iter = iter(train_loader)\n",
    "                batch = next(data_iter)\n",
    "            \n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = input_ids.clone()\n",
    "            \n",
    "            # Tracking\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            t0 = time.time()\n",
    "            \n",
    "            # Forward & Backward\n",
    "            outputs = self.model(input_ids, labels=labels)\n",
    "            loss = outputs.loss / CONF[\"grad_accum\"]\n",
    "            loss.backward()\n",
    "            \n",
    "            if (i + 1) % CONF[\"grad_accum\"] == 0:\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                # Metrics\n",
    "                dt = time.time() - t0\n",
    "                tokens_processed = input_ids.numel()\n",
    "                mem_gb = torch.cuda.max_memory_allocated() / 1e9\n",
    "                \n",
    "                # Log actual loss (not scaled)\n",
    "                current_loss = loss.item() * CONF[\"grad_accum\"]\n",
    "                self.metrics[\"loss\"].append(current_loss)\n",
    "                self.metrics[\"memory_gb\"].append(mem_gb)\n",
    "                self.metrics[\"throughput_tok_sec\"].append(tokens_processed / dt)\n",
    "                \n",
    "                progress_bar.set_postfix({\n",
    "                    \"Loss\": f\"{current_loss:.4f}\", \n",
    "                    \"Mem\": f\"{mem_gb:.2f}GB\"\n",
    "                })\n",
    "\n",
    "    def evaluate_lambada(self, n_samples=200):\n",
    "        \"\"\"Zero-shot evaluation on LAMBADA (Last Word Prediction).\"\"\"\n",
    "        print(\"Running LAMBADA Evaluation...\")\n",
    "        self.model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for i, row in enumerate(lambada_dataset):\n",
    "            if i >= n_samples: break\n",
    "            \n",
    "            full_text = row['text']\n",
    "            split_text = full_text.split()\n",
    "            context = \" \".join(split_text[:-1])\n",
    "            target_word = \" \" + split_text[-1]\n",
    "            \n",
    "            inputs = tokenizer(context, return_tensors=\"pt\").to(device)\n",
    "            target_id = tokenizer(target_word)[\"input_ids\"][0]\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(inputs[\"input_ids\"])\n",
    "                pred_id = torch.argmax(outputs.logits[0, -1, :]).item()\n",
    "            \n",
    "            if pred_id == target_id:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "            \n",
    "        acc = correct / total\n",
    "        self.metrics[\"lambada_acc\"] = acc\n",
    "        print(f\"LAMBADA Accuracy: {acc:.2%}\")\n",
    "\n",
    "    def measure_inference_speed(self, n_tokens=50):\n",
    "        \"\"\"Benchmarks generation speed.\"\"\"\n",
    "        print(\"Benchmarking Inference Speed...\")\n",
    "        self.model.eval()\n",
    "        input_ids = torch.randint(0, CONF[\"vocab_size\"], (1, 128)).to(device)\n",
    "        \n",
    "        # Warmup\n",
    "        _ = self.model(input_ids)\n",
    "        \n",
    "        start = time.time()\n",
    "        curr_ids = input_ids\n",
    "        with torch.no_grad():\n",
    "            for _ in range(n_tokens):\n",
    "                outputs = self.model(curr_ids)\n",
    "                next_token = torch.argmax(outputs.logits[:, -1, :], dim=-1).unsqueeze(1)\n",
    "                curr_ids = torch.cat([curr_ids, next_token], dim=1)\n",
    "                \n",
    "        duration = time.time() - start\n",
    "        speed = n_tokens / duration\n",
    "        self.metrics[\"inference_speed\"] = speed\n",
    "        print(f\"Inference Speed: {speed:.2f} tokens/sec\")\n",
    "\n",
    "    def save_metrics(self):\n",
    "        \"\"\"Saves metrics to JSON for future comparison with Mamba.\"\"\"\n",
    "        with open(CONF[\"metrics_filename\"], \"w\") as f:\n",
    "            json.dump(self.metrics, f)\n",
    "        print(f\"Metrics saved to {CONF['metrics_filename']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4868920e-40a4-4799-94f2-d88481a59946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Parameters: 354,823,168\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "183305251dce4e3086f901af0bfae787",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training GPT-2:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 394.00 MiB. GPU 0 has a total capacity of 11.64 GiB of which 70.88 MiB is free. Process 1634211 has 11.56 GiB memory in use. Of the allocated memory 10.90 GiB is allocated by PyTorch, and 544.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel Parameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer.count_parameters()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# 1. Train\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONF\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain_steps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# 2. Evaluate\u001b[39;00m\n\u001b[32m      9\u001b[39m trainer.evaluate_lambada()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mGPT2Trainer.train\u001b[39m\u001b[34m(self, steps)\u001b[39m\n\u001b[32m     38\u001b[39m outputs = \u001b[38;5;28mself\u001b[39m.model(input_ids, labels=labels)\n\u001b[32m     39\u001b[39m loss = outputs.loss / CONF[\u001b[33m\"\u001b[39m\u001b[33mgrad_accum\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (i + \u001b[32m1\u001b[39m) % CONF[\u001b[33m\"\u001b[39m\u001b[33mgrad_accum\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[32m0\u001b[39m:\n\u001b[32m     43\u001b[39m     \u001b[38;5;28mself\u001b[39m.optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/long-attention/.venv/lib/python3.12/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/long-attention/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/long-attention/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 394.00 MiB. GPU 0 has a total capacity of 11.64 GiB of which 70.88 MiB is free. Process 1634211 has 11.56 GiB memory in use. Of the allocated memory 10.90 GiB is allocated by PyTorch, and 544.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Initialize\n",
    "trainer = GPT2Trainer(gpt2_config)\n",
    "print(f\"Model Parameters: {trainer.count_parameters():,}\")\n",
    "\n",
    "# 1. Train\n",
    "trainer.train(CONF[\"train_steps\"])\n",
    "\n",
    "# 2. Evaluate\n",
    "trainer.evaluate_lambada()\n",
    "trainer.measure_inference_speed()\n",
    "\n",
    "# 3. Save Data for later Mamba comparison\n",
    "trainer.save_metrics()\n",
    "\n",
    "# 4. Simple Visualization for GPT-2 only\n",
    "def smooth(scalars, weight=0.9):\n",
    "    last = scalars[0]\n",
    "    smoothed = []\n",
    "    for point in scalars:\n",
    "        smoothed_val = last * weight + (1 - weight) * point\n",
    "        smoothed.append(smoothed_val)\n",
    "        last = smoothed_val\n",
    "    return smoothed\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(smooth(trainer.metrics[\"loss\"]), label=\"GPT-2 Loss\")\n",
    "plt.title(\"GPT-2 Training Loss (Smoothed)\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Cross Entropy Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0790d378-9236-4eae-9d71-820f3551596c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
