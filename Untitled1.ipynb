{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d47adf3a-95a4-416f-b8e2-a0d1d1b87dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  GitPython not available. Git operations will be skipped.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "from hugging\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers import (\n",
    "    GPT2LMHeadModel, \n",
    "    GPT2Tokenizer, \n",
    "    GPT2Config,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import torchmetrics\n",
    "from torchmetrics.text import Perplexity, BLEUScore\n",
    "from torchmetrics import MeanMetric\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from datetime import datetime\n",
    "import subprocess\n",
    "\n",
    "# Try to import git, but don't fail if not available\n",
    "try:\n",
    "    import git\n",
    "    GIT_AVAILABLE = True\n",
    "except ImportError:\n",
    "    GIT_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è  GitPython not available. Git operations will be skipped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ce6a319-0062-46f6-bbbc-de15c8b0470b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Created directories: ./checkpoints, ./runs/tensorboard\n",
      "üöÄ Training Config:\n",
      "   Device: cuda\n",
      "   Output Dir: ./checkpoints\n",
      "   Log Dir: ./runs/tensorboard\n",
      "   Run Name: train_20251228_080806\n",
      "\n",
      "üí° Vast.ai Tip: N·∫øu c√≥ persistent volume, ƒë·ªïi output_dir sang /workspace ho·∫∑c /data\n"
     ]
    }
   ],
   "source": [
    "# Training Configuration\n",
    "class TrainingConfig:\n",
    "    # Model\n",
    "    model_name = \"gpt2\"  # Default Transformer\n",
    "    vocab_size = 50257\n",
    "    n_positions = 1024  # Context length\n",
    "    n_embd = 768\n",
    "    n_layer = 12\n",
    "    n_head = 12\n",
    "    \n",
    "    # Dataset\n",
    "    dataset_name = \"cerebras/SlimPajama-627B\"\n",
    "    max_seq_length = 1024\n",
    "    \n",
    "    # Training\n",
    "    batch_size = 4\n",
    "    gradient_accumulation_steps = 8\n",
    "    learning_rate = 3e-4\n",
    "    num_epochs = 1\n",
    "    warmup_steps = 1000\n",
    "    max_steps = 10000  # Limit for demo\n",
    "    eval_steps = 500\n",
    "    save_steps = 1000\n",
    "    logging_steps = 100\n",
    "    \n",
    "    # Paths\n",
    "    # Vast.ai tip: S·ª≠ d·ª•ng /workspace ho·∫∑c /data cho persistent storage\n",
    "    # output_dir = \"/workspace/checkpoints\"  # Uncomment n·∫øu c√≥ persistent volume\n",
    "    output_dir = \"./checkpoints\"\n",
    "    log_dir = \"./runs/tensorboard\"\n",
    "    run_name = f\"train_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    \n",
    "    # Device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    mixed_precision = True\n",
    "    \n",
    "    def __init__(self):\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        os.makedirs(self.log_dir, exist_ok=True)\n",
    "        print(f\"üìÅ Created directories: {self.output_dir}, {self.log_dir}\")\n",
    "\n",
    "config = TrainingConfig()\n",
    "print(f\"üöÄ Training Config:\")\n",
    "print(f\"   Device: {config.device}\")\n",
    "print(f\"   Output Dir: {config.output_dir}\")\n",
    "print(f\"   Log Dir: {config.log_dir}\")\n",
    "print(f\"   Run Name: {config.run_name}\")\n",
    "print(f\"\\nüí° Vast.ai Tip: N·∫øu c√≥ persistent volume, ƒë·ªïi output_dir sang /workspace ho·∫∑c /data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2874f9dc-bc7d-43d8-b6ba-8541ede68c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. HuggingFace Authentication (n·∫øu c·∫ßn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3b99eed-9178-461e-99a4-c32207e5e07a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31314e8f4264458281c2133de5955ffc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50b2df6d-3e85-4716-8e46-717ced699669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d848762ffb148d09fea01339bc88aa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a46e14fd43c04f84adfab698c54af7bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "988a3a82001345a4ad7c786edf02e755",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86ee87b3cd4945ed998e3527b85807b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "023998075ac34774bf02d58c6f28ba3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Vocab size: 50257\n",
      "üì• Loading SlimPajama dataset (streaming)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2816e308cd6842099181d164d05fbb5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/59166 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49f01ff241c14e5eb39f1eff66a37c25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/31428 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc79ffa07f274e82966458a24039d442",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/31411 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ba6feeba098447bbb205c1244721ad1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/59166 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11df11a6c6c040549de9f1a5b289d3a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/31428 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f9d5fa7205f48eea8977dd6c69b7081",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/31411 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset loaded successfully\n"
     ]
    }
   ],
   "source": [
    "## 3. Load Dataset v√† Tokenizer\n",
    "# Load tokenizer\n",
    "print(\"üì• Loading tokenizer...\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(config.model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "print(f\"   Vocab size: {len(tokenizer)}\")\n",
    "\n",
    "# Load dataset in streaming mode\n",
    "# Vast.ai tip: Streaming mode kh√¥ng c·∫ßn download to√†n b·ªô dataset, ti·∫øt ki·ªám disk space\n",
    "print(\"üì• Loading SlimPajama dataset (streaming)...\")\n",
    "try:\n",
    "    train_dataset = load_dataset(\n",
    "        config.dataset_name,\n",
    "        split=\"train\",\n",
    "        streaming=True\n",
    "    )\n",
    "    \n",
    "    # Take a subset for validation\n",
    "    eval_dataset = load_dataset(\n",
    "        config.dataset_name,\n",
    "        split=\"train\",\n",
    "        streaming=True\n",
    "    )\n",
    "    print(\"‚úÖ Dataset loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading dataset: {e}\")\n",
    "    print(\"   Make sure you have internet connection and HuggingFace access\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03f972aa-6ef6-4a2a-9827-c9e4f51cba67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Creating train dataset...\n",
      "üîÑ Creating eval dataset...\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing function\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize text and truncate to max_seq_length\"\"\"\n",
    "    texts = examples['text'] if isinstance(examples, dict) else [examples['text']]\n",
    "    \n",
    "    # Tokenize\n",
    "    tokenized = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        max_length=config.max_seq_length,\n",
    "        padding='max_length',\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'input_ids': tokenized['input_ids'].squeeze(0),\n",
    "        'attention_mask': tokenized['attention_mask'].squeeze(0)\n",
    "    }\n",
    "\n",
    "# Create iterable dataset wrapper\n",
    "class SlimPajamaDataset(IterableDataset):\n",
    "    def __init__(self, dataset, tokenizer, max_length, max_samples=None):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.max_samples = max_samples\n",
    "        \n",
    "    def __iter__(self):\n",
    "        count = 0\n",
    "        for example in self.dataset:\n",
    "            if self.max_samples and count >= self.max_samples:\n",
    "                break\n",
    "            tokenized = tokenize_function(example)\n",
    "            yield tokenized\n",
    "            count += 1\n",
    "\n",
    "# Create datasets\n",
    "print(\"üîÑ Creating train dataset...\")\n",
    "train_iterable = SlimPajamaDataset(\n",
    "    train_dataset, \n",
    "    tokenizer, \n",
    "    config.max_seq_length,\n",
    "    max_samples=config.max_steps * config.batch_size * config.gradient_accumulation_steps\n",
    ")\n",
    "\n",
    "print(\"üîÑ Creating eval dataset...\")\n",
    "eval_iterable = SlimPajamaDataset(\n",
    "    eval_dataset,\n",
    "    tokenizer,\n",
    "    config.max_seq_length,\n",
    "    max_samples=100  # Small eval set\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df109ee2-3035-4dc4-a267-532ff0b1d8ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Initializing GPT-2 model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f36c3e763544586bd81e068f1c4a4d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d23b5e52d03b45b78443feff355cbe85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Total parameters: 124,439,808\n",
      "   Trainable parameters: 124,439,808\n",
      "   Model size: 0.50 GB (FP32)\n"
     ]
    }
   ],
   "source": [
    "## 4. Kh·ªüi t·∫°o Model\n",
    "# Initialize model\n",
    "print(\"ü§ñ Initializing GPT-2 model...\")\n",
    "model_config = GPT2Config(\n",
    "    vocab_size=config.vocab_size,\n",
    "    n_positions=config.n_positions,\n",
    "    n_embd=config.n_embd,\n",
    "    n_layer=config.n_layer,\n",
    "    n_head=config.n_head,\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(config.model_name, config=model_config)\n",
    "model = model.to(config.device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"   Total parameters: {total_params:,}\")\n",
    "print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"   Model size: {total_params * 4 / 1e9:.2f} GB (FP32)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ef48e0b-4159-4573-8109-2acdf5b6333b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5. Setup Optimizer v√† Scheduler\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config.learning_rate,\n",
    "    betas=(0.9, 0.95),\n",
    "    weight_decay=0.1\n",
    ")\n",
    "\n",
    "# Scheduler will be created after we know total steps\n",
    "# We'll create it in the training loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ff029de-c00c-431f-b919-fe296ab2b25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Metrics evaluator initialized\n"
     ]
    }
   ],
   "source": [
    "## 6. H√†m ƒë√°nh gi√° Metrics v·ªõi Torchmetrics\n",
    "# Initialize metrics\n",
    "class MetricsEvaluator:\n",
    "    \"\"\"ƒê√°nh gi√° metrics s·ª≠ d·ª•ng Torchmetrics\"\"\"\n",
    "    \n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "        \n",
    "        # Loss metric\n",
    "        self.loss_metric = MeanMetric().to(device)\n",
    "        \n",
    "        # Perplexity metric\n",
    "        self.perplexity_metric = Perplexity(ignore_index=-100).to(device)\n",
    "        \n",
    "        # Additional metrics\n",
    "        self.learning_rate_metric = MeanMetric().to(device)\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Reset t·∫•t c·∫£ metrics\"\"\"\n",
    "        self.loss_metric.reset()\n",
    "        self.perplexity_metric.reset()\n",
    "        self.learning_rate_metric.reset()\n",
    "    \n",
    "    def update(self, logits, labels, loss, lr=None):\n",
    "        \"\"\"\n",
    "        Update metrics v·ªõi batch m·ªõi\n",
    "        \n",
    "        Args:\n",
    "            logits: Model predictions [batch, seq_len, vocab_size]\n",
    "            labels: Ground truth labels [batch, seq_len]\n",
    "            loss: Computed loss value\n",
    "            lr: Current learning rate (optional)\n",
    "        \"\"\"\n",
    "        # Update loss\n",
    "        self.loss_metric.update(loss.item())\n",
    "        \n",
    "        # Update perplexity\n",
    "        # Perplexity metric expects: logits [batch, seq, vocab] and targets [batch, seq]\n",
    "        # Shift for next-token prediction\n",
    "        shift_logits = logits[..., :-1, :].contiguous()  # [batch, seq-1, vocab]\n",
    "        shift_labels = labels[..., 1:].contiguous()     # [batch, seq-1]\n",
    "        \n",
    "        # Set padding tokens to -100 (ignore_index) so Perplexity metric ignores them\n",
    "        shift_labels_masked = shift_labels.clone()\n",
    "        padding_mask = (shift_labels == tokenizer.pad_token_id) | (shift_labels < 0)\n",
    "        shift_labels_masked[padding_mask] = -100\n",
    "        \n",
    "        # Update perplexity with proper 3D shape\n",
    "        self.perplexity_metric.update(shift_logits, shift_labels_masked)\n",
    "        \n",
    "        # Update learning rate if provided\n",
    "        if lr is not None:\n",
    "            self.learning_rate_metric.update(lr)\n",
    "    \n",
    "    def compute(self):\n",
    "        \"\"\"Compute v√† return t·∫•t c·∫£ metrics\"\"\"\n",
    "        metrics = {\n",
    "            'loss': self.loss_metric.compute().item(),\n",
    "            'perplexity': self.perplexity_metric.compute().item(),\n",
    "            'learning_rate': self.learning_rate_metric.compute().item() if self.learning_rate_metric._update_count > 0 else 0.0\n",
    "        }\n",
    "        return metrics\n",
    "    \n",
    "    def compute_and_reset(self):\n",
    "        \"\"\"Compute metrics v√† reset cho epoch ti·∫øp theo\"\"\"\n",
    "        metrics = self.compute()\n",
    "        self.reset()\n",
    "        return metrics\n",
    "\n",
    "# Initialize evaluator\n",
    "metrics_evaluator = MetricsEvaluator(config.device)\n",
    "print(\"‚úÖ Metrics evaluator initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cacff74c-b401-4de2-9fee-9a1dd15fe863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä TensorBoard logging to: ./runs/tensorboard/train_20251228_080806\n",
      "   View with: tensorboard --logdir ./runs/tensorboard\n"
     ]
    }
   ],
   "source": [
    "## 7. Setup TensorBoard\n",
    "# Initialize TensorBoard writer\n",
    "tb_writer = SummaryWriter(log_dir=os.path.join(config.log_dir, config.run_name))\n",
    "print(f\"üìä TensorBoard logging to: {tb_writer.log_dir}\")\n",
    "print(f\"   View with: tensorboard --logdir {config.log_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6287e4c7-05bd-460c-b0fb-17aa692457f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting training for 10000 steps...\n",
      "   Batch size: 4\n",
      "   Gradient accumulation: 8\n",
      "   Effective batch size: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3837/2417048183.py:24: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler() if config.mixed_precision else None\n"
     ]
    }
   ],
   "source": [
    "## 8. Training Loop\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # Causal LM, not masked LM\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_iterable,\n",
    "    batch_size=config.batch_size,\n",
    "    collate_fn=data_collator,\n",
    "    num_workers=0  # Streaming dataset doesn't support multiprocessing\n",
    ")\n",
    "\n",
    "eval_loader = DataLoader(\n",
    "    eval_iterable,\n",
    "    batch_size=config.batch_size,\n",
    "    collate_fn=data_collator,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "# Mixed precision scaler\n",
    "scaler = torch.cuda.amp.GradScaler() if config.mixed_precision else None\n",
    "\n",
    "# Training state\n",
    "global_step = 0\n",
    "best_eval_loss = float('inf')\n",
    "\n",
    "# Create scheduler\n",
    "total_steps = config.max_steps\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=config.warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(f\"üöÄ Starting training for {total_steps} steps...\")\n",
    "print(f\"   Batch size: {config.batch_size}\")\n",
    "print(f\"   Gradient accumulation: {config.gradient_accumulation_steps}\")\n",
    "print(f\"   Effective batch size: {config.batch_size * config.gradient_accumulation_steps}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e23cdf0-d5f1-4b1f-b81f-b9b8350e6033",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:   0%|          | 0/10000 [07:41<?, ?it/s]\u001b[A\n",
      "/tmp/ipykernel_3837/1713481210.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "\n",
      "Training:   0%|          | 0/10000 [02:04<?, ?it/s, loss=27.9449, ppl=inf, lr=1.51e-05]\u001b[A\n",
      "Training:   1%|          | 100/10000 [02:04<3:24:41,  1.24s/it, loss=27.9449, ppl=inf, lr=1.51e-05]\u001b[A\n",
      "Training:   1%|          | 100/10000 [04:05<3:24:41,  1.24s/it, loss=27.2630, ppl=inf, lr=4.51e-05]\u001b[A\n",
      "Training:   2%|‚ñè         | 200/10000 [04:05<3:20:08,  1.23s/it, loss=27.2630, ppl=inf, lr=4.51e-05]\u001b[A\n",
      "Training:   2%|‚ñè         | 200/10000 [06:09<3:20:08,  1.23s/it, loss=27.2661, ppl=inf, lr=7.52e-05]\u001b[A\n",
      "Training:   3%|‚ñé         | 300/10000 [06:09<3:19:02,  1.23s/it, loss=27.2661, ppl=inf, lr=7.52e-05]\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 45\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# Update metrics\u001b[39;00m\n\u001b[32m     44\u001b[39m current_lr = scheduler.get_last_lr()[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m (step + \u001b[32m1\u001b[39m) % config.gradient_accumulation_steps == \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0.0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m \u001b[43mmetrics_evaluator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurrent_lr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcurrent_lr\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m     50\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# Logging\u001b[39;00m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (step + \u001b[32m1\u001b[39m) % config.gradient_accumulation_steps == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mMetricsEvaluator.update\u001b[39m\u001b[34m(self, logits, labels, loss, lr)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[33;03mUpdate metrics v·ªõi batch m·ªõi\u001b[39;00m\n\u001b[32m     27\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     32\u001b[39m \u001b[33;03m    lr: Current learning rate (optional)\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Update loss\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[38;5;28mself\u001b[39m.loss_metric.update(\u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Update perplexity\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Perplexity metric expects: logits [batch, seq, vocab] and targets [batch, seq]\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Shift for next-token prediction\u001b[39;00m\n\u001b[32m     40\u001b[39m shift_logits = logits[..., :-\u001b[32m1\u001b[39m, :].contiguous()  \u001b[38;5;66;03m# [batch, seq-1, vocab]\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "model.train()\n",
    "metrics_evaluator.reset()\n",
    "\n",
    "progress_bar = tqdm(total=total_steps, desc=\"Training\")\n",
    "\n",
    "for step, batch in enumerate(train_loader):\n",
    "    if global_step >= total_steps:\n",
    "        break\n",
    "    \n",
    "    # Move batch to device\n",
    "    input_ids = batch['input_ids'].to(config.device)\n",
    "    labels = batch['labels'].to(config.device)\n",
    "    \n",
    "    # Forward pass with mixed precision\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if config.mixed_precision and scaler:\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(input_ids=input_ids, labels=labels)\n",
    "            loss = outputs.loss / config.gradient_accumulation_steps\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        if (step + 1) % config.gradient_accumulation_steps == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "    else:\n",
    "        outputs = model(input_ids=input_ids, labels=labels)\n",
    "        loss = outputs.loss / config.gradient_accumulation_steps\n",
    "        loss.backward()\n",
    "        \n",
    "        if (step + 1) % config.gradient_accumulation_steps == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "    # Update metrics\n",
    "    current_lr = scheduler.get_last_lr()[0] if (step + 1) % config.gradient_accumulation_steps == 0 else 0.0\n",
    "    metrics_evaluator.update(\n",
    "        outputs.logits, \n",
    "        labels, \n",
    "        outputs.loss * config.gradient_accumulation_steps,\n",
    "        lr=current_lr if current_lr > 0 else None\n",
    "    )\n",
    "    \n",
    "    # Logging\n",
    "    if (step + 1) % config.gradient_accumulation_steps == 0:\n",
    "        global_step += 1\n",
    "        \n",
    "        if global_step % config.logging_steps == 0:\n",
    "            metrics = metrics_evaluator.compute()\n",
    "            \n",
    "            # Log to TensorBoard\n",
    "            tb_writer.add_scalar('train/loss', metrics['loss'], global_step)\n",
    "            tb_writer.add_scalar('train/perplexity', metrics['perplexity'], global_step)\n",
    "            tb_writer.add_scalar('train/learning_rate', metrics['learning_rate'], global_step)\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f\"{metrics['loss']:.4f}\",\n",
    "                'ppl': f\"{metrics['perplexity']:.2f}\",\n",
    "                'lr': f\"{metrics['learning_rate']:.2e}\"\n",
    "            })\n",
    "            progress_bar.update(config.logging_steps)\n",
    "            \n",
    "            # Reset metrics for next logging period\n",
    "            metrics_evaluator.reset()\n",
    "        \n",
    "        # Evaluation\n",
    "        if global_step % config.eval_steps == 0:\n",
    "            print(f\"\\nüîç Evaluating at step {global_step}...\")\n",
    "            model.eval()\n",
    "            eval_metrics_evaluator = MetricsEvaluator(config.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                eval_count = 0\n",
    "                for eval_batch in eval_loader:\n",
    "                    if eval_count >= 10:  # Limit eval batches\n",
    "                        break\n",
    "                    \n",
    "                    eval_input_ids = eval_batch['input_ids'].to(config.device)\n",
    "                    eval_labels = eval_batch['labels'].to(config.device)\n",
    "                    \n",
    "                    if config.mixed_precision:\n",
    "                        with torch.cuda.amp.autocast():\n",
    "                            eval_outputs = model(input_ids=eval_input_ids, labels=eval_labels)\n",
    "                    else:\n",
    "                        eval_outputs = model(input_ids=eval_input_ids, labels=eval_labels)\n",
    "                    \n",
    "                    eval_metrics_evaluator.update(\n",
    "                        eval_outputs.logits,\n",
    "                        eval_labels,\n",
    "                        eval_outputs.loss\n",
    "                    )\n",
    "                    eval_count += 1\n",
    "            \n",
    "            eval_metrics = eval_metrics_evaluator.compute_and_reset()\n",
    "            \n",
    "            # Log eval metrics to TensorBoard\n",
    "            tb_writer.add_scalar('eval/loss', eval_metrics['loss'], global_step)\n",
    "            tb_writer.add_scalar('eval/perplexity', eval_metrics['perplexity'], global_step)\n",
    "            \n",
    "            print(f\"   Eval Loss: {eval_metrics['loss']:.4f}\")\n",
    "            print(f\"   Eval Perplexity: {eval_metrics['perplexity']:.2f}\")\n",
    "            \n",
    "            # Save best model\n",
    "            if eval_metrics['loss'] < best_eval_loss:\n",
    "                best_eval_loss = eval_metrics['loss']\n",
    "                save_path = os.path.join(config.output_dir, f\"best_model_step_{global_step}\")\n",
    "                model.save_pretrained(save_path)\n",
    "                tokenizer.save_pretrained(save_path)\n",
    "                print(f\"   üíæ Saved best model to {save_path}\")\n",
    "            \n",
    "            model.train()\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if global_step % config.save_steps == 0:\n",
    "            save_path = os.path.join(config.output_dir, f\"checkpoint_step_{global_step}\")\n",
    "            model.save_pretrained(save_path)\n",
    "            tokenizer.save_pretrained(save_path)\n",
    "            print(f\"üíæ Saved checkpoint to {save_path}\")\n",
    "\n",
    "progress_bar.close()\n",
    "tb_writer.close()\n",
    "print(\"\\n‚úÖ Training completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae26f06-8c00-4649-8e86-150dfc0a9742",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 8. L∆∞u k·∫øt qu·∫£ v√† commit l√™n GitHub\n",
    "# L∆∞u training summary\n",
    "training_summary = {\n",
    "    'run_name': config.run_name,\n",
    "    'total_steps': global_step,\n",
    "    'best_eval_loss': best_eval_loss,\n",
    "    'config': {\n",
    "        'model_name': config.model_name,\n",
    "        'batch_size': config.batch_size,\n",
    "        'learning_rate': config.learning_rate,\n",
    "        'max_seq_length': config.max_seq_length,\n",
    "    },\n",
    "    'timestamp': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "summary_path = os.path.join(config.output_dir, 'training_summary.json')\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(training_summary, f, indent=2)\n",
    "\n",
    "print(f\"üìù Training summary saved to {summary_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
