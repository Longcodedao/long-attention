{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c98040e-13e5-4951-8999-b09b91932e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "from model import HoloConfig, HoloForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb7dd5af-d2f6-4857-806f-be5fd19b4584",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 'small'\n",
    "small_n_config = HoloConfig.from_preset(size, use_version = 1)\n",
    "small_t_config = HoloConfig.from_preset(size, use_version = 2)\n",
    "\n",
    "model_n = HoloForCausalLM(small_n_config)\n",
    "model_t = HoloForCausalLM(small_t_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e22feec-6c74-49ca-82d7-9ea26eeb8e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing models...\n",
      "\n",
      "============================================================\n",
      "Starting Inference Benchmark (Batch Size: 8)\n",
      "============================================================\n",
      "\n",
      "Testing Sequence Length: 512\n",
      "  -> Version 1 (Naive/Eager): 113.96 ms | 35943.35 tok/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/long-attention/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:1988: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error: accessing tensor output of CUDAGraphs that has been overwritten by a subsequent run. Stack trace: File \"/workspace/long-attention/model/layers.py\", line 59, in fused_pos_step\n    return (mem * q).real / scale. To prevent overwriting, clone the tensor outside of torch.compile() or call torch.compiler.cudagraph_mark_step_begin() before each model invocation.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 121\u001b[39m\n\u001b[32m    113\u001b[39m models_to_test = {\n\u001b[32m    114\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mVersion 1 (Naive/Eager)\u001b[39m\u001b[33m\"\u001b[39m: model_n,\n\u001b[32m    115\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mVersion 2 (Triton Fused)\u001b[39m\u001b[33m\"\u001b[39m: model_t\n\u001b[32m    116\u001b[39m }\n\u001b[32m    118\u001b[39m \u001b[38;5;66;03m# Run benchmark\u001b[39;00m\n\u001b[32m    119\u001b[39m \u001b[38;5;66;03m# Note: Ensure your GPU has enough VRAM for 8192. \u001b[39;00m\n\u001b[32m    120\u001b[39m \u001b[38;5;66;03m# If OOM, reduce batch_size to 1 or max seq_len.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m df_results = \u001b[43mrun_inference_benchmark\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodels_to_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseq_lens\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m4096\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m8192\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\n\u001b[32m    125\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[38;5;66;03m# ==========================================\u001b[39;00m\n\u001b[32m    128\u001b[39m \u001b[38;5;66;03m# 4. Visualization\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[38;5;66;03m# ==========================================\u001b[39;00m\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mplot_results\u001b[39m(df):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 104\u001b[39m, in \u001b[36mrun_inference_benchmark\u001b[39m\u001b[34m(models_dict, seq_lens, batch_size, warmup, repetitions)\u001b[39m\n\u001b[32m    102\u001b[39m                 torch.cuda.empty_cache()\n\u001b[32m    103\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m pd.DataFrame(results)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 63\u001b[39m, in \u001b[36mrun_inference_benchmark\u001b[39m\u001b[34m(models_dict, seq_lens, batch_size, warmup, repetitions)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(warmup):\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m         _ = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m torch.cuda.synchronize() \u001b[38;5;66;03m# Wait for warmup to finish\u001b[39;00m\n\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# 3. Measurement\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/long-attention/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/long-attention/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/long-attention/model/modeling_holo.py:94\u001b[39m, in \u001b[36mHoloForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, labels, inputs_embeds, **kwargs)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids=\u001b[38;5;28;01mNone\u001b[39;00m, labels=\u001b[38;5;28;01mNone\u001b[39;00m, inputs_embeds=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mholo\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m     hidden_states = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m     96\u001b[39m     logits = \u001b[38;5;28mself\u001b[39m.lm_head(hidden_states)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/long-attention/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/long-attention/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/long-attention/model/modeling_holo.py:69\u001b[39m, in \u001b[36mHoloModel.forward\u001b[39m\u001b[34m(self, input_ids, inputs_embeds, **kwargs)\u001b[39m\n\u001b[32m     63\u001b[39m         hidden_states = torch.utils.checkpoint.checkpoint(\n\u001b[32m     64\u001b[39m             create_custom_forward(block),\n\u001b[32m     65\u001b[39m             hidden_states,\n\u001b[32m     66\u001b[39m             use_reentrant=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     67\u001b[39m         )\n\u001b[32m     68\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m         hidden_states = \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.ln_f(hidden_states)\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m BaseModelOutputWithPast(last_hidden_state=hidden_states)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/long-attention/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/long-attention/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/long-attention/model/layers.py:148\u001b[39m, in \u001b[36mHoloBlock.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m    146\u001b[39m     \u001b[38;5;66;03m# Residual connection 1 (Mixer)\u001b[39;00m\n\u001b[32m    147\u001b[39m     res = x\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    149\u001b[39m     x = res + \u001b[38;5;28mself\u001b[39m.gamma1 * x\n\u001b[32m    151\u001b[39m     \u001b[38;5;66;03m# Residual connection 2 (MLP)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/long-attention/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/long-attention/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/long-attention/model/layers.py:115\u001b[39m, in \u001b[36mHoloAttentionV2.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    112\u001b[39m g_pos = \u001b[38;5;28mself\u001b[39m.gate[:, \u001b[32m0\u001b[39m].view(\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m, H, \u001b[32m1\u001b[39m)\n\u001b[32m    113\u001b[39m g_assoc = \u001b[38;5;28mself\u001b[39m.gate[:, \u001b[32m1\u001b[39m].view(\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m, H, \u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m out_combined = (\u001b[43mout_pos\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mg_pos\u001b[49m) + (out_assoc * g_assoc)\n\u001b[32m    117\u001b[39m \u001b[38;5;66;03m# --- 5. Concatenate & Output ---\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[38;5;66;03m# Flatten H and D back to hd_dim\u001b[39;00m\n\u001b[32m    119\u001b[39m out_combined = out_combined.flatten(\u001b[32m2\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error: accessing tensor output of CUDAGraphs that has been overwritten by a subsequent run. Stack trace: File \"/workspace/long-attention/model/layers.py\", line 59, in fused_pos_step\n    return (mem * q).real / scale. To prevent overwriting, clone the tensor outside of torch.compile() or call torch.compiler.cudagraph_mark_step_begin() before each model invocation."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "# ==========================================\n",
    "# 1. Setup Models (User provided code)\n",
    "# ==========================================\n",
    "# Assuming HoloConfig and HoloForCausalLM are imported\n",
    "# from holo_gpt.config import HoloConfig\n",
    "# from holo_gpt.model import HoloForCausalLM\n",
    "\n",
    "print(\"Initializing models...\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "size = 'small'\n",
    "small_n_config = HoloConfig.from_preset(size, use_version = 1)\n",
    "small_t_config = HoloConfig.from_preset(size, use_version = 2)\n",
    "\n",
    "model_n = HoloForCausalLM(small_n_config)\n",
    "model_t = HoloForCausalLM(small_t_config)\n",
    "\n",
    "# (Mocking the models for the script logic - UNCOMMENT YOUR ACTUAL IMPORTS ABOVE)\n",
    "# Ensure your models are moved to .to(device) and .eval()\n",
    "model_n = model_n.to(device).eval()\n",
    "model_t = model_t.to(device).eval()\n",
    "\n",
    "# ==========================================\n",
    "# 2. The Benchmark Function\n",
    "# ==========================================\n",
    "\n",
    "def run_inference_benchmark(\n",
    "    models_dict, \n",
    "    seq_lens=[512, 1024, 2048, 4096, 8192], \n",
    "    batch_size=1, \n",
    "    warmup=5, \n",
    "    repetitions=20\n",
    "):\n",
    "    results = []\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Starting Inference Benchmark (Batch Size: {batch_size})\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    for seq_len in seq_lens:\n",
    "        print(f\"\\nTesting Sequence Length: {seq_len}\")\n",
    "        \n",
    "        # Create dummy input\n",
    "        # Note: We use high values to ensure no embedding index errors\n",
    "        input_ids = torch.randint(0, 1000, (batch_size, seq_len), device=device)\n",
    "        \n",
    "        for name, model in models_dict.items():\n",
    "            try:\n",
    "                # 1. Clear Cache to prevent OOM from previous runs affecting this one\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "                \n",
    "                # 2. Warmup\n",
    "                # GPU needs to compile kernels and settle clock speeds\n",
    "                with torch.no_grad():\n",
    "                    for _ in range(warmup):\n",
    "                        _ = model(input_ids)\n",
    "                \n",
    "                torch.cuda.synchronize() # Wait for warmup to finish\n",
    "                \n",
    "                # 3. Measurement\n",
    "                start_event = torch.cuda.Event(enable_timing=True)\n",
    "                end_event = torch.cuda.Event(enable_timing=True)\n",
    "                \n",
    "                start_event.record()\n",
    "                with torch.no_grad():\n",
    "                    for _ in range(repetitions):\n",
    "                        _ = model(input_ids)\n",
    "                end_event.record()\n",
    "                \n",
    "                torch.cuda.synchronize() # Wait for all GPU work to finish\n",
    "                \n",
    "                # 4. Calculate Stats\n",
    "                total_time_ms = start_event.elapsed_time(end_event)\n",
    "                avg_time_ms = total_time_ms / repetitions\n",
    "                tokens_per_sec = (batch_size * seq_len) / (avg_time_ms / 1000)\n",
    "                \n",
    "                print(f\"  -> {name}: {avg_time_ms:.2f} ms | {tokens_per_sec:.2f} tok/s\")\n",
    "                \n",
    "                results.append({\n",
    "                    \"Model\": name,\n",
    "                    \"Seq_Len\": seq_len,\n",
    "                    \"Latency (ms)\": avg_time_ms,\n",
    "                    \"Throughput (tok/s)\": tokens_per_sec\n",
    "                })\n",
    "                \n",
    "            except RuntimeError as e:\n",
    "                if \"out of memory\" in str(e):\n",
    "                    print(f\"  -> {name}: OOM (Out of Memory)\")\n",
    "                    results.append({\n",
    "                        \"Model\": name,\n",
    "                        \"Seq_Len\": seq_len,\n",
    "                        \"Latency (ms)\": None,\n",
    "                        \"Throughput (tok/s)\": 0\n",
    "                    })\n",
    "                    torch.cuda.empty_cache()\n",
    "                else:\n",
    "                    raise e\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# ==========================================\n",
    "# 3. Run It\n",
    "# ==========================================\n",
    "\n",
    "# Dict of models to test\n",
    "models_to_test = {\n",
    "    \"Version 1 (Naive/Eager)\": model_n,\n",
    "    \"Version 2 (Triton Fused)\": model_t\n",
    "}\n",
    "\n",
    "# Run benchmark\n",
    "# Note: Ensure your GPU has enough VRAM for 8192. \n",
    "# If OOM, reduce batch_size to 1 or max seq_len.\n",
    "df_results = run_inference_benchmark(\n",
    "    models_to_test, \n",
    "    seq_lens=[512, 1024, 2048, 4096, 8192], \n",
    "    batch_size=8\n",
    ")\n",
    "\n",
    "# ==========================================\n",
    "# 4. Visualization\n",
    "# ==========================================\n",
    "\n",
    "def plot_results(df):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Plot 1: Latency (Lower is better)\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for model_name in df['Model'].unique():\n",
    "        subset = df[df['Model'] == model_name]\n",
    "        plt.plot(subset['Seq_Len'], subset['Latency (ms)'], marker='o', label=model_name)\n",
    "    \n",
    "    plt.title('Inference Latency (Lower is Better)')\n",
    "    plt.xlabel('Sequence Length')\n",
    "    plt.ylabel('Time (ms)')\n",
    "    plt.grid(True, which='both', linestyle='--', alpha=0.7)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot 2: Throughput (Higher is better)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for model_name in df['Model'].unique():\n",
    "        subset = df[df['Model'] == model_name]\n",
    "        plt.plot(subset['Seq_Len'], subset['Throughput (tok/s)'], marker='o', label=model_name)\n",
    "    \n",
    "    plt.title('Throughput (Higher is Better)')\n",
    "    plt.xlabel('Sequence Length')\n",
    "    plt.ylabel('Tokens / Sec')\n",
    "    plt.grid(True, which='both', linestyle='--', alpha=0.7)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nResults Summary:\")\n",
    "print(df_results)\n",
    "\n",
    "plot_results(df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7de703-df70-48e8-bc00-ef5efd69a9c0",
   "metadata": {},
   "source": [
    "### Backward speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eddc658-32be-42f6-b36c-609b703c31ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "\n",
    "# ==========================================\n",
    "# 1. Setup Models (Mock imports for context)\n",
    "# ==========================================\n",
    "# Ensure your models are on GPU\n",
    "# model_n = model_n.cuda().train()\n",
    "# model_t = model_t.cuda().train()\n",
    "\n",
    "# ==========================================\n",
    "# 2. Benchmark Function\n",
    "# ==========================================\n",
    "\n",
    "def run_training_benchmark(\n",
    "    models_dict, \n",
    "    seq_lens=[512, 1024, 2048, 4096, 8192], \n",
    "    batch_size=1, \n",
    "    warmup=3, \n",
    "    repetitions=10\n",
    "):\n",
    "    results = []\n",
    "    device = torch.device(\"cuda\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"TRAINING BENCHMARK (Forward + Backward) | Batch Size: {batch_size}\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    for seq_len in seq_lens:\n",
    "        print(f\"\\n--- Sequence Length: {seq_len} ---\")\n",
    "        \n",
    "        # Create Data\n",
    "        input_ids = torch.randint(0, 1000, (batch_size, seq_len), device=device)\n",
    "        labels = torch.randint(0, 1000, (batch_size, seq_len), device=device)\n",
    "        \n",
    "        for name, model in models_dict.items():\n",
    "            # Reset\n",
    "            model.zero_grad(set_to_none=True)\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            gc.collect()\n",
    "\n",
    "            try:\n",
    "                # 1. Warmup\n",
    "                # We must run fwd+bwd to compile the backward graph\n",
    "                for _ in range(warmup):\n",
    "                    outputs = model(input_ids)\n",
    "                    loss = outputs.logits.mean() # Dummy loss\n",
    "                    loss.backward()\n",
    "                    model.zero_grad(set_to_none=True)\n",
    "                \n",
    "                torch.cuda.synchronize()\n",
    "                \n",
    "                # 2. Measurement (Time)\n",
    "                start_event = torch.cuda.Event(enable_timing=True)\n",
    "                end_event = torch.cuda.Event(enable_timing=True)\n",
    "                \n",
    "                # Reset memory stats again before the timed run to get clean read\n",
    "                torch.cuda.reset_peak_memory_stats()\n",
    "                \n",
    "                start_event.record()\n",
    "                for _ in range(repetitions):\n",
    "                    outputs = model(input_ids)\n",
    "                    loss = outputs.logits.mean()\n",
    "                    loss.backward()\n",
    "                    model.zero_grad(set_to_none=True)\n",
    "                end_event.record()\n",
    "                \n",
    "                torch.cuda.synchronize()\n",
    "                \n",
    "                # 3. Measurement (Memory)\n",
    "                peak_mem_bytes = torch.cuda.max_memory_allocated()\n",
    "                peak_mem_gb = peak_mem_bytes / (1024**3)\n",
    "                \n",
    "                # 4. Stats\n",
    "                total_time_ms = start_event.elapsed_time(end_event)\n",
    "                avg_time_ms = total_time_ms / repetitions\n",
    "                tokens_per_sec = (batch_size * seq_len) / (avg_time_ms / 1000)\n",
    "                \n",
    "                print(f\"  -> {name}: {avg_time_ms:.2f} ms | {tokens_per_sec:.2f} tok/s | Mem: {peak_mem_gb:.2f} GB\")\n",
    "                \n",
    "                results.append({\n",
    "                    \"Model\": name,\n",
    "                    \"Seq_Len\": seq_len,\n",
    "                    \"Latency (ms)\": avg_time_ms,\n",
    "                    \"Throughput (tok/s)\": tokens_per_sec,\n",
    "                    \"Peak Memory (GB)\": peak_mem_gb\n",
    "                })\n",
    "\n",
    "            except RuntimeError as e:\n",
    "                if \"out of memory\" in str(e).lower():\n",
    "                    print(f\"  -> {name}: ❌ OOM (Out of Memory)\")\n",
    "                    results.append({\n",
    "                        \"Model\": name,\n",
    "                        \"Seq_Len\": seq_len,\n",
    "                        \"Latency (ms)\": None,\n",
    "                        \"Throughput (tok/s)\": 0,\n",
    "                        \"Peak Memory (GB)\": 0 # Indicate fail\n",
    "                    })\n",
    "                    torch.cuda.empty_cache()\n",
    "                else:\n",
    "                    print(f\"  -> {name}: ❌ Error: {e}\")\n",
    "                    raise e\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# ==========================================\n",
    "# 3. Execution\n",
    "# ==========================================\n",
    "\n",
    "# Make sure models are in TRAIN mode\n",
    "model_n.train()\n",
    "model_t.train()\n",
    "\n",
    "models_to_test = {\n",
    "    \"Eager (Naive)\": model_n,\n",
    "    \"Triton (Fused)\": model_t\n",
    "}\n",
    "\n",
    "# Run it\n",
    "df_train = run_training_benchmark(\n",
    "    models_to_test, \n",
    "    seq_lens=[512, 1024, 2048, 4096, 8192], \n",
    "    batch_size=8\n",
    ")\n",
    "\n",
    "# ==========================================\n",
    "# 4. Visualization\n",
    "# ==========================================\n",
    "\n",
    "def plot_training_results(df):\n",
    "    plt.figure(figsize=(18, 5))\n",
    "    \n",
    "    # Filter out OOMs for plotting lines\n",
    "    df_clean = df[df['Throughput (tok/s)'] > 0]\n",
    "\n",
    "    # Plot 1: Time\n",
    "    plt.subplot(1, 3, 1)\n",
    "    for name in df['Model'].unique():\n",
    "        subset = df_clean[df_clean['Model'] == name]\n",
    "        plt.plot(subset['Seq_Len'], subset['Latency (ms)'], marker='o', label=name)\n",
    "    plt.title('Training Step Latency (Lower is Better)')\n",
    "    plt.xlabel('Sequence Length')\n",
    "    plt.ylabel('Time (ms)')\n",
    "    plt.grid(True, alpha=0.5)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot 2: Memory\n",
    "    plt.subplot(1, 3, 2)\n",
    "    for name in df['Model'].unique():\n",
    "        subset = df_clean[df_clean['Model'] == name]\n",
    "        plt.plot(subset['Seq_Len'], subset['Peak Memory (GB)'], marker='s', label=name)\n",
    "    plt.title('Peak VRAM Usage (Lower is Better)')\n",
    "    plt.xlabel('Sequence Length')\n",
    "    plt.ylabel('Memory (GB)')\n",
    "    plt.grid(True, alpha=0.5)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot 3: Throughput\n",
    "    plt.subplot(1, 3, 3)\n",
    "    for name in df['Model'].unique():\n",
    "        subset = df_clean[df_clean['Model'] == name]\n",
    "        plt.plot(subset['Seq_Len'], subset['Throughput (tok/s)'], marker='^', label=name)\n",
    "    plt.title('Training Throughput (Higher is Better)')\n",
    "    plt.xlabel('Sequence Length')\n",
    "    plt.ylabel('Tokens / Sec')\n",
    "    plt.grid(True, alpha=0.5)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nFinal Results:\")\n",
    "print(df_train)\n",
    "plot_training_results(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d20343-50e6-4164-9d05-104a84da4448",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (My Project Name)",
   "language": "python",
   "name": "my-project-name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
