{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7800b71-a63d-4ac0-aa79-9659dd1c64ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from model.holo import HoloConfig, HoloForCausalLM\n",
    "from transformers import MambaConfig, MambaForCausalLM\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm \n",
    "import numpy as np\n",
    "import torch.nn.utils.rnn as rnn_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6803546-3711-4e8e-9b04-8e5e6f1101c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NIAHDataset(Dataset):\n",
    "    def __init__(self, size, min_len, max_len, vocab_size):\n",
    "        self.size = size\n",
    "        self.min_len = min_len\n",
    "        self.max_len = max_len\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # Special Tokens\n",
    "        self.start_tokens = torch.tensor([3, 4])\n",
    "        self.trigger_tokens = torch.tensor([5, 4, 3, 6])\n",
    "        self.flag_token = torch.tensor([2]) \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 1. Randomize Length for this specific sample\n",
    "        # This prevents the model from overfitting to a specific horizon\n",
    "        curr_len = np.random.randint(self.min_len, self.max_len + 1)\n",
    "        \n",
    "        # 2. Generate Key\n",
    "        key = torch.randint(10, self.vocab_size, (1,))\n",
    "\n",
    "        # 3. Calculate Noise\n",
    "        # Overhead: Start(2) + Flag(1) + Key(1) + Trigger(4) + Target(1) = 9\n",
    "        noise_len = curr_len - 9\n",
    "        if noise_len < 0: noise_len = 0 # Safety clipping\n",
    "\n",
    "        noise = torch.randint(10, self.vocab_size, (noise_len,))\n",
    "\n",
    "        # 4. Insert Key Randomly\n",
    "        insert_idx = torch.randint(0, noise_len + 1, (1,)).item()\n",
    "        \n",
    "        input_ids = torch.cat([\n",
    "            self.start_tokens,\n",
    "            noise[:insert_idx],\n",
    "            self.flag_token,     # The Flag\n",
    "            key,                 # The Needle\n",
    "            noise[insert_idx:],\n",
    "            self.trigger_tokens,\n",
    "            key\n",
    "        ])\n",
    "\n",
    "        labels = input_ids.clone()\n",
    "        labels[:-1] = -100 \n",
    "        \n",
    "        return {\"input_ids\": input_ids, \"labels\": labels}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f68c51a-e10e-4316-bd22-4a149add585f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 20 \n",
    "VOCAB_SIZE = 1000\n",
    "\n",
    "train_dataset = NIAHDataset(size=50000, min_len = 10, max_len = 30,  vocab_size=VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d19eb87-f33a-4a8a-b50c-fc58b1abb230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the input_ids: torch.Size([17])\n",
      "Shape of the labels: torch.Size([17])\n",
      "\n",
      "\n",
      "Input_ids: tensor([  3,   4, 987,   2, 246, 694, 712,  73, 654,  81, 297, 429,   5,   4,\n",
      "          3,   6, 246])\n",
      "Labels: tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100,  246])\n"
     ]
    }
   ],
   "source": [
    "examples = train_dataset[0]\n",
    "input_ids, labels = examples['input_ids'], examples['labels']\n",
    "\n",
    "print(f\"Shape of the input_ids: {input_ids.shape}\")\n",
    "print(f\"Shape of the labels: {labels.shape}\")\n",
    "print(\"\\n\")\n",
    "print(f\"Input_ids: {input_ids}\")\n",
    "print(f\"Labels: {labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc44f527-aef1-40c6-aea7-bd3225acfdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class NIAHDataset_Version2(Dataset):\n",
    "#     def __init__(self, size = 5000, seq_len = 256, vocab_size = 1000):\n",
    "#         \"\"\"\n",
    "#         Improved Synthetic Needle-In-A-Haystack Dataset.\n",
    "        \n",
    "#         Structure:\n",
    "#         [Noise Part 1] ... [Needle: KEY, VALUE] ... [Noise Part 2] ... [Query: KEY, VALUE]\n",
    "        \n",
    "#         The model sees everything up to the final 'KEY' and must predict the final 'VALUE'.\n",
    "#         \"\"\"\n",
    "\n",
    "#         self.size = size \n",
    "#         self.seq_len = seq_len \n",
    "\n",
    "#         # We reserve the last token in the vocab as the \"Needle Key\"\n",
    "#         # The rest (0 to vocab_size - 2) are used for Noise and Values \n",
    "#         self.key_token = vocab_size - 1\n",
    "#         self.noise_range = vocab_size - 1\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.size\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         # 1. Generate the Target Value (The Needle's content)\n",
    "#         # We pick a random token from the noise range\n",
    "#         target_token = torch.randint(0, self.noise_range, (1, ))\n",
    "\n",
    "#         # 2. Define the Needle: [KEY, VALUE]\n",
    "#         needle = torch.tensor([self.key_token, target_token.item()])\n",
    "\n",
    "#         # 3. Calculate Haystack (Noise) Length\n",
    "#         # Sequence = [Haystack1] + [Needle(2)] + [Haystack2] + [Query(2)]\n",
    "#         # Total overhead = 2 (Needle) + 2 (Query Trigger + Target) = 4 tokens\n",
    "#         haystack_len = self.seq_len - 4\n",
    "\n",
    "#         # 4. Generate Haystack\n",
    "#         haystack = torch.randint(0, self.noise_range, (haystack_len, ))\n",
    "\n",
    "#         # 5. Insert Needle at Random Depth (The \"Magic\")\n",
    "#         # Random split point for the haystack\n",
    "#         insert_idx = torch.randint(0, haystack_len + 1, (1,)).item()\n",
    "\n",
    "#         context_part1 = haystack[: insert_idx]\n",
    "#         context_part2 = haystack[insert_idx:]\n",
    "\n",
    "#         # 6. Construct the Full Input Sequence (Ground Truth)\n",
    "#         # We construct the VALID sequence ending in the target.\n",
    "#         # [Part1] [KEY, VAL] [Part2] [KEY, VAL]\n",
    "#         input_ids = torch.cat([\n",
    "#             context_part1,\n",
    "#             needle,         # The inserted needle\n",
    "#             context_part2,\n",
    "#             needle          # The query (Trigger + Ground Truth Target)\n",
    "#         ])\n",
    "\n",
    "#         # 7. Create Labels (Masked Loss)\n",
    "#         # We want the model to only learn from the FINAL prediction.\n",
    "#         labels = input_ids.clone()\n",
    "        \n",
    "#         # Mask everything with -100 (Ignored by CrossEntropyLoss)\n",
    "#         labels[:] = -100\n",
    "        \n",
    "#         # Unmask ONLY the last token (The Target Value)\n",
    "#         # Note: In HF Causal training, label[i] is the target for input[i-1].\n",
    "#         # So providing the full sequence as labels works perfectly; \n",
    "#         # the model tries to predict the last token from the second-to-last.\n",
    "#         labels[-1] = target_token.item()\n",
    "        \n",
    "#         return {\n",
    "#             \"input_ids\": input_ids, \n",
    "#             \"labels\": labels\n",
    "#         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88adf2e0-9931-4bc2-b93e-daf712677d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_collate_fn(batch):\n",
    "    # Extract inputs and labels\n",
    "    inputs = [item['input_ids'] for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "    \n",
    "    # Pad sequences to the longest in the batch (dynamic padding)\n",
    "    # batch_first=True makes shape [Batch, Seq_Len]\n",
    "    inputs_padded = rnn_utils.pad_sequence(inputs, batch_first=True, padding_value=0)\n",
    "    labels_padded = rnn_utils.pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": inputs_padded,\n",
    "        \"labels\": labels_padded\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c52c5675-21d1-4eb7-a060-f7c5f72bdbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(logits, labels):\n",
    "    \"\"\"\n",
    "    Calculates accuracy for the final token prediction.\n",
    "    \n",
    "    Args:\n",
    "        logits: [Batch, Seq_Len, Vocab] - The raw output from the model.\n",
    "        labels: [Batch, Seq_Len] - The input_ids (or targets).\n",
    "    \n",
    "    Logic:\n",
    "    The model predicts token[t+1] using the state at token[t].\n",
    "    We want to check if the model correctly predicted the LAST token (Target).\n",
    "    The state responsible for predicting the LAST token is the SECOND TO LAST token (Trigger).\n",
    "    \n",
    "    Index -1: The actual Target token (Ground Truth).\n",
    "    Index -2: The Trigger token (Model output here predicts the Target).\n",
    "    \"\"\"\n",
    "    # Check prediction at position -2 (The token BEFORE the target)\n",
    "    key_logit = logits[..., -2, :] \n",
    "    pred_token = torch.argmax(key_logit, dim=-1)\n",
    "    \n",
    "    # Check against the actual Target (which is at -1 in the input/labels)\n",
    "    target_token = labels[..., -1]\n",
    "    \n",
    "    return (pred_token == target_token).float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee732967-f2be-4c29-93f0-9506a9b4169d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_pipeline(model_name, model, train_loader, eval_seq_lens, vocab_size, device):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        model_name (str): Name for logging.\n",
    "        model (nn.Module): The HF-style model (returns outputs.loss).\n",
    "        train_loader (DataLoader): Loader for the fixed-length training data.\n",
    "        eval_seq_lens (list): List of lengths to test generalization on (e.g. [256, 512, 1024]).\n",
    "        vocab_size (int): Vocab size for generating eval data on the fly.\n",
    "        device (str): 'cuda' or 'cpu'.\n",
    "    \"\"\"\n",
    "    print(f\"\\n[{model_name}] Starting Training...\")\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.01)\n",
    "    model.train()\n",
    "\n",
    "    # --- TRAINING PHASE ---\n",
    "    # We use the train_loader which usually has a fixed sequence length (e.g. 256)\n",
    "    # This replaces the \"variable length loop\" from the old code, \n",
    "    # relying on the model to learn the mechanism from the fixed length dataset.\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=f\"Training {model_name}\", leave=True)\n",
    "    \n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        # Move batch to device\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Forward pass (HF models compute loss automatically if labels are passed)\n",
    "        outputs = model(input_ids=input_ids, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    # --- EVALUATION PHASE (Length Generalization) ---\n",
    "    print(f\"\\n[{model_name}] Evaluating Length Generalization...\")\n",
    "    results = []\n",
    "    model.eval()\n",
    "    \n",
    "    for length in eval_seq_lens:\n",
    "        # Generate a small temporary dataset for this specific length\n",
    "        # This mirrors the 'generator_fn' logic from the old code\n",
    "        # eval_ds = NIAHDataset(size=100, seq_len=length, vocab_size=vocab_size)\n",
    "        eval_ds = NIAHDataset(size=100,\n",
    "                              min_len = length, \n",
    "                              max_len = length,  \n",
    "                              vocab_size=vocab_size)\n",
    "        eval_loader = DataLoader(eval_ds, batch_size=1, shuffle=False)\n",
    "        \n",
    "        accs = []\n",
    "        with torch.no_grad():\n",
    "            for batch in eval_loader:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "\n",
    "                output = model(input_ids = input_ids)\n",
    "                acc = calculate_accuracy(output.logits, input_ids)\n",
    "                accs.append(acc)\n",
    "\n",
    "        avg_acc = np.mean(accs)\n",
    "        print(f\"  Len {length}: {avg_acc:.2%}\")\n",
    "        results.append(avg_acc)\n",
    "        \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "739ce14d-9ea6-46ea-9ddb-79dc53b3dd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "VOCAB_SIZE = 1000\n",
    "MIN_SEQ_LEN = 32\n",
    "MAX_SEQ_LEN = 128\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01fa096b-1746-4fa0-8abc-a595e7f37d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  3,   4, 207, 941, 101, 193,  45, 283, 841, 749, 391,   2, 842, 429,\n",
      "         567, 917,   5,   4,   3,   6, 842,   0,   0,   0],\n",
      "        [  3,   4, 992, 742, 849, 868, 702, 898, 282, 363, 126,   2, 440, 705,\n",
      "         159, 821, 706, 151, 818,   5,   4,   3,   6, 440],\n",
      "        [  3,   4, 908,   2, 249,  14, 328,  61,  60, 987, 590,  25, 773, 952,\n",
      "         483, 951, 646, 119,   5,   4,   3,   6, 249,   0],\n",
      "        [  3,   4,  75,  87,   2, 920, 790, 693,   5,   4,   3,   6, 920,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0]])\n",
      "tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100,  842, -100, -100, -100],\n",
      "        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,  440],\n",
      "        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,  249, -100],\n",
      "        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "          920, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]])\n"
     ]
    }
   ],
   "source": [
    "train_dataset = NIAHDataset(size=50000, min_len = 10, max_len = 30,  vocab_size=VOCAB_SIZE)\n",
    "\n",
    "# 2. Create the DataLoader\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=4, \n",
    "    shuffle=True, \n",
    "    num_workers=2,# Can use parallel loading now\n",
    "    collate_fn=pad_collate_fn\n",
    ")\n",
    "\n",
    "iter_loader = next(iter(train_loader))\n",
    "inputs_ids, labels = iter_loader['input_ids'], iter_loader['labels']\n",
    "\n",
    "print(inputs_ids)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26d4ba3d-c5a2-4131-9732-08511cd03ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = NIAHDataset(size=50000, \n",
    "                            min_len = MIN_SEQ_LEN, \n",
    "                            max_len = MAX_SEQ_LEN,  \n",
    "                            vocab_size=VOCAB_SIZE)\n",
    "\n",
    "# 2. Create the DataLoader\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    num_workers=2,# Can use parallel loading now\n",
    "    collate_fn=pad_collate_fn\n",
    ")\n",
    "\n",
    "\n",
    "holo_config =  HoloConfig(\n",
    "    d_model=128, \n",
    "    num_hidden_layers = 2,\n",
    "    num_heads = 4,\n",
    "    vocab_size=VOCAB_SIZE, \n",
    "    resid_dropout = 0.0, \n",
    "    dropout = 0.0,\n",
    "    use_version=2\n",
    ")\n",
    "holo_model = HoloForCausalLM(holo_config).to(device)\n",
    "\n",
    "config_kwargs = {\n",
    "     \"vocab_size\": VOCAB_SIZE, \n",
    "     \"ssm_cfg\": {\"dropout\": 0.0 }\n",
    "}\n",
    "mamba_config = MambaConfig(\n",
    "    hidden_size = 128,\n",
    "    num_hidden_layers = 2, \n",
    "    **config_kwargs\n",
    ")\n",
    "\n",
    "mamba_model = MambaForCausalLM(mamba_config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fe55b9e-5caa-4070-b169-a2ff7edb3873",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Mamba] Starting Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23ca1ee13e1a4770a2e2328548c0ceb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Mamba:   0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Mamba] Evaluating Length Generalization...\n",
      "  Len 128: 77.00%\n",
      "  Len 256: 82.00%\n",
      "  Len 512: 72.00%\n",
      "  Len 1024: 63.00%\n",
      "  Len 2048: 60.00%\n",
      "  Len 4096: 47.00%\n",
      "  Len 8192: 23.00%\n"
     ]
    }
   ],
   "source": [
    "results = train_eval_pipeline(\n",
    "        \"Mamba\", \n",
    "        mamba_model, \n",
    "        train_loader, \n",
    "        eval_seq_lens=[128, 256, 512, 1024, 2048, 4096, 8192], \n",
    "        vocab_size=VOCAB_SIZE, \n",
    "        device=device\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a211c78-a29d-4221-9ae4-e5adbbcac6bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Holo] Starting Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71002c03f9d04322928df2f05931976d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Holo:   0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Holo] Evaluating Length Generalization...\n",
      "  Len 128: 0.00%\n",
      "  Len 256: 0.00%\n",
      "  Len 512: 0.00%\n",
      "  Len 1024: 0.00%\n",
      "  Len 2048: 0.00%\n",
      "  Len 4096: 0.00%\n",
      "  Len 8192: 0.00%\n"
     ]
    }
   ],
   "source": [
    "results = train_eval_pipeline(\n",
    "        \"Holo\", \n",
    "        holo_model, \n",
    "        train_loader, \n",
    "        eval_seq_lens=[128, 256, 512, 1024, 2048, 4096, 8192],\n",
    "        vocab_size=VOCAB_SIZE, \n",
    "        device=device\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21b4a9c-6c43-4d10-be46-8f60e964058d",
   "metadata": {},
   "source": [
    "### Play around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12026cca-d70b-4909-ab97-e2a6fd00831e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the input: torch.Size([1, 253])\n",
      "Shape of the labels: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "def generate_niah(batch_size, seq_len, vocab_size):\n",
    "    key = torch.randint(10, vocab_size, (batch_size,))\n",
    "    # Ensure noise doesn't contain the \"trigger\" tokens (3, 4, 5, 6)\n",
    "    noise = torch.randint(10, vocab_size, (batch_size, seq_len-10))\n",
    "    start = torch.tensor([3, 4]).expand(batch_size, -1)\n",
    "    end = torch.tensor([5, 4, 3, 6]).expand(batch_size, -1)\n",
    "\n",
    "    inputs = []\n",
    "    for b in range(batch_size):\n",
    "        seq = torch.cat([start[b], key[b].unsqueeze(0), noise[b], end[b]])\n",
    "        inputs.append(seq)\n",
    "\n",
    "    return torch.stack(inputs).to(device), key.to(device)\n",
    "\n",
    "\n",
    "input_ids, labels = generate_niah(1, 256, 100)\n",
    "print(f\"Shape of the input: {input_ids.shape}\")\n",
    "print(f\"Shape of the labels: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f88c4c-cd0d-464f-89c3-1a529852ce1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (MyProject)",
   "language": "python",
   "name": "myproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
